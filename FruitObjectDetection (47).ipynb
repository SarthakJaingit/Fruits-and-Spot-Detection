{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FruitObjectDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZfnWC_NF7NM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch \n",
        "from torch import nn, optim \n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import itertools\n",
        "import glob \n",
        "from PIL import Image\n",
        "import csv \n",
        "import cv2\n",
        "import re\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.ops.boxes import box_iou\n",
        "import random\n",
        "import torchvision\n",
        "import warnings\n",
        "\n",
        "!pip install --upgrade albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2, ToTensor\n",
        "!git clone https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
        "%cd Ranger-Deep-Learning-Optimizer\n",
        "!pip install -e .\n",
        "from ranger import Ranger  \n",
        "%cd ..\n",
        "#https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1\n",
        "!git clone https://github.com/davda54/sam.git\n",
        "%cd sam\n",
        "import sam\n",
        "print(\"Imported SAM Successfully from github .py file\")\n",
        "%cd ..\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_pckeYPGF0b",
        "outputId": "4ca8716c-6eb8-417c-ef24-fe9cc42a0c98"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCS2doQYbNpf"
      },
      "source": [
        "## To Do Right Now: \n",
        "\n",
        "# Coding/ Technical Stuff to add to paper\n",
        "#Solve CUDA Problem\n",
        "\n",
        "# Visuaulize Eff train loader and try to debug infer probelem.\n",
        "\n",
        "### Once finished with effecient det can create ensemble model that only output boxes that are both predicted or nearly predicted by both models. If mobile net outputs 0 bboxes than run eff det and see if any boudning boxes have a good score.\n",
        "https://github.com/rwightman/efficientdet-pytorch\n",
        "https://github.com/rwightman/efficientdet-pytorch/blob/abba1d5a3611471ac88d49a473f993f72f9e1aba/effdet/config/model_config.py\n",
        "\n",
        "### Find links to put saved model and weights into rasberry pi model\n",
        "\n",
        "# Paper\n",
        "\n",
        "### Work on creating a slide presentation to keep the paper contents.\n",
        "\n",
        "### Move to .py script\n",
        "\n",
        "### Labeling the other data. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4UKDVNnIqQN"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/LatestFruit Defects Dataset .zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/noisy_dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY83cUEFAjoW",
        "outputId": "d5e67144-fa4b-4303-ebab-210fb04cebbb"
      },
      "source": [
        "#For one strawberry batch please drop watermark rows\n",
        "strawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 3 Labeled/FreshStrawberryBatch3Labels.csv\", header = None)\n",
        "strawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 2 Labeled/FreshStrawberriesBatch2Labels.csv\", header = None)\n",
        "strawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 1 Labeled/Strawberrybatch1.csv\", header = None)\n",
        "rottenApple_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch1Labeled/RottenAppleBatch1Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch2Labeled/RottenApplesBatch2Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch3Labaled/RottenApplesBatch3Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch1RottenStrawBerryLabels/RottenStrawberriesBatch1Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch2RottenStrawBerryLabels/RottenStrawBerryBatch2.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch3RottenStrawberrylabel/rottenStrawberryBtch3labels.csv\", header = None)\n",
        "freshApples_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplebtch2label/FreshApplesBatch2LabelsFresh.csv\", header = None)\n",
        "freshApples_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplesBatch1Labels/FreshAppleBatch1Labels.csv\", header = None)\n",
        "rottenTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/Rotten TomatoBatch1/Batch1TomoatosLabelsBbox.csv\", header = None)\n",
        "rottenTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottnTomatoBatch2/RottenTomatyoBatch2Labelss.csv\", header = None)\n",
        "rottenTomato_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatBtch3/RottenTomatoesBatch3Labssles.csv\", header = None)\n",
        "rottenTomato_csv_batch_4 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatoesBatch4/Tomatobatch4labelssRotten.csv\", header = None)\n",
        "freshTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatoesBatch1Labelss/FreshTomatoesLabelsBatch1Labels.csv\", header = None)\n",
        "freshTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatBatch2Labessls/Batch2TomatlabelsFresh.csv\", header = None)\n",
        "\n",
        "strawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_3.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_4.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "\n",
        "#Drop some watermark data for Fresh StrawBerry Batch 1 Labeled images [59, 9, 93]\n",
        "\n",
        "# strawberry_csv_batch_1 = strawberry_csv_batch_1[Image_id not in [\"FreshStrawberries59.jpeg, FreshStrawberries9.jpeg, FreshStrawberries93.jpeg\"]]\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries59.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries9.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries93.jpeg\"].index, inplace = True)\n",
        "freshTomato_csv_batch_1.drop(freshTomato_csv_batch_1[freshTomato_csv_batch_1[\"Image_id\"] == \"Fresh Tomatoes66AddonPart1.jpeg\"].index, inplace = True)\n",
        "\n",
        "strawberry_csv_batch_1 = strawberry_csv_batch_1.reset_index(drop=True)\n",
        "freshTomato_csv_batch_1 = freshTomato_csv_batch_1.reset_index(drop = True)\n",
        "\n",
        "#Stack all the csv files together. \n",
        "list_of_all_dataframes = [strawberry_csv_batch_1, strawberry_csv_batch_2, strawberry_csv_batch_3, rottenApple_csv_batch_1, \n",
        "                          rottenApple_csv_batch_2, rottenApple_csv_batch_3, rottenStrawberry_csv_batch_1, rottenStrawberry_csv_batch_2, \n",
        "                          rottenStrawberry_csv_batch_3, freshApples_csv_batch_2, freshApples_csv_batch_1, rottenTomato_csv_batch_1, \n",
        "                          rottenTomato_csv_batch_2, rottenTomato_csv_batch_3, rottenTomato_csv_batch_4, freshTomato_csv_batch_1, \n",
        "                          freshTomato_csv_batch_2]\n",
        "fruit_df = pd.concat(list_of_all_dataframes, ignore_index = True)\n",
        "\n",
        "total_row_sum_check = 0 \n",
        "for dataframe in list_of_all_dataframes:\n",
        "  total_row_sum_check += dataframe.shape[0]\n",
        "print(\"Checked total rows from all the dataframes combined: {}\".format(total_row_sum_check))\n",
        "\n",
        "def run_dataframe_check():\n",
        "  assert total_row_sum_check == fruit_df.shape[0]\n",
        "  print(\"DataFrame shape: {}\".format(fruit_df.shape))\n",
        "  print(\"Unique Fruit Labels {}\".format(fruit_df[\"Fruit\"].unique()))\n",
        "  print(\"Number of Unique Images {}\".format(len(fruit_df[\"Image_id\"].unique())))\n",
        "\n",
        "run_dataframe_check()\n",
        "\n",
        "#Specify more image types when \n",
        "def more_specific_Image_id(image_id, fruit):\n",
        "  if fruit == \"Bad_Spots\":\n",
        "    if re.search(\"RottenStrawberries\", image_id):\n",
        "      return \"Strawberry_Bad_Spot\"\n",
        "    elif re.search(\"RottenApples\", image_id):\n",
        "      return \"Apple_Bad_Spot\"\n",
        "    elif re.search(\"Rotten Tomatoes\", image_id):\n",
        "      return \"Tomato_Bad_Spot\"\n",
        "    else:\n",
        "      raise ValueError(\"Could not find a match for some of the Image_ids\")\n",
        "\n",
        "  else:\n",
        "    return fruit\n",
        "\n",
        "fruit_df[\"Fruit\"] = fruit_df.apply(lambda row: more_specific_Image_id(row.Image_id, row.Fruit), axis = 1)\n",
        "\n",
        "run_dataframe_check()\n",
        "\n",
        "#Post Processing \n",
        "fruit_df = fruit_df[fruit_df[\"Image_id\"] != \"FreshStrawberries15.jpeg\"]\n",
        "\n",
        "bounding_box_dict = dict()\n",
        "labels_dict = dict()\n",
        "classes = [\"Placeholder\", \"Apples\", \"Strawberry\", \"Tomato\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\", \"Tomato_Bad_Spot\"]\n",
        "classes = [\"Apples\", \"Strawberry\", \"Tomato\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\", \"Tomato_Bad_Spot\"]\n",
        "# classes = [\"Apples\", \"Strawberry\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\"]\n",
        "print(classes)\n",
        "# classes = [\"Bad_Spots\", \"Strawberry\", \"Apples\"]\n",
        "\n",
        "for row_index in range(len(fruit_df)): \n",
        "  current_image_file = fruit_df.iloc[row_index][\"Image_id\"]\n",
        "  if current_image_file not in bounding_box_dict:\n",
        "    bounding_box_dict[current_image_file] = list()\n",
        "    labels_dict[current_image_file] = list()\n",
        "  bounding_box_dict[current_image_file].append(fruit_df.iloc[row_index, 1:5].to_list())\n",
        "  labels_dict[current_image_file].append(classes.index(fruit_df.iloc[row_index, 0]))\n",
        "\n",
        "print(len(bounding_box_dict))\n",
        "print(len(labels_dict))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checked total rows from all the dataframes combined: 1882\n",
            "DataFrame shape: (1882, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Bad_Spots' 'Tomato']\n",
            "Number of Unique Images 532\n",
            "DataFrame shape: (1882, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Apple_Bad_Spot' 'Strawberry_Bad_Spot' 'Tomato'\n",
            " 'Tomato_Bad_Spot']\n",
            "Number of Unique Images 532\n",
            "['Apples', 'Strawberry', 'Tomato', 'Apple_Bad_Spot', 'Strawberry_Bad_Spot', 'Tomato_Bad_Spot']\n",
            "531\n",
            "531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15qhrCwGUPxp"
      },
      "source": [
        "## Class function + util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgVTPiupUTQa"
      },
      "source": [
        "def ffile_path(image_id, full_image_file_paths):\n",
        "  for image_path in full_image_file_paths:\n",
        "    if image_id in image_path:\n",
        "      return image_path\n",
        "\n",
        "def find_area_bb(bb_coord):\n",
        "  bb_coord = bb_coord.numpy()\n",
        "  area_of_each_bb = list()\n",
        "  for pair_of_coord in bb_coord:\n",
        "    area_of_each_bb.append(\n",
        "        (pair_of_coord[2] - pair_of_coord[0]) * (pair_of_coord[3] - pair_of_coord[1])\n",
        "    )\n",
        "  return torch.tensor(area_of_each_bb, dtype=torch.int32)\n",
        "\n",
        "def convert_min_max(bb_coord):\n",
        "  for pair_of_coord in bb_coord:\n",
        "    pair_of_coord[2], pair_of_coord[3] = (pair_of_coord[0] + pair_of_coord[-2]), (pair_of_coord[1] + pair_of_coord[-1])\n",
        "  return bb_coord\n",
        "\n",
        "class FruitDetectDataset(object):\n",
        "  def __init__(self, id_labels, id_bounding_boxes, transforms, mode, noisy_dataset_path = None):\n",
        "\n",
        "    assert len(id_labels) == len(id_bounding_boxes)\n",
        "    assert sorted(id_labels.keys()) == sorted(id_bounding_boxes.keys())\n",
        "    self.imgs_key = sorted(id_labels.keys())\n",
        "\n",
        "    if noisy_dataset_path:\n",
        "      self.noisy_fp = [fp for fp in glob.glob(os.path.join(noisy_dataset_path, \"*.JPEG\"))]\n",
        "      \n",
        "      print(\"Noisy Has been subsetted\")\n",
        "      #Go to this code if you want to subset.\n",
        "      self.noisy_fp = self.noisy_fp[:40]\n",
        "      \n",
        "    else:\n",
        "      print(\"Dataset getting configured without noise loader\")\n",
        "      self.noisy_fp = list()\n",
        "\n",
        "    # np.random.shuffle(self.imgs_key)\n",
        "    if (mode == \"train\"):\n",
        "      self.imgs_key = self.imgs_key[:int(len(self.imgs_key) * 0.8)]\n",
        "      if noisy_dataset_path:\n",
        "        print(\"Extended {} noisy images to train set\".format(int(len(self.noisy_fp) * 0.8)))\n",
        "        self.imgs_key.extend(self.noisy_fp[:int(len(self.noisy_fp) * 0.8)])\n",
        "    elif (mode == \"test\"):\n",
        "      self.imgs_key = self.imgs_key[int(len(self.imgs_key) * 0.8):]\n",
        "      if noisy_dataset_path:\n",
        "        print(\"Extended {} noisy images to test set\".format(int(len(self.noisy_fp) * 0.2)))\n",
        "        self.imgs_key.extend(self.noisy_fp[int(len(self.noisy_fp) * 0.8):])\n",
        "    else:\n",
        "      raise ValueError(\"Invalid Mode choose from train or test\")\n",
        "\n",
        "    self.id_labels = id_labels\n",
        "    self.id_bounding_boxes = id_bounding_boxes\n",
        "    self.full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_key = self.imgs_key[idx]\n",
        "    if img_key in self.noisy_fp:\n",
        "      img_path = img_key\n",
        "      boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "      labels = torch.as_tensor([], dtype = torch.int64)\n",
        "    else:\n",
        "      img_path = ffile_path(self.imgs_key[idx], self.full_image_file_paths) \n",
        "      boxes = convert_min_max(torch.as_tensor(self.id_bounding_boxes[self.imgs_key[idx]], dtype=torch.float32))\n",
        "      labels = torch.as_tensor(self.id_labels[self.imgs_key[idx]], dtype=torch.int64)\n",
        "    \n",
        "    img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = find_area_bb(boxes)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    \n",
        "    #Query about transforms for labels of images\n",
        "    if self.transforms: \n",
        "      sample = {\n",
        "                'image': img,\n",
        "                'bboxes': target['boxes'],\n",
        "                'labels': labels\n",
        "            }\n",
        "\n",
        "      sample = self.transforms(**sample)\n",
        "      img = sample['image']\n",
        "\n",
        "      if img_key not in self.noisy_fp:\n",
        "        target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "    \n",
        "    \n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDh2_pg4J2yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4cde1f-df38-44d1-b206-7a3a93090024"
      },
      "source": [
        "\n",
        "#The Drawing function.\n",
        "# COLORS = [(255, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0)]\n",
        "COLORS = [(0, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0), (255, 0, 0)]\n",
        "\n",
        "def draw_boxes(boxes, labels, image, infer = False, put_text = True):\n",
        "    # read the image with OpenCV\n",
        "    image = image.permute(1, 2, 0).numpy()\n",
        "    if infer:\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    for i, box in enumerate(boxes):\n",
        "        color = COLORS[labels[i] % len(COLORS)]\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (int(box[0]), int(box[1])),\n",
        "            (int(box[2]), int(box[3])),\n",
        "            color, 2\n",
        "        )\n",
        "        if put_text:\n",
        "          cv2.putText(image, classes[labels[i]], (int(box[0]), int(box[1]-5)),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n",
        "                      lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "# Albumentations\n",
        "def get_transforms(mode):\n",
        "  if (mode == \"train\"):\n",
        "    return A.Compose([\n",
        "                      A.OneOf([\n",
        "                      A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                     val_shift_limit=0.2, p=0.9),\n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "                      A.HorizontalFlip(),\n",
        "                      A.VerticalFlip(), \n",
        "                      # A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), p=1),\n",
        "                      # ToTensor(),\n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"test\"):\n",
        "    return A.Compose([\n",
        "                      # A.Resize(512, 512), \n",
        "                      # A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                      # std=(0.229, 0.224, 0.225), p=1),\n",
        "                      # ToTensor()\n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"effdet_train\"):\n",
        "    return A.Compose([\n",
        "                      A.OneOf([\n",
        "                      A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                     val_shift_limit=0.2, p=0.9),\n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "                      A.HorizontalFlip(),\n",
        "                      A.VerticalFlip(), \n",
        "                      A.Resize(height = 512, width=512), \n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"effdet_test\"):\n",
        "    return A.Compose([\n",
        "                      A.Resize(height = 512, width = 512), \n",
        "                      ToTensorV2()])\n",
        "  else:\n",
        "    raise ValueError(\"mode is wrong value can either be train or test\")\n",
        "\n",
        "class NoiseDataset(object):\n",
        "\n",
        "  def __init__(self, noise_file_path, size, camera_size):\n",
        "\n",
        "    self.size = size\n",
        "    self.noise_file_path = [fp for fp in glob.glob(os.path.join(noise_file_path, \"*.JPEG\"))]\n",
        "    self.transforms = transforms.Compose([\n",
        "                                          transforms.Resize((camera_size, camera_size)), \n",
        "                                          transforms.ToTensor()])\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    current_file_path = self.noise_file_path[idx]\n",
        "    img = Image.open(current_file_path).convert(\"RGB\")\n",
        "\n",
        "    img = self.transforms(img)\n",
        "    return img\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.size:\n",
        "      return self.size\n",
        "    return len(self.noise_file_path)\n",
        "    \n",
        "\n",
        "#Using this stack overflow (https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders)\n",
        "#(Suppose for example, you want to create batches of a list of varying dimension tensors. The below code pads sequences with 0 until the maximum sequence size of the batch,)\n",
        "#Collate_fn is a function that is used to process your batches before you pass it to dataloader. In my case since I have different sized images I need a way to stack batches b/c torch.stack won't work.\n",
        "#So I use zip which can accept tensors of different lengths and make them stacked with the size of the lowest length list given. Therefore stacking all the images in a batch \n",
        "#Successfully unlike torch.stack and doing that processing to every batch makes collate_fn vital since I have different image sizes.\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return tuple([list(a) for a in zip(*batch)])\n",
        "    # return tuple(zip(*batch))\n",
        "\n",
        "train_batch_size = 2\n",
        "test_batch_size = 2\n",
        "noise_path = \"/content/noisy_dataset\"\n",
        "\n",
        "train_dataset = FruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"train\"), mode = \"train\", noisy_dataset_path=noise_path)                               \n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= collate_fn)\n",
        "\n",
        "test_dataset = FruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"test\"), mode = \"test\", noisy_dataset_path=noise_path)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = test_batch_size, shuffle = True, collate_fn= collate_fn)\n",
        "\n",
        "noise_dataset = NoiseDataset(noise_path, 100, 512)\n",
        "noise_loader = torch.utils.data.DataLoader(noise_dataset, batch_size = test_batch_size, shuffle = True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Noisy Has been subsetted\n",
            "Extended 32 noisy images to train set\n",
            "Noisy Has been subsetted\n",
            "Extended 8 noisy images to test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "-uf9fdpPK-SQ",
        "outputId": "181f8557-cec8-472e-9729-bbaa35f2022d"
      },
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(2):\n",
        "    ax = fig.add_subplot(2, 2/2, idx+1, xticks=[], yticks=[])\n",
        "    image = draw_boxes(labels[idx][\"boxes\"], labels[idx][\"labels\"], images[idx])\n",
        "    plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A0EkOWcPSX0"
      },
      "source": [
        "## What we learned\n",
        "is that the model outputs losses when in train mode \n",
        "when in model.eval model, the model code then return only a prediction with no losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NQrBD22VWxD"
      },
      "source": [
        "def calculate_metrics(target_box,predictions_box,scores, device):\n",
        "\n",
        "    #Get most confident boxes first and least confident last\n",
        "    predictions_box = predictions_box[scores.argsort().flip(-1)]\n",
        "    iou_mat = box_iou(target_box,predictions_box)\n",
        "    #return a one by one matrix that is form (target_box, prediction_box) or (1, 1)\n",
        "    target_boxes_count, prediction_boxes_count = iou_mat.shape\n",
        "    \n",
        "    mAP_Matrix = torch.zeros_like(iou_mat)\n",
        "    # if not matrix coordinates that relate to nothing.\n",
        "    if not iou_mat[:,0].eq(0.).all():\n",
        "      index_of_biggest_iou = iou_mat[:,0].argsort()[-1]\n",
        "      mAP_Matrix[index_of_biggest_iou,0] = 1\n",
        "\n",
        "    for pr_idx in range(1,prediction_boxes_count):\n",
        "        not_assigned = torch.logical_not(mAP_Matrix[:,:pr_idx].sum(1)).long()\n",
        "        targets = not_assigned * iou_mat[:,pr_idx]\n",
        "\n",
        "        if targets.eq(0).all():\n",
        "            continue\n",
        "\n",
        "        pivot = targets.argsort()[-1]\n",
        "        mAP_Matrix[pivot,pr_idx] = 1\n",
        "\n",
        "    # mAP calculation\n",
        "    tp = mAP_Matrix.sum()\n",
        "    fp = mAP_Matrix.sum(0).eq(0).sum()\n",
        "    fn = mAP_Matrix.sum(1).eq(0).sum()\n",
        "\n",
        "    mAP = tp / (tp+fp)\n",
        "    mAR = tp / (tp+fn)\n",
        "\n",
        "    return mAP, mAR\n",
        "\n",
        "def run_metrics_for_batch(output, targets, mAP, mAR, missed_images, device):\n",
        "  for pos_in_batch, image_pred in enumerate(output):\n",
        "    assert (len(image_pred[\"boxes\"]) == len(image_pred[\"labels\"]) == len(image_pred[\"scores\"]))\n",
        "    if len(image_pred[\"boxes\"]) != 0:\n",
        "      curr_mAP, curr_mAR = calculate_metrics(targets[pos_in_batch][\"boxes\"], output[pos_in_batch][\"boxes\"], output[pos_in_batch][\"scores\"], device)\n",
        "      mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
        "    else:\n",
        "      missed_images += 1 \n",
        "  \n",
        "  return mAP, mAR, missed_images\n",
        "\n",
        "# def run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device):\n",
        "#     assert (len(scores) == len(classification) == len(transformed_anchors))\n",
        "#     if len(transformed_anchors) != 0:\n",
        "#       curr_mAP, curr_mAR = calculate_metrics(targets[0][:, :4], transformed_anchors, scores, device)\n",
        "#       mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
        "#     else:\n",
        "#       missed_images += 1 \n",
        "      \n",
        "#     return mAP, mAR, missed_images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB_w3zeIOa8X"
      },
      "source": [
        "def train(net, epochs, train_loader, test_loader, noise_loader, lr, weight_decay, \n",
        "          print_every = 6, lo_test_dataset = len(test_dataset), lo_train_dataset = len(train_dataset),\n",
        "          lo_noise_dataset = len(noise_dataset)):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Check which parameters can calculate gradients. \n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "\n",
        "    # optimizer = Ranger(net.parameters(), lr = lr, weight_decay= weight_decay)\n",
        "    base_optimizer = Ranger\n",
        "    optimizer = sam.SAM(net.parameters(), base_optimizer, lr = lr, weight_decay = weight_decay)\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
        "\n",
        "    net.to(device)\n",
        "    print(\"Device: {}\".format(device))\n",
        "    print(\"Optimizer: {}\".format(optimizer))\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        \n",
        "        train_loss = steps = train_mAP = train_mAR = missed_train_images = 0\n",
        "        \n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "            net.train()\n",
        "\n",
        "            steps += 1\n",
        "            \n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            net.eval()\n",
        "            train_mAP, train_mAR, missed_train_images = run_metrics_for_batch(net(images), targets, train_mAP, train_mAR, missed_train_images, device)\n",
        "            net.train()\n",
        "\n",
        "            losses.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            optimizer.first_step(zero_grad = True)\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            losses.backward()\n",
        "            optimizer.second_step(zero_grad = True)\n",
        "\n",
        "            train_loss +=  losses.item()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (steps % print_every) == 0:\n",
        "\n",
        "              with torch.no_grad():\n",
        "                test_mAP = test_mAR = missed_test_images = test_loss = correct_missed_images = 0\n",
        "\n",
        "                for noise_images in noise_loader:\n",
        "                  \n",
        "                  net.eval()\n",
        "                  if device == torch.device(\"cuda\"):\n",
        "                    noise_images = [noise_image.to(device) for noise_image in noise_images]\n",
        "                  \n",
        "                  output = net(noise_images)\n",
        "\n",
        "                  for ii in range(len(output)):\n",
        "                    if len(output[ii][\"boxes\"]) == 0:\n",
        "                      correct_missed_images += 1\n",
        "\n",
        "                for images, targets in test_loader:\n",
        "\n",
        "                  net.eval()\n",
        "                  if device == torch.device(\"cuda\"):\n",
        "                    images = [image.to(device) for image in images]\n",
        "                    targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "                  output = net(images)\n",
        "                  test_mAP, test_mAR, missed_test_images = run_metrics_for_batch(output, targets, test_mAP, test_mAR, missed_test_images, device)\n",
        "\n",
        "                  net.train()\n",
        "                  test_loss_dict = net(images, targets)\n",
        "                  test_losses = sum(loss for loss in test_loss_dict.values())\n",
        "                  test_loss += test_losses.item()\n",
        "\n",
        "                for param_group in optimizer.param_groups:\n",
        "                  learning_rate_extract = param_group[\"lr\"]\n",
        "                print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Test_loss: {:0.2f} | Test mAP: {:0.2f}% | Missed Test Images: {} | Seperate Noise Loader: {} / {}\".format(\n",
        "                    epoch + 1, epochs, steps, learning_rate_extract, train_loss, test_loss,  \n",
        "                    (test_mAP / float(lo_test_dataset)) * 100.,missed_test_images,\n",
        "                    correct_missed_images, lo_noise_dataset))\n",
        "\n",
        "              assert (steps % print_every) == 0\n",
        "              train_loss = 0\n",
        "                 \n",
        "        print(\"\\n Epoch {} Final Train mAP: {:0.2f}% | Epoch {} Final Missed Train Images: {} out of {} images \\n\".format(\n",
        "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, missed_train_images, lo_train_dataset\n",
        "        ))\n",
        "    \n",
        "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))\n",
        "\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYAu4FDknTwH"
      },
      "source": [
        "# Effecient Det Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWHkD1yEt1LY"
      },
      "source": [
        "!pip install timm\n",
        "!pip install effdet\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, unwrap_bench\n",
        "from effdet.efficientdet import HeadNet\n",
        "from effdet.data.transforms import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EHpxq96t-Ii"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "MAX_NUM_INSTANCES = 100\n",
        "class DetectionFastCollate:\n",
        "    \"\"\" A detection specific, optimized collate function w/ a bit of state.\n",
        "    Optionally performs anchor labelling. Doing this here offloads some work from the\n",
        "    GPU and the main training process thread and increases the load on the dataloader\n",
        "    threads.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            instance_keys=None,\n",
        "            instance_shapes=None,\n",
        "            instance_fill=-1,\n",
        "            max_instances=MAX_NUM_INSTANCES,\n",
        "            anchor_labeler=None,\n",
        "    ):\n",
        "        instance_keys = instance_keys or {'bbox', 'bbox_ignore', 'cls'}\n",
        "        instance_shapes = instance_shapes or dict(\n",
        "            bbox=(max_instances, 4), bbox_ignore=(max_instances, 4), cls=(max_instances,))\n",
        "        self.instance_info = {k: dict(fill=instance_fill, shape=instance_shapes[k]) for k in instance_keys}\n",
        "        self.max_instances = max_instances\n",
        "        self.anchor_labeler = anchor_labeler\n",
        "\n",
        "    def __call__(self, batch):\n",
        "\n",
        "        # print(batch[0][2])\n",
        "        # print(type(batch[0][2]))\n",
        "        \n",
        "        batch_size = len(batch)\n",
        "        target = dict()\n",
        "        labeler_outputs = dict()\n",
        "        img_tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n",
        "        mAP_translated_boxes = list()\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            img_tensor[i] += torch.from_numpy(batch[i][0])\n",
        "            mAP_translated_boxes.append(torch.from_numpy(batch[i][2]))\n",
        "            labeler_inputs = {}\n",
        "            for tk, tv in batch[i][1].items():\n",
        "                instance_info = self.instance_info.get(tk, None)\n",
        "                if instance_info is not None:\n",
        "                    # target tensor is associated with a detection instance\n",
        "                    tv = torch.from_numpy(tv).to(dtype=torch.float32)\n",
        "                    if self.anchor_labeler is None:\n",
        "                        if i == 0:\n",
        "                            shape = (batch_size,) + instance_info['shape']\n",
        "                            target_tensor = torch.full(shape, instance_info['fill'], dtype=torch.float32)\n",
        "                            target[tk] = target_tensor\n",
        "                        else:\n",
        "                            target_tensor = target[tk]\n",
        "                        num_elem = min(tv.shape[0], self.max_instances)\n",
        "                        target_tensor[i, 0:num_elem] = tv[0:num_elem]\n",
        "                    else:\n",
        "                        # no need to pass gt tensors through when labeler in use\n",
        "                        if tk in ('bbox', 'cls'):\n",
        "                            labeler_inputs[tk] = tv\n",
        "                else:\n",
        "                    # target tensor is an image-level annotation / metadata\n",
        "                    if i == 0:\n",
        "                        # first batch elem, create destination tensors\n",
        "                        if isinstance(tv, (tuple, list)):\n",
        "                            # per batch elem sequence\n",
        "                            shape = (batch_size, len(tv))\n",
        "                            dtype = torch.float32 if isinstance(tv[0], (float, np.floating)) else torch.int32\n",
        "                        else:\n",
        "                            # per batch elem scalar\n",
        "                            shape = batch_size,\n",
        "                            dtype = torch.float32 if isinstance(tv, (float, np.floating)) else torch.int64\n",
        "                        target_tensor = torch.zeros(shape, dtype=dtype)\n",
        "                        target[tk] = target_tensor\n",
        "                    else:\n",
        "                        target_tensor = target[tk]\n",
        "                    target_tensor[i] = torch.tensor(tv, dtype=target_tensor.dtype)\n",
        "\n",
        "            if self.anchor_labeler is not None:\n",
        "                cls_targets, box_targets, num_positives = self.anchor_labeler.label_anchors(\n",
        "                    labeler_inputs['bbox'], labeler_inputs['cls'], filter_valid=False)\n",
        "                if i == 0:\n",
        "                    # first batch elem, create destination tensors, separate key per level\n",
        "                    for j, (ct, bt) in enumerate(zip(cls_targets, box_targets)):\n",
        "                        labeler_outputs[f'label_cls_{j}'] = torch.zeros(\n",
        "                            (batch_size,) + ct.shape, dtype=torch.int64)\n",
        "                        labeler_outputs[f'label_bbox_{j}'] = torch.zeros(\n",
        "                            (batch_size,) + bt.shape, dtype=torch.float32)\n",
        "                    labeler_outputs['label_num_positives'] = torch.zeros(batch_size)\n",
        "                for j, (ct, bt) in enumerate(zip(cls_targets, box_targets)):\n",
        "                    labeler_outputs[f'label_cls_{j}'][i] = ct\n",
        "                    labeler_outputs[f'label_bbox_{j}'][i] = bt\n",
        "                labeler_outputs['label_num_positives'][i] = num_positives\n",
        "        if labeler_outputs:\n",
        "            target.update(labeler_outputs)\n",
        "\n",
        "        return img_tensor, target, mAP_translated_boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjFx0ZuYFca"
      },
      "source": [
        "class EffdetFruitDetectDataset(object):\n",
        "  def __init__(self, id_labels, id_bounding_boxes, transforms, mode, yxyx = True):\n",
        "\n",
        "    assert len(id_labels) == len(id_bounding_boxes)\n",
        "    assert sorted(id_labels.keys()) == sorted(id_bounding_boxes.keys())\n",
        "    self.imgs_key = sorted(id_labels.keys())\n",
        "\n",
        "    np.random.shuffle(self.imgs_key)\n",
        "    if (mode == \"train\"):\n",
        "      self.imgs_key = self.imgs_key[:int(len(self.imgs_key) * 0.8)]\n",
        "    else:\n",
        "      self.imgs_key = self.imgs_key[int(len(self.imgs_key) * 0.8):]\n",
        "\n",
        "    self.id_labels = id_labels\n",
        "    self.id_bounding_boxes = id_bounding_boxes\n",
        "    self.full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "\n",
        "    self.transforms = transforms\n",
        "    self.yxyx = yxyx\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_path = ffile_path(self.imgs_key[idx], self.full_image_file_paths) \n",
        "    # img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    boxes = convert_min_max(self.id_bounding_boxes[self.imgs_key[idx]])\n",
        "    #boxes in xyxy format\n",
        "    # mAP_testable_bbox = np.array(boxes)\n",
        "    \n",
        "    labels = self.id_labels[self.imgs_key[idx]]\n",
        "    \n",
        "    reference = {}\n",
        "    reference['boxes'] = boxes\n",
        "    reference['labels'] = labels\n",
        "\n",
        "    width, height = img.size\n",
        "    target = dict(img_idx=idx, img_size=(width, height))\n",
        "\n",
        "    bboxes = []\n",
        "    labels = []\n",
        "\n",
        "    ann ={}\n",
        "    ann['bbox'] = boxes\n",
        "    ann['label'] = labels\n",
        "\n",
        "    for ii, ann in enumerate(boxes):\n",
        "            ignore = False\n",
        "            x1, y1, x2, y2 = ann\n",
        "            label = reference[\"labels\"][ii]\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            if w < 1 or h < 1:\n",
        "                ignore = True\n",
        "\n",
        "            bbox = ann\n",
        "\n",
        "            if self.yxyx:\n",
        "              bbox = [y1, x1, y2, x2]\n",
        "            \n",
        "            bboxes.append(bbox)\n",
        "            labels.append(label)\n",
        "\n",
        "\n",
        "    bboxes = np.array(bboxes, ndmin=2, dtype=np.float32) - 1\n",
        "\n",
        "\n",
        "    labels = np.array(labels, dtype=np.float32)\n",
        "\n",
        "    ann = dict(\n",
        "            bbox=bboxes.astype(np.float32),\n",
        "            cls=labels.astype(np.int64))\n",
        "\n",
        "    target.update(ann)\n",
        "\n",
        "    if self.transforms is not None:\n",
        "        img, target = self.transforms(img, target)\n",
        "    \n",
        "    mAP_testable_bbox = target[\"bbox\"]\n",
        "    mAP_testable_bbox[:, [0, 1]] = mAP_testable_bbox[:, [1, 0]]\n",
        "    mAP_testable_bbox[:, [2, 3]] = mAP_testable_bbox[:, [3, 2]]\n",
        "    \n",
        "    #mAP_testable_bbox = target[\"bbox\"]\n",
        "\n",
        "    return img, target, mAP_testable_bbox\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs_key)\n",
        "\n",
        "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
        "IMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)\n",
        "IMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)\n",
        "\n",
        "def run_metrics_for_batch_for_effdet(output, targets, mAP, mAR, threshold, full_threshold, device):\n",
        "    \n",
        "  for batch_num, _ in enumerate(output):\n",
        "    \n",
        "    boxes, scores = output[batch_num][:, :4], output[batch_num][:, 4]\n",
        "    curr_target = targets[batch_num]\n",
        "    if len(boxes) == 0:\n",
        "      raise RuntimeError(\"Cannot run metrics on an empty box\")\n",
        "    \n",
        "    boxes, scores, full_threshold = run_thresholding_for_effdet(boxes, scores, threshold, full_threshold) \n",
        "    curr_mAP, curr_mAR = calculate_metrics(curr_target, boxes, scores, device)\n",
        "    mAP += curr_mAP\n",
        "    if not torch.isnan(curr_mAR):\n",
        "        mAR += curr_mAR \n",
        "  \n",
        "  return mAP, mAR, full_threshold\n",
        "\n",
        "def run_thresholding_for_effdet(boxes, scores, threshold, full_threshold):\n",
        "    zipped_box_scores = list(zip(boxes, scores))\n",
        "    kept_indices = list()\n",
        "    for ii, data in enumerate(zipped_box_scores):\n",
        "        _, scores = data\n",
        "        if scores > threshold:\n",
        "            kept_indices.append(ii)       \n",
        "    if len(kept_indices) == 0:\n",
        "        kept_indices.append(0)\n",
        "        full_threshold += 1 \n",
        "    zipped_box_scores = [data for ii, data in enumerate(zipped_box_scores) if ii in kept_indices]\n",
        "    boxes, scores = zip(*zipped_box_scores)\n",
        "    boxes, scores = torch.stack(list(boxes)), torch.stack(list(scores))\n",
        "    \n",
        "    return boxes, scores, full_threshold\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujHCSsyzvcsT"
      },
      "source": [
        "def define_effdet_parameters():\n",
        "  global image_size\n",
        "  global model_name\n",
        "  global num_cls\n",
        "  global pretrained\n",
        "  global pretrained_backbone\n",
        "  global redundant_bias\n",
        "  global label_smoothing\n",
        "  global legacy_focal \n",
        "  global jit_loss\n",
        "  global soft_nms\n",
        "  global bench_labeler\n",
        "\n",
        "  image_size = 640\n",
        "  model_name = \"efficientdet_q1\"\n",
        "  num_cls = 6\n",
        "  pretrained=True\n",
        "  pretrained_backbone=True\n",
        "  redundant_bias=None\n",
        "  label_smoothing=None\n",
        "  legacy_focal=None\n",
        "  jit_loss=None\n",
        "  soft_nms=None\n",
        "  bench_labeler=None\n",
        "\n",
        "define_effdet_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmonKCf2YtY2"
      },
      "source": [
        "from effdet.anchors import Anchors, AnchorLabeler\n",
        "from effdet.factory import create_model, create_model_from_config\n",
        "from timm.utils import *\n",
        "from contextlib import suppress\n",
        "from collections import OrderedDict\n",
        "from timm.models.layers import set_layer_config\n",
        "\n",
        "model = create_model(\n",
        "  model_name,\n",
        "  bench_task='train',\n",
        "  num_classes=num_cls,\n",
        "  pretrained=pretrained,\n",
        "  pretrained_backbone= pretrained_backbone,\n",
        "  redundant_bias=redundant_bias,\n",
        "  label_smoothing= label_smoothing,\n",
        "  legacy_focal=legacy_focal,\n",
        "  jit_loss=jit_loss,\n",
        "  soft_nms=soft_nms,\n",
        "  bench_labeler=bench_labeler,\n",
        "  checkpoint_path='',\n",
        "  max_det_per_image = 5, #This value should be changed. The amount of images effdet predicts. May affect mAP and mAR. \n",
        ")\n",
        "\n",
        "amp_autocast = suppress\n",
        "model_config = model.config\n",
        "anchor_labeler = AnchorLabeler(\n",
        "            Anchors.from_config(model_config), num_cls, match_threshold=0.5)\n",
        "\n",
        "transform = transforms_coco_eval(\n",
        "            (image_size, image_size),\n",
        "            interpolation='bilinear',\n",
        "            use_prefetcher=True,\n",
        "            fill_color='mean',\n",
        "            mean=IMAGENET_DEFAULT_MEAN,\n",
        "            std=IMAGENET_DEFAULT_STD)\n",
        "\n",
        "train_batch_size = 2\n",
        "valid_batch_size = 2\n",
        "\n",
        "eff_train_dataset = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, transform, mode = \"train\")\n",
        "eff_train_loader = torch.utils.data.DataLoader(eff_train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= DetectionFastCollate(anchor_labeler=anchor_labeler))\n",
        "\n",
        "eff_test_dataset = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, transform, mode = \"test\")\n",
        "eff_test_loader = torch.utils.data.DataLoader(eff_test_dataset, batch_size = valid_batch_size, shuffle = True, collate_fn = DetectionFastCollate(anchor_labeler=anchor_labeler))\n",
        "# eff_test_dataset_2 = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, transform, mode = \"test\", yxyx=False)\n",
        "# eff_test_loader_2 = torch.utils.data.DataLoader(eff_test_dataset_2, batch_size = valid_batch_size, shuffle = True, collate_fn = DetectionFastCollate(anchor_labeler=anchor_labeler))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbXEJySC236L"
      },
      "source": [
        "def train_epoch(\n",
        "        epoch, model, loader, optimizer, \n",
        "        lr_scheduler=None, saver=None, output_dir='',  loss_scaler=None, model_ema=None):\n",
        "  \n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    else:\n",
        "      warnings.warn(\"Function does not support handling models on CPU\")\n",
        "\n",
        "    batch_time_m = AverageMeter()\n",
        "    data_time_m = AverageMeter()\n",
        "    losses_m = AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    last_idx = len(loader) - 1\n",
        "    num_updates = epoch * len(loader)\n",
        "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
        "    for batch_idx, (input, target, _) in enumerate(loader):\n",
        "        last_batch = batch_idx == last_idx\n",
        "        data_time_m.update(time.time() - end)\n",
        "\n",
        "        input = input.cuda().float()\n",
        "        target2={}\n",
        "        for k,v in target.items():\n",
        "          target2[k]=v.cuda()\n",
        "  \n",
        "\n",
        "        with amp_autocast():\n",
        "          output = model(input, target2)\n",
        "        loss = output['loss']\n",
        "\n",
        "        losses_m.update(loss.item(), input.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        " \n",
        "        loss.backward()\n",
        "        # optimizer.first_step(zero_grad = True)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "\n",
        "        # with amp_autocast():\n",
        "        #   output = model(input, target2)\n",
        "        # loss = output['loss']\n",
        "        # loss.backward()\n",
        "        # optimizer.second_step(zero_grad = True)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        num_updates += 1\n",
        "        # scheduler.step()\n",
        "\n",
        "        batch_time_m.update(time.time() - end)\n",
        "        if last_batch or batch_idx % 10 == 0:\n",
        "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
        "            lr = sum(lrl) / len(lrl)\n",
        "\n",
        "            print(\n",
        "                'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '\n",
        "                'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f})  '\n",
        "                'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '\n",
        "                '({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
        "                'LR: {lr:.3e}  '\n",
        "                'Data: {data_time.val:.3f} ({data_time.avg:.3f})'.format(\n",
        "                    epoch,\n",
        "                    batch_idx, len(loader),\n",
        "                    100. * batch_idx / last_idx,\n",
        "                    loss=losses_m,\n",
        "                    batch_time=batch_time_m,\n",
        "                    rate=input.size(0) * 1 / batch_time_m.val,\n",
        "                    rate_avg=input.size(0) * 1  / batch_time_m.avg,\n",
        "                    lr=lr,\n",
        "                    data_time=data_time_m))\n",
        "\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "\n",
        "\n",
        "    if hasattr(optimizer, 'sync_lookahead'):\n",
        "        optimizer.sync_lookahead()\n",
        "\n",
        "    return OrderedDict([('loss', losses_m.avg)])\n",
        "\n",
        "from itertools import chain\n",
        "\n",
        "#Changed the evaluator argument.\n",
        "def validate(bench, model, loader, valid_batch_size, threshold, args, log_suffix=''):\n",
        "    batch_time_m = AverageMeter()\n",
        "    losses_m = AverageMeter()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    last_idx = len(loader) - 1\n",
        "    with torch.no_grad():\n",
        "        valid_mAP, valid_mAR, full_threshold, steps = 0, 0, 0, 0\n",
        "      #Changed this line \n",
        "        for (input, target, mAP_formatted_target) in loader:\n",
        "            steps += 1 \n",
        "            input = input.cuda().float()\n",
        "            target2={}\n",
        "            for k,v in target.items():\n",
        "              target2[k]=v.cuda()\n",
        "            \n",
        "            #Get mAP and mAR (Can possibly add some thresholding to this value)\n",
        "            pred = bench(input)\n",
        "\n",
        "            mAP_formatted_target = [tensor.cuda() for tensor in mAP_formatted_target]\n",
        "            valid_mAP, valid_mAR, full_threshold = run_metrics_for_batch_for_effdet(pred, mAP_formatted_target, valid_mAP, valid_mAR, \n",
        "                                                                    threshold, full_threshold, device)\n",
        "\n",
        "            #Get Test Loss\n",
        "            output = model(input, target2)\n",
        "            loss = output['loss']\n",
        "            reduced_loss = loss.data\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "            losses_m.update(reduced_loss.item(), input.size(0))\n",
        "\n",
        "            batch_time_m.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "    num_of_images_tested = valid_batch_size * steps\n",
        "    metrics = OrderedDict([('loss', losses_m.avg), (\"Valid mAP\", str(int(valid_mAP / num_of_images_tested * 100)) + \"%\"), (\"Valid mAR\", str(int(valid_mAR / num_of_images_tested * 100)) + \"%\"),\n",
        "                           ('# of Full Thresh', full_threshold)])\n",
        "    return metrics\n",
        "\n",
        "def create_effdet_model(checkpoint):\n",
        "  pretrained=True\n",
        "  pretrained = pretrained or not checkpoint  # might as well try to validate something\n",
        "  extra_args = {\"image_size\":(image_size, image_size)}\n",
        "  with set_layer_config(scriptable='store_true'):\n",
        "    bench = create_model(\n",
        "              model_name,\n",
        "              bench_task='predict',\n",
        "              num_classes=num_cls,\n",
        "              pretrained=pretrained,\n",
        "              pretrained_backbone=pretrained_backbone,\n",
        "              redundant_bias=redundant_bias,\n",
        "              label_smoothing=label_smoothing,\n",
        "              legacy_focal=legacy_focal,\n",
        "              jit_loss=jit_loss,\n",
        "              soft_nms=soft_nms,\n",
        "              bench_labeler=bench_labeler,\n",
        "              checkpoint_path=checkpoint, \n",
        "              max_det_per_image = 5, \n",
        "              **extra_args\n",
        "          )\n",
        "    \n",
        "  bench.cuda()\n",
        "  bench.eval()\n",
        "\n",
        "  return bench\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIcjkG9PQwNR"
      },
      "source": [
        "Train: 6 [ 211/212 (100%)]  Loss:  0.727989 (0.7556)  Time: 0.172s,   11.60/s  (0.175s,   11.41/s)  LR: 1.000e-04  Data: 0.029 (0.031)\n",
        "train OrderedDict([('loss', 0.7555575621015621)])\n",
        "test OrderedDict([('loss', 1.0847256957927598), ('Valid mAP', '47%'), ('Valid mAR', '77%')])\n",
        "\n",
        "\n",
        "Ranger:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "edl4rQVv3Myj",
        "outputId": "2759826b-4355-4a30-9f0f-19e2f5e921f0"
      },
      "source": [
        "from timm.utils import get_outdir\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "saving_folder = \"exp21\"\n",
        "\n",
        "output_dir = get_outdir('/content', 'train', saving_folder)\n",
        "mAP_helper_checkpoints_dir = get_outdir('/content', 'mAP', saving_folder)\n",
        "eval_metric='loss'\n",
        "decreasing = True if eval_metric == 'loss' else False\n",
        "\n",
        "optimizer= torch.optim.AdamW(model.parameters(), lr=.0001)\n",
        "# base_optimizer = torch.optim.AdamW\n",
        "# optimizer = sam.SAM(model.parameters(), base_optimizer, lr = .0001)\n",
        "\n",
        "# optimizer = Ranger(model.parameters(), lr = 0.0001, weight_decay = 1e-5)\n",
        "\n",
        "saver = CheckpointSaver(\n",
        "model, optimizer, args=None, model_ema=None, amp_scaler=None,\n",
        "checkpoint_dir=output_dir, decreasing=decreasing, unwrap_fn = unwrap_bench)\n",
        "\n",
        "saver_for_mAP_calc = CheckpointSaver(\n",
        "    model, optimizer, args = None, model_ema = None, amp_scaler = None,\n",
        "    checkpoint_dir = mAP_helper_checkpoints_dir, decreasing=decreasing, unwrap_fn = unwrap_bench)\n",
        "\n",
        "\n",
        "#Change this for epochs.\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "\n",
        "  train_eval_metrics= train_epoch(\n",
        "          epoch , model, eff_train_loader, optimizer,saver=saver)\n",
        "  print('train',train_eval_metrics)\n",
        "\n",
        "  #Saved prev trained model\n",
        "  saver_for_mAP_calc.save_checkpoint(epoch = epoch, metric = train_eval_metrics[\"loss\"])\n",
        "\n",
        "  checkpoint = os.path.join(mAP_helper_checkpoints_dir,\"last.pth.tar\")\n",
        "  bench = create_effdet_model(checkpoint)\n",
        "  valid_eval_metrics = validate(bench, model, eff_test_loader, valid_batch_size, 0.3, args = None)\n",
        "\n",
        "  #Make sure to uncomment this line for saving the model\n",
        "  best_metric, best_epoch = saver.save_checkpoint(epoch=epoch,metric=valid_eval_metrics['loss'])\n",
        "\n",
        "  print('test',valid_eval_metrics)\n",
        "  print('')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 1 [   0/212 (  0%)]  Loss:  1.449292 (1.4493)  Time: 0.449s,    4.45/s  (0.449s,    4.45/s)  LR: 1.000e-04  Data: 0.042 (0.042)\n",
            "Train: 1 [  10/212 (  5%)]  Loss:  0.594213 (1.1361)  Time: 0.346s,    5.77/s  (0.363s,    5.51/s)  LR: 1.000e-04  Data: 0.040 (0.042)\n",
            "Train: 1 [  20/212 (  9%)]  Loss:  1.271463 (1.1647)  Time: 0.359s,    5.57/s  (0.359s,    5.56/s)  LR: 1.000e-04  Data: 0.041 (0.044)\n",
            "Train: 1 [  30/212 ( 14%)]  Loss:  1.389009 (1.1443)  Time: 0.348s,    5.75/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.039 (0.044)\n",
            "Train: 1 [  40/212 ( 19%)]  Loss:  0.839429 (1.1272)  Time: 0.353s,    5.66/s  (0.356s,    5.61/s)  LR: 1.000e-04  Data: 0.038 (0.044)\n",
            "Train: 1 [  50/212 ( 24%)]  Loss:  1.189385 (1.1245)  Time: 0.343s,    5.83/s  (0.356s,    5.62/s)  LR: 1.000e-04  Data: 0.036 (0.044)\n",
            "Train: 1 [  60/212 ( 28%)]  Loss:  1.367401 (1.1168)  Time: 0.372s,    5.38/s  (0.356s,    5.62/s)  LR: 1.000e-04  Data: 0.057 (0.044)\n",
            "Train: 1 [  70/212 ( 33%)]  Loss:  0.653200 (1.0777)  Time: 0.350s,    5.71/s  (0.356s,    5.62/s)  LR: 1.000e-04  Data: 0.040 (0.044)\n",
            "Train: 1 [  80/212 ( 38%)]  Loss:  1.131307 (1.0617)  Time: 0.359s,    5.58/s  (0.356s,    5.62/s)  LR: 1.000e-04  Data: 0.043 (0.044)\n",
            "Train: 1 [  90/212 ( 43%)]  Loss:  1.106156 (1.0608)  Time: 0.362s,    5.53/s  (0.356s,    5.62/s)  LR: 1.000e-04  Data: 0.047 (0.044)\n",
            "Train: 1 [ 100/212 ( 47%)]  Loss:  1.159486 (1.0743)  Time: 0.355s,    5.63/s  (0.356s,    5.62/s)  LR: 1.000e-04  Data: 0.041 (0.044)\n",
            "Train: 1 [ 110/212 ( 52%)]  Loss:  1.194459 (1.0646)  Time: 0.364s,    5.50/s  (0.356s,    5.62/s)  LR: 1.000e-04  Data: 0.050 (0.044)\n",
            "Train: 1 [ 120/212 ( 57%)]  Loss:  1.116911 (1.0655)  Time: 0.353s,    5.67/s  (0.357s,    5.61/s)  LR: 1.000e-04  Data: 0.041 (0.044)\n",
            "Train: 1 [ 130/212 ( 62%)]  Loss:  0.657976 (1.0646)  Time: 0.359s,    5.58/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.040 (0.044)\n",
            "Train: 1 [ 140/212 ( 66%)]  Loss:  1.160197 (1.0505)  Time: 0.354s,    5.65/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.043 (0.044)\n",
            "Train: 1 [ 150/212 ( 71%)]  Loss:  0.766446 (1.0519)  Time: 0.354s,    5.65/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.042 (0.044)\n",
            "Train: 1 [ 160/212 ( 76%)]  Loss:  0.766527 (1.0489)  Time: 0.356s,    5.62/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.043 (0.044)\n",
            "Train: 1 [ 170/212 ( 81%)]  Loss:  0.584855 (1.0487)  Time: 0.352s,    5.68/s  (0.357s,    5.61/s)  LR: 1.000e-04  Data: 0.038 (0.044)\n",
            "Train: 1 [ 180/212 ( 85%)]  Loss:  1.248806 (1.0427)  Time: 0.358s,    5.59/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.047 (0.044)\n",
            "Train: 1 [ 190/212 ( 90%)]  Loss:  0.759187 (1.0396)  Time: 0.359s,    5.57/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.050 (0.044)\n",
            "Train: 1 [ 200/212 ( 95%)]  Loss:  0.582119 (1.0318)  Time: 0.350s,    5.71/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.041 (0.044)\n",
            "Train: 1 [ 210/212 (100%)]  Loss:  1.399506 (1.0217)  Time: 0.347s,    5.76/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.040 (0.044)\n",
            "Train: 1 [ 211/212 (100%)]  Loss:  1.356376 (1.0233)  Time: 0.379s,    5.28/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.069 (0.044)\n",
            "train OrderedDict([('loss', 1.0232600954905995)])\n",
            "test OrderedDict([('loss', 0.9594255162176685), ('Valid mAP', '81%'), ('Valid mAR', '62%'), ('# of Full Thresh', 14)])\n",
            "\n",
            "Train: 2 [   0/212 (  0%)]  Loss:  1.040378 (1.0404)  Time: 0.352s,    5.69/s  (0.352s,    5.69/s)  LR: 1.000e-04  Data: 0.042 (0.042)\n",
            "Train: 2 [  10/212 (  5%)]  Loss:  0.930990 (1.0264)  Time: 0.372s,    5.37/s  (0.357s,    5.61/s)  LR: 1.000e-04  Data: 0.045 (0.044)\n",
            "Train: 2 [  20/212 (  9%)]  Loss:  0.901618 (0.9296)  Time: 0.366s,    5.47/s  (0.358s,    5.59/s)  LR: 1.000e-04  Data: 0.041 (0.044)\n",
            "Train: 2 [  30/212 ( 14%)]  Loss:  0.706022 (0.9084)  Time: 0.352s,    5.69/s  (0.357s,    5.59/s)  LR: 1.000e-04  Data: 0.042 (0.044)\n",
            "Train: 2 [  40/212 ( 19%)]  Loss:  0.829926 (0.8917)  Time: 0.346s,    5.79/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.037 (0.044)\n",
            "Train: 2 [  50/212 ( 24%)]  Loss:  0.956785 (0.8930)  Time: 0.345s,    5.79/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.038 (0.044)\n",
            "Train: 2 [  60/212 ( 28%)]  Loss:  0.497784 (0.8960)  Time: 0.358s,    5.59/s  (0.358s,    5.59/s)  LR: 1.000e-04  Data: 0.041 (0.045)\n",
            "Train: 2 [  70/212 ( 33%)]  Loss:  0.986284 (0.9012)  Time: 0.357s,    5.61/s  (0.358s,    5.59/s)  LR: 1.000e-04  Data: 0.046 (0.045)\n",
            "Train: 2 [  80/212 ( 38%)]  Loss:  1.190889 (0.8989)  Time: 0.366s,    5.47/s  (0.358s,    5.58/s)  LR: 1.000e-04  Data: 0.044 (0.045)\n",
            "Train: 2 [  90/212 ( 43%)]  Loss:  0.457318 (0.8863)  Time: 0.347s,    5.76/s  (0.358s,    5.58/s)  LR: 1.000e-04  Data: 0.039 (0.045)\n",
            "Train: 2 [ 100/212 ( 47%)]  Loss:  1.126706 (0.8824)  Time: 0.361s,    5.54/s  (0.359s,    5.57/s)  LR: 1.000e-04  Data: 0.051 (0.045)\n",
            "Train: 2 [ 110/212 ( 52%)]  Loss:  0.996252 (0.8803)  Time: 0.357s,    5.60/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.044 (0.045)\n",
            "Train: 2 [ 120/212 ( 57%)]  Loss:  0.959722 (0.8729)  Time: 0.370s,    5.40/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.058 (0.045)\n",
            "Train: 2 [ 130/212 ( 62%)]  Loss:  0.445437 (0.8551)  Time: 0.353s,    5.66/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.043 (0.045)\n",
            "Train: 2 [ 140/212 ( 66%)]  Loss:  1.023284 (0.8617)  Time: 0.364s,    5.50/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.044 (0.045)\n",
            "Train: 2 [ 150/212 ( 71%)]  Loss:  0.532533 (0.8581)  Time: 0.355s,    5.63/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.039 (0.045)\n",
            "Train: 2 [ 160/212 ( 76%)]  Loss:  0.848879 (0.8565)  Time: 0.353s,    5.66/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.039 (0.045)\n",
            "Train: 2 [ 170/212 ( 81%)]  Loss:  0.889936 (0.8459)  Time: 0.351s,    5.70/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.039 (0.045)\n",
            "Train: 2 [ 180/212 ( 85%)]  Loss:  0.932598 (0.8417)  Time: 0.354s,    5.66/s  (0.358s,    5.58/s)  LR: 1.000e-04  Data: 0.040 (0.045)\n",
            "Train: 2 [ 190/212 ( 90%)]  Loss:  0.920890 (0.8452)  Time: 0.357s,    5.60/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.041 (0.045)\n",
            "Train: 2 [ 200/212 ( 95%)]  Loss:  1.017471 (0.8394)  Time: 0.355s,    5.63/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.047 (0.045)\n",
            "Train: 2 [ 210/212 (100%)]  Loss:  0.401399 (0.8363)  Time: 0.351s,    5.70/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.039 (0.045)\n",
            "Train: 2 [ 211/212 (100%)]  Loss:  0.394445 (0.8342)  Time: 0.350s,    5.71/s  (0.358s,    5.58/s)  LR: 1.000e-04  Data: 0.041 (0.045)\n",
            "train OrderedDict([('loss', 0.8342317588767916)])\n",
            "test OrderedDict([('loss', 0.8099005790514366), ('Valid mAP', '72%'), ('Valid mAR', '78%'), ('# of Full Thresh', 9)])\n",
            "\n",
            "Train: 3 [   0/212 (  0%)]  Loss:  0.318533 (0.3185)  Time: 0.360s,    5.55/s  (0.360s,    5.55/s)  LR: 1.000e-04  Data: 0.044 (0.044)\n",
            "Train: 3 [  10/212 (  5%)]  Loss:  0.909918 (0.7812)  Time: 0.364s,    5.50/s  (0.357s,    5.61/s)  LR: 1.000e-04  Data: 0.045 (0.044)\n",
            "Train: 3 [  20/212 (  9%)]  Loss:  0.886522 (0.7205)  Time: 0.371s,    5.39/s  (0.357s,    5.59/s)  LR: 1.000e-04  Data: 0.050 (0.044)\n",
            "Train: 3 [  30/212 ( 14%)]  Loss:  0.697552 (0.7149)  Time: 0.357s,    5.61/s  (0.357s,    5.61/s)  LR: 1.000e-04  Data: 0.045 (0.044)\n",
            "Train: 3 [  40/212 ( 19%)]  Loss:  0.463903 (0.7148)  Time: 0.347s,    5.76/s  (0.356s,    5.61/s)  LR: 1.000e-04  Data: 0.036 (0.044)\n",
            "Train: 3 [  50/212 ( 24%)]  Loss:  0.315269 (0.7218)  Time: 0.350s,    5.72/s  (0.357s,    5.61/s)  LR: 1.000e-04  Data: 0.040 (0.044)\n",
            "Train: 3 [  60/212 ( 28%)]  Loss:  0.585003 (0.7151)  Time: 0.366s,    5.47/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.050 (0.044)\n",
            "Train: 3 [  70/212 ( 33%)]  Loss:  0.829757 (0.7214)  Time: 0.351s,    5.70/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.039 (0.044)\n",
            "Train: 3 [  80/212 ( 38%)]  Loss:  0.801013 (0.7382)  Time: 0.360s,    5.56/s  (0.358s,    5.59/s)  LR: 1.000e-04  Data: 0.045 (0.045)\n",
            "Train: 3 [  90/212 ( 43%)]  Loss:  0.543204 (0.7369)  Time: 0.359s,    5.57/s  (0.358s,    5.59/s)  LR: 1.000e-04  Data: 0.048 (0.045)\n",
            "Train: 3 [ 100/212 ( 47%)]  Loss:  0.650635 (0.7339)  Time: 0.355s,    5.63/s  (0.358s,    5.59/s)  LR: 1.000e-04  Data: 0.046 (0.045)\n",
            "Train: 3 [ 110/212 ( 52%)]  Loss:  0.531719 (0.7261)  Time: 0.353s,    5.66/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.038 (0.044)\n",
            "Train: 3 [ 120/212 ( 57%)]  Loss:  0.445311 (0.7194)  Time: 0.349s,    5.73/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.039 (0.044)\n",
            "Train: 3 [ 130/212 ( 62%)]  Loss:  0.825724 (0.7305)  Time: 0.368s,    5.44/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.045 (0.045)\n",
            "Train: 3 [ 140/212 ( 66%)]  Loss:  0.991339 (0.7251)  Time: 0.360s,    5.55/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.046 (0.045)\n",
            "Train: 3 [ 150/212 ( 71%)]  Loss:  0.817906 (0.7185)  Time: 0.356s,    5.63/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.040 (0.044)\n",
            "Train: 3 [ 160/212 ( 76%)]  Loss:  0.412777 (0.7114)  Time: 0.349s,    5.74/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.039 (0.044)\n",
            "Train: 3 [ 170/212 ( 81%)]  Loss:  0.733571 (0.7117)  Time: 0.357s,    5.61/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.046 (0.044)\n",
            "Train: 3 [ 180/212 ( 85%)]  Loss:  0.431545 (0.7130)  Time: 0.357s,    5.60/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.040 (0.044)\n",
            "Train: 3 [ 190/212 ( 90%)]  Loss:  0.551847 (0.7119)  Time: 0.358s,    5.58/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.046 (0.044)\n",
            "Train: 3 [ 200/212 ( 95%)]  Loss:  0.289057 (0.7101)  Time: 0.360s,    5.55/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.048 (0.045)\n",
            "Train: 3 [ 210/212 (100%)]  Loss:  0.921771 (0.7079)  Time: 0.368s,    5.43/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.049 (0.045)\n",
            "Train: 3 [ 211/212 (100%)]  Loss:  0.991946 (0.7092)  Time: 0.369s,    5.42/s  (0.357s,    5.60/s)  LR: 1.000e-04  Data: 0.048 (0.045)\n",
            "train OrderedDict([('loss', 0.7092291361318445)])\n",
            "test OrderedDict([('loss', 0.6666649192850166), ('Valid mAP', '74%'), ('Valid mAR', '79%'), ('# of Full Thresh', 9)])\n",
            "\n",
            "Train: 4 [   0/212 (  0%)]  Loss:  0.455936 (0.4559)  Time: 0.348s,    5.74/s  (0.348s,    5.74/s)  LR: 1.000e-04  Data: 0.040 (0.040)\n",
            "Train: 4 [  10/212 (  5%)]  Loss:  0.496658 (0.5792)  Time: 0.355s,    5.63/s  (0.360s,    5.56/s)  LR: 1.000e-04  Data: 0.044 (0.046)\n",
            "Train: 4 [  20/212 (  9%)]  Loss:  0.596561 (0.6233)  Time: 0.352s,    5.69/s  (0.360s,    5.56/s)  LR: 1.000e-04  Data: 0.040 (0.045)\n",
            "Train: 4 [  30/212 ( 14%)]  Loss:  0.635837 (0.6215)  Time: 0.359s,    5.58/s  (0.359s,    5.57/s)  LR: 1.000e-04  Data: 0.042 (0.046)\n",
            "Train: 4 [  40/212 ( 19%)]  Loss:  0.561953 (0.6227)  Time: 0.353s,    5.67/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.038 (0.045)\n",
            "Train: 4 [  50/212 ( 24%)]  Loss:  1.056348 (0.6446)  Time: 0.359s,    5.58/s  (0.359s,    5.57/s)  LR: 1.000e-04  Data: 0.048 (0.045)\n",
            "Train: 4 [  60/212 ( 28%)]  Loss:  0.259105 (0.6488)  Time: 0.353s,    5.66/s  (0.359s,    5.57/s)  LR: 1.000e-04  Data: 0.040 (0.045)\n",
            "Train: 4 [  70/212 ( 33%)]  Loss:  0.474083 (0.6408)  Time: 0.353s,    5.67/s  (0.359s,    5.57/s)  LR: 1.000e-04  Data: 0.042 (0.045)\n",
            "Train: 4 [  80/212 ( 38%)]  Loss:  0.809101 (0.6347)  Time: 0.354s,    5.65/s  (0.358s,    5.59/s)  LR: 1.000e-04  Data: 0.042 (0.045)\n",
            "Train: 4 [  90/212 ( 43%)]  Loss:  0.669501 (0.6380)  Time: 0.363s,    5.51/s  (0.358s,    5.58/s)  LR: 1.000e-04  Data: 0.052 (0.045)\n",
            "Train: 4 [ 100/212 ( 47%)]  Loss:  0.615232 (0.6385)  Time: 0.356s,    5.61/s  (0.358s,    5.59/s)  LR: 1.000e-04  Data: 0.044 (0.045)\n",
            "Train: 4 [ 110/212 ( 52%)]  Loss:  0.527612 (0.6330)  Time: 0.363s,    5.51/s  (0.358s,    5.59/s)  LR: 1.000e-04  Data: 0.049 (0.045)\n",
            "Train: 4 [ 120/212 ( 57%)]  Loss:  0.478169 (0.6330)  Time: 0.349s,    5.73/s  (0.358s,    5.58/s)  LR: 1.000e-04  Data: 0.038 (0.045)\n",
            "Train: 4 [ 130/212 ( 62%)]  Loss:  0.982049 (0.6253)  Time: 0.384s,    5.21/s  (0.358s,    5.58/s)  LR: 1.000e-04  Data: 0.067 (0.045)\n",
            "Train: 4 [ 140/212 ( 66%)]  Loss:  0.904561 (0.6277)  Time: 0.362s,    5.52/s  (0.358s,    5.58/s)  LR: 1.000e-04  Data: 0.049 (0.045)\n",
            "Train: 4 [ 150/212 ( 71%)]  Loss:  0.381410 (0.6269)  Time: 0.345s,    5.79/s  (0.358s,    5.58/s)  LR: 1.000e-04  Data: 0.037 (0.045)\n",
            "Train: 4 [ 160/212 ( 76%)]  Loss:  0.981766 (0.6255)  Time: 0.374s,    5.34/s  (0.358s,    5.58/s)  LR: 1.000e-04  Data: 0.057 (0.045)\n",
            "Train: 4 [ 170/212 ( 81%)]  Loss:  0.861292 (0.6288)  Time: 0.361s,    5.54/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.048 (0.045)\n",
            "Train: 4 [ 180/212 ( 85%)]  Loss:  0.936092 (0.6308)  Time: 0.381s,    5.25/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.067 (0.045)\n",
            "Train: 4 [ 190/212 ( 90%)]  Loss:  0.572393 (0.6297)  Time: 0.365s,    5.48/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.050 (0.045)\n",
            "Train: 4 [ 200/212 ( 95%)]  Loss:  0.686618 (0.6285)  Time: 0.355s,    5.63/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.044 (0.045)\n",
            "Train: 4 [ 210/212 (100%)]  Loss:  0.846339 (0.6284)  Time: 0.360s,    5.56/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.045 (0.045)\n",
            "Train: 4 [ 211/212 (100%)]  Loss:  0.669579 (0.6286)  Time: 0.366s,    5.47/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.053 (0.045)\n",
            "train OrderedDict([('loss', 0.6285552080228644)])\n",
            "test OrderedDict([('loss', 0.6063087426613425), ('Valid mAP', '70%'), ('Valid mAR', '76%'), ('# of Full Thresh', 10)])\n",
            "\n",
            "Train: 5 [   0/212 (  0%)]  Loss:  0.911862 (0.9119)  Time: 0.369s,    5.42/s  (0.369s,    5.42/s)  LR: 1.000e-04  Data: 0.055 (0.055)\n",
            "Train: 5 [  10/212 (  5%)]  Loss:  0.701596 (0.5229)  Time: 0.350s,    5.71/s  (0.358s,    5.58/s)  LR: 1.000e-04  Data: 0.040 (0.044)\n",
            "Train: 5 [  20/212 (  9%)]  Loss:  0.293328 (0.5349)  Time: 0.350s,    5.71/s  (0.356s,    5.62/s)  LR: 1.000e-04  Data: 0.040 (0.043)\n",
            "Train: 5 [  30/212 ( 14%)]  Loss:  0.743022 (0.5355)  Time: 0.350s,    5.72/s  (0.359s,    5.58/s)  LR: 1.000e-04  Data: 0.041 (0.044)\n",
            "Train: 5 [  40/212 ( 19%)]  Loss:  0.739061 (0.5411)  Time: 0.352s,    5.68/s  (0.358s,    5.59/s)  LR: 1.000e-04  Data: 0.043 (0.044)\n",
            "Train: 5 [  50/212 ( 24%)]  Loss:  0.728176 (0.5654)  Time: 0.374s,    5.35/s  (0.359s,    5.57/s)  LR: 1.000e-04  Data: 0.058 (0.045)\n",
            "Train: 5 [  60/212 ( 28%)]  Loss:  0.603824 (0.5619)  Time: 0.366s,    5.47/s  (0.359s,    5.57/s)  LR: 1.000e-04  Data: 0.047 (0.045)\n",
            "Train: 5 [  70/212 ( 33%)]  Loss:  0.599028 (0.5449)  Time: 0.348s,    5.75/s  (0.358s,    5.58/s)  LR: 1.000e-04  Data: 0.038 (0.044)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d8a757cc4789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   train_eval_metrics= train_epoch(\n\u001b[0;32m---> 32\u001b[0;31m           epoch , model, eff_train_loader, optimizer,saver=saver)\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_eval_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d04bd79f82c4>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, model, loader, optimizer, lr_scheduler, saver, output_dir, loss_scaler, model_ema)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# optimizer.first_step(zero_grad = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdeB7YZ5SQhR"
      },
      "source": [
        "best_effdet_model = create_effdet_model(\"/content/train/exp21/checkpoint-1.pth.tar\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4FNA0KuXdCr"
      },
      "source": [
        "def draw_boxes_effdet(boxes, labels, image, infer = False, put_text = True):\n",
        "    # read the image with OpenCV\n",
        "    if infer:\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    for i, box in enumerate(boxes):\n",
        "        color = COLORS[int(labels[i] % len(COLORS))]\n",
        "        try:\n",
        "          cv2.rectangle(\n",
        "              image,\n",
        "              (int(box[0]), int(box[1])),\n",
        "              (int(box[2]), int(box[3])),\n",
        "              color, 2\n",
        "          )\n",
        "        except:\n",
        "          continue \n",
        "        if put_text:\n",
        "          cv2.putText(image, classes[int(labels[i])], (int(box[0]), int(box[1]-5)),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n",
        "                      lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "def infer_on_effdet_output(image_file_path, trained_bench, distance_thresh):\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  #Transforming Images with transforms_coco_eval. \n",
        "\n",
        "\n",
        "  dummy_label = dict()\n",
        "  label_struct = {'img_idx': torch.Size([1]),\n",
        "    'img_scale': torch.Size([1]),\n",
        "    'img_size': torch.Size([1, 2]),\n",
        "    'label_bbox_0': torch.Size([1, 64, 64, 36]),\n",
        "    'label_bbox_1': torch.Size([1, 32, 32, 36]),\n",
        "    'label_bbox_2': torch.Size([1, 16, 16, 36]),\n",
        "    'label_bbox_3': torch.Size([1, 8, 8, 36]),\n",
        "    'label_bbox_4': torch.Size([1, 4, 4, 36]),\n",
        "    'label_cls_0': torch.Size([1, 64, 64, 9]),\n",
        "    'label_cls_1': torch.Size([1, 32, 32, 9]),\n",
        "    'label_cls_2': torch.Size([1, 16, 16, 9]),\n",
        "    'label_cls_3': torch.Size([1, 8, 8, 9]),\n",
        "    'label_cls_4': torch.Size([1, 4, 4, 9]),\n",
        "    'label_num_positives': torch.Size([1])}\n",
        "\n",
        "  for key in label_struct:\n",
        "    dummy_label[key]= torch.zeros(label_struct[key], dtype = torch.int64) \n",
        "  \n",
        "\n",
        "  transform = transforms_coco_eval(\n",
        "            (image_size, image_size),\n",
        "            interpolation='bilinear',\n",
        "            use_prefetcher=True,\n",
        "            fill_color='mean',\n",
        "            mean=IMAGENET_DEFAULT_MEAN,\n",
        "            std=IMAGENET_DEFAULT_STD)\n",
        "  opened_image = Image.open(image_file_path).convert(\"RGB\")\n",
        "  torch_image, _ = transform(opened_image, dummy_label)\n",
        "  torch_image = torch.tensor(torch_image, dtype = torch.float).unsqueeze(0).to(device)\n",
        "\n",
        "  # torch_image = F.resize(F.to_tensor(Image.open(image_file_path).convert(\"RGB\")).unsqueeze(0).to(device), [image_size, image_size])\n",
        "  output = trained_bench(torch_image)\n",
        "\n",
        "  results = dict()\n",
        "  results[\"boxes\"] = output[0, :, :4].detach().cpu()\n",
        "  results[\"scores\"] = output[0, :, 4].cpu()\n",
        "  results[\"labels\"] = output[0, :, -1].cpu()\n",
        "\n",
        "  #Empty GPU cache\n",
        "  del torch_image\n",
        "  del output\n",
        "\n",
        "  valid_box_count = 0\n",
        "  for ii, score in enumerate(results[\"scores\"]):\n",
        "    if score < distance_thresh:\n",
        "      low_index_start = ii\n",
        "      break\n",
        "    else:\n",
        "      valid_box_count += 1\n",
        "  \n",
        "  if valid_box_count == len(results[\"scores\"]):\n",
        "    low_index_start = len(results[\"scores\"])\n",
        "  \n",
        "  for key in results:\n",
        "    results[key] = results[key][:low_index_start]\n",
        "\n",
        "  torch_image = draw_boxes_effdet(results[\"boxes\"], results[\"labels\"], np.array(opened_image.resize((image_size, image_size))), infer = False, put_text = True)\n",
        "  plt.imshow(torch_image)\n",
        "\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZfIAJ1_L_Je"
      },
      "source": [
        "### Get this model to infer correctly and be able to plot them on the image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "ceghlSQXdDOD",
        "outputId": "e12eb9a1-1c24-4dd7-cc1b-289e745d379b"
      },
      "source": [
        "infer_on_effdet_output(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApples111.jpeg\", best_effdet_model, distance_thresh = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-3773a1821020>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minfer_on_effdet_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApples111.jpeg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_effdet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-897f6da9ac98>\u001b[0m in \u001b[0;36minfer_on_effdet_output\u001b[0;34m(image_file_path, trained_bench, distance_thresh)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;31m# torch_image = F.resize(F.to_tensor(Image.open(image_file_path).convert(\"RGB\")).unsqueeze(0).to(device), [image_size, image_size])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_bench\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/effdet/bench.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, img_info)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_info\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mclass_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         class_out, box_out, indices, classes = _post_process(\n\u001b[1;32m     94\u001b[0m             \u001b[0mclass_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_levels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_levels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/effdet/efficientdet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0mx_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/timm/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# add stem out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stage_out_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                     \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/timm/models/efficientnet_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_dw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1793\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1796\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 14.76 GiB total capacity; 13.68 GiB already allocated; 5.75 MiB free; 13.72 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPmuew0CnX92"
      },
      "source": [
        "# The Mobile Net Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbDtIsBs-OWD"
      },
      "source": [
        "backbone = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
        "backbone.roi_heads.box_predictor.cls_score.out_features = 6\n",
        "backbone.roi_heads.box_predictor.bbox_pred.out_features = 24\n",
        "# backbone.roi_heads.box_predictor.cls_score.out_features = 3\n",
        "# backbone.roi_heads.box_predictor.bbox_pred.out_features = 12\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPCVMZBY_GaP",
        "outputId": "92a6ebba-be7e-417d-c799-99baed4270e6"
      },
      "source": [
        "another_one_1 = train(backbone, 5, train_loader, test_loader, noise_loader, 0.001, weight_decay = 1e-4, print_every = 80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Device: cuda\n",
            "Optimizer: SAM (\n",
            "Parameter Group 0\n",
            "    N_sma_threshhold: 5\n",
            "    alpha: 0.5\n",
            "    betas: (0.95, 0.999)\n",
            "    eps: 1e-05\n",
            "    initial_lr: 0.001\n",
            "    k: 6\n",
            "    lr: 0.001\n",
            "    rho: 0.05\n",
            "    step_counter: 0\n",
            "    weight_decay: 0.0001\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5 | Batch Number: 80 | LR: 0.00099 | Train_loss: 106.89 | Test_loss: 62.96 | Test mAP: 43.39% | Missed Test Images: 0 | Seperate Noise Loader: 5 / 100\n",
            "Epoch 1/5 | Batch Number: 160 | LR: 0.00095 | Train_loss: 66.04 | Test_loss: 51.57 | Test mAP: 41.02% | Missed Test Images: 0 | Seperate Noise Loader: 3 / 100\n",
            "\n",
            " Epoch 1 Final Train mAP: 29.55% | Epoch 1 Final Missed Train Images: 1 out of 456 images \n",
            "\n",
            "Epoch 2/5 | Batch Number: 80 | LR: 0.00083 | Train_loss: 54.52 | Test_loss: 58.23 | Test mAP: 45.46% | Missed Test Images: 1 | Seperate Noise Loader: 8 / 100\n",
            "Epoch 2/5 | Batch Number: 160 | LR: 0.00074 | Train_loss: 54.63 | Test_loss: 65.66 | Test mAP: 45.21% | Missed Test Images: 2 | Seperate Noise Loader: 19 / 100\n",
            "\n",
            " Epoch 2 Final Train mAP: 33.25% | Epoch 2 Final Missed Train Images: 4 out of 456 images \n",
            "\n",
            "Epoch 3/5 | Batch Number: 80 | LR: 0.00055 | Train_loss: 59.18 | Test_loss: 57.74 | Test mAP: 29.91% | Missed Test Images: 1 | Seperate Noise Loader: 28 / 100\n",
            "Epoch 3/5 | Batch Number: 160 | LR: 0.00044 | Train_loss: 57.17 | Test_loss: 50.26 | Test mAP: 32.64% | Missed Test Images: 1 | Seperate Noise Loader: 27 / 100\n",
            "\n",
            " Epoch 3 Final Train mAP: 33.48% | Epoch 3 Final Missed Train Images: 7 out of 456 images \n",
            "\n",
            "Epoch 4/5 | Batch Number: 80 | LR: 0.00025 | Train_loss: 49.01 | Test_loss: 85.06 | Test mAP: 52.32% | Missed Test Images: 5 | Seperate Noise Loader: 60 / 100\n",
            "Epoch 4/5 | Batch Number: 160 | LR: 0.00016 | Train_loss: 59.35 | Test_loss: 52.93 | Test mAP: 41.71% | Missed Test Images: 2 | Seperate Noise Loader: 47 / 100\n",
            "\n",
            " Epoch 4 Final Train mAP: 40.42% | Epoch 4 Final Missed Train Images: 19 out of 456 images \n",
            "\n",
            "Epoch 5/5 | Batch Number: 80 | LR: 0.00004 | Train_loss: 56.69 | Test_loss: 76.69 | Test mAP: 46.52% | Missed Test Images: 3 | Seperate Noise Loader: 53 / 100\n",
            "Epoch 5/5 | Batch Number: 160 | LR: 0.00001 | Train_loss: 53.90 | Test_loss: 77.58 | Test mAP: 45.39% | Missed Test Images: 4 | Seperate Noise Loader: 59 / 100\n",
            "\n",
            " Epoch 5 Final Train mAP: 45.58% | Epoch 5 Final Missed Train Images: 20 out of 456 images \n",
            "\n",
            "Time for Total Training 214.29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjzRmGEPmjjF",
        "outputId": "2f32c1c8-573b-4684-b150-bf1c74948b83"
      },
      "source": [
        "another_one = train(backbone, 10, train_loader, test_loader, 0.001, weight_decay = 1e-4, print_every = 80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Device: cuda\n",
            "Optimizer: SAM (\n",
            "Parameter Group 0\n",
            "    N_sma_threshhold: 5\n",
            "    alpha: 0.5\n",
            "    betas: (0.95, 0.999)\n",
            "    eps: 1e-05\n",
            "    initial_lr: 0.001\n",
            "    k: 6\n",
            "    lr: 0.001\n",
            "    rho: 0.05\n",
            "    step_counter: 0\n",
            "    weight_decay: 0.0001\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 | Batch Number: 80 | LR: 0.00100 | Train_loss: 109.40 | Test_loss: 50.10 | Test mAP: 34.56% | Test mAR: nan% | Missed Test Images: 0\n",
            "Epoch 1/10 | Batch Number: 160 | LR: 0.00099 | Train_loss: 60.02 | Test_loss: 55.98 | Test mAP: 37.12% | Test mAR: nan% | Missed Test Images: 0\n",
            "\n",
            " Epoch 1 Final Train mAP: 30.74% | Epoch 1 Final Train mAR: nan% | Epoch 1 Final Missed Train Images: 0 out of 456 images \n",
            "\n",
            "Epoch 2/10 | Batch Number: 80 | LR: 0.00096 | Train_loss: 58.38 | Test_loss: 57.53 | Test mAP: 60.33% | Test mAR: nan% | Missed Test Images: 1\n",
            "Epoch 2/10 | Batch Number: 160 | LR: 0.00093 | Train_loss: 63.52 | Test_loss: 53.23 | Test mAP: 34.23% | Test mAR: nan% | Missed Test Images: 1\n",
            "\n",
            " Epoch 2 Final Train mAP: 31.06% | Epoch 2 Final Train mAR: nan% | Epoch 2 Final Missed Train Images: 6 out of 456 images \n",
            "\n",
            "Epoch 3/10 | Batch Number: 80 | LR: 0.00087 | Train_loss: 59.26 | Test_loss: 83.11 | Test mAP: 47.73% | Test mAR: nan% | Missed Test Images: 3\n",
            "Epoch 3/10 | Batch Number: 160 | LR: 0.00083 | Train_loss: 61.76 | Test_loss: 55.83 | Test mAP: 27.57% | Test mAR: nan% | Missed Test Images: 1\n",
            "\n",
            " Epoch 3 Final Train mAP: 30.96% | Epoch 3 Final Train mAR: nan% | Epoch 3 Final Missed Train Images: 6 out of 456 images \n",
            "\n",
            "Epoch 4/10 | Batch Number: 80 | LR: 0.00075 | Train_loss: 58.68 | Test_loss: 79.41 | Test mAP: 38.63% | Test mAR: nan% | Missed Test Images: 4\n",
            "Epoch 4/10 | Batch Number: 160 | LR: 0.00070 | Train_loss: 61.69 | Test_loss: 66.05 | Test mAP: 38.82% | Test mAR: nan% | Missed Test Images: 3\n",
            "\n",
            " Epoch 4 Final Train mAP: 33.09% | Epoch 4 Final Train mAR: nan% | Epoch 4 Final Missed Train Images: 8 out of 456 images \n",
            "\n",
            "Epoch 5/10 | Batch Number: 80 | LR: 0.00060 | Train_loss: 58.89 | Test_loss: 80.11 | Test mAP: 38.94% | Test mAR: nan% | Missed Test Images: 1\n",
            "Epoch 5/10 | Batch Number: 160 | LR: 0.00055 | Train_loss: 56.70 | Test_loss: 78.22 | Test mAP: 35.02% | Test mAR: nan% | Missed Test Images: 1\n",
            "\n",
            " Epoch 5 Final Train mAP: 34.09% | Epoch 5 Final Train mAR: nan% | Epoch 5 Final Missed Train Images: 7 out of 456 images \n",
            "\n",
            "Epoch 6/10 | Batch Number: 80 | LR: 0.00044 | Train_loss: 52.62 | Test_loss: 86.22 | Test mAP: 43.91% | Test mAR: nan% | Missed Test Images: 2\n",
            "Epoch 6/10 | Batch Number: 160 | LR: 0.00039 | Train_loss: 59.55 | Test_loss: 93.01 | Test mAP: 46.96% | Test mAR: nan% | Missed Test Images: 3\n",
            "\n",
            " Epoch 6 Final Train mAP: 37.65% | Epoch 6 Final Train mAR: nan% | Epoch 6 Final Missed Train Images: 20 out of 456 images \n",
            "\n",
            "Epoch 7/10 | Batch Number: 80 | LR: 0.00029 | Train_loss: 54.05 | Test_loss: 99.96 | Test mAP: 39.45% | Test mAR: nan% | Missed Test Images: 4\n",
            "Epoch 7/10 | Batch Number: 160 | LR: 0.00025 | Train_loss: 58.93 | Test_loss: 79.80 | Test mAP: 34.77% | Test mAR: nan% | Missed Test Images: 1\n",
            "\n",
            " Epoch 7 Final Train mAP: 38.81% | Epoch 7 Final Train mAR: nan% | Epoch 7 Final Missed Train Images: 18 out of 456 images \n",
            "\n",
            "Epoch 8/10 | Batch Number: 80 | LR: 0.00016 | Train_loss: 58.33 | Test_loss: 90.27 | Test mAP: 38.50% | Test mAR: nan% | Missed Test Images: 3\n",
            "Epoch 8/10 | Batch Number: 160 | LR: 0.00012 | Train_loss: 52.78 | Test_loss: 92.37 | Test mAP: 45.57% | Test mAR: nan% | Missed Test Images: 3\n",
            "\n",
            " Epoch 8 Final Train mAP: 43.94% | Epoch 8 Final Train mAR: nan% | Epoch 8 Final Missed Train Images: 24 out of 456 images \n",
            "\n",
            "Epoch 9/10 | Batch Number: 80 | LR: 0.00007 | Train_loss: 52.34 | Test_loss: 99.14 | Test mAP: 48.14% | Test mAR: nan% | Missed Test Images: 4\n",
            "Epoch 9/10 | Batch Number: 160 | LR: 0.00004 | Train_loss: 59.78 | Test_loss: 97.68 | Test mAP: 42.97% | Test mAR: nan% | Missed Test Images: 4\n",
            "\n",
            " Epoch 9 Final Train mAP: 47.44% | Epoch 9 Final Train mAR: nan% | Epoch 9 Final Missed Train Images: 29 out of 456 images \n",
            "\n",
            "Epoch 10/10 | Batch Number: 80 | LR: 0.00001 | Train_loss: 53.61 | Test_loss: 105.94 | Test mAP: 44.46% | Test mAR: nan% | Missed Test Images: 4\n",
            "Epoch 10/10 | Batch Number: 160 | LR: 0.00000 | Train_loss: 53.82 | Test_loss: 105.05 | Test mAP: 45.50% | Test mAR: nan% | Missed Test Images: 4\n",
            "\n",
            " Epoch 10 Final Train mAP: 51.20% | Epoch 10 Final Train mAR: nan% | Epoch 10 Final Missed Train Images: 27 out of 456 images \n",
            "\n",
            "Time for Total Training 381.80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-cknP0_uy0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "916917ab-36c0-4f8a-fd3c-0fcfe2fa1754"
      },
      "source": [
        "# https://github.com/amdegroot/ssd.pytorch/blob/master/layers/box_utils.py#L48\n",
        "def intersect(box_a, box_b):\n",
        "\n",
        "    A = box_a.size(0)\n",
        "    B = box_b.size(0)\n",
        "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
        "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
        "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
        "    return inter[:, :, 0] * inter[:, :, 1]\n",
        "\n",
        "def jaccard_iou(box_a, box_b):\n",
        "\n",
        "    inter = intersect(box_a, box_b)\n",
        "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
        "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
        "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
        "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
        "    union = area_a + area_b - inter\n",
        "    return inter / union  # [A,B]\n",
        "\n",
        "def calculate_iou_on_label(results, len_of_results, iou_thresh, device):\n",
        "  for current_index, _ in enumerate(results[\"boxes\"]):\n",
        "    if current_index >= len_of_results:\n",
        "      break\n",
        "\n",
        "    current_index_iou = jaccard_iou(results[\"boxes\"][current_index].view(1, -1).to(device),\n",
        "                                    results[\"boxes\"].to(device))\n",
        "    \n",
        "    mask = (current_index_iou > iou_thresh) & (current_index_iou != 1)\n",
        "    mask = mask.squeeze()\n",
        "    for key in results:\n",
        "      results[key] = results[key][~mask]\n",
        "\n",
        "    len_of_results -= sum(mask)\n",
        "  \n",
        "  return results\n",
        "\n",
        "def get_labels_categ(classes, want):\n",
        "  fruit_index_list, bad_spot_index_list = list(), list()\n",
        "  for ii, name in enumerate(classes):\n",
        "    if re.search(\"Spot\", name):\n",
        "      bad_spot_index_list.append(ii)\n",
        "    elif re.search(\"Placeholder\", name):\n",
        "      continue\n",
        "    else:\n",
        "      fruit_index_list.append(ii)\n",
        "  \n",
        "  if want == \"fruit\":\n",
        "    return fruit_index_list\n",
        "  elif want == \"bad_spot\":\n",
        "    return bad_spot_index_list\n",
        "  else:\n",
        "    raise ValueError(\"want Type not applicable [fruit or bad_spot only]\")\n",
        "\n",
        "print(classes)\n",
        "get_labels_categ(classes, \"bad_spot\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Placeholder', 'Apples', 'Strawberry', 'Tomato', 'Apple_Bad_Spot', 'Strawberry_Bad_Spot', 'Tomato_Bad_Spot']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 5, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFhjlggXlAx7"
      },
      "source": [
        "def infer_image(image_file_path, trained_model, distance_thresh, iou_thresh, webcam = False, show_image = True):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  #Just load it up as PIL. Avoid using cv2 because do not need albumentations\n",
        "  if not webcam:\n",
        "    torch_image = F.to_tensor(Image.open(image_file_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    trained_model.to(device)\n",
        "    trained_model.eval()\n",
        "    print(\"Image Size: {}\".format(torch_image.size()))\n",
        "\n",
        "    start_time = time.time()\n",
        "    results = trained_model(torch_image)\n",
        "    end_time = time.time() - start_time\n",
        "\n",
        "    print(\"Time of Inference {:0.2f}\".format(end_time))\n",
        "  else:\n",
        "\n",
        "    torch_image = F.to_tensor(image_file_path).unsqueeze(1).to(device)\n",
        "\n",
        "    results = trained_model(torch_image)\n",
        "\n",
        "  valid_box_count = 0\n",
        "  for ii, score in enumerate(results[0][\"scores\"]):\n",
        "    if score < distance_thresh:\n",
        "      low_index_start = ii\n",
        "      break\n",
        "    else:\n",
        "      valid_box_count += 1\n",
        "\n",
        "  if valid_box_count == len(results[0][\"scores\"]):\n",
        "    low_index_start = len(results[0][\"scores\"])\n",
        "  \n",
        "  for key in results[0]:\n",
        "    results[0][key] = results[0][key][:low_index_start]\n",
        "  \n",
        "  #This is where I place the order of the list\n",
        "  fruit_spot_iou_thresh, bad_spot_iou_thresh = iou_thresh\n",
        "\n",
        "  #Update when I get more data of fruits and when running for script beware of classes.\n",
        "  bad_spot_index = [ii for ii, label in enumerate(results[0][\"labels\"]) if label in get_labels_categ(classes, \"bad_spot\")]\n",
        "  fruit_index = [ii for ii, _ in enumerate(results[0][\"labels\"]) if ii not in bad_spot_index]\n",
        "\n",
        "  bad_spot_results, fruit_results = dict(), dict()\n",
        "\n",
        "  for key in results[0]:\n",
        "    bad_spot_results[key], fruit_results[key] = results[0][key][[bad_spot_index]], results[0][key][[fruit_index]]\n",
        "\n",
        "  assert len(bad_spot_results[\"boxes\"]) == len(bad_spot_results[\"scores\"]) == len(bad_spot_results[\"labels\"])\n",
        "  assert len(fruit_results[\"boxes\"]) == len(fruit_results[\"scores\"]) == len(fruit_results[\"labels\"])\n",
        "\n",
        "  len_of_bad_spots, len_of_fruit = len(bad_spot_results[\"boxes\"]), len(fruit_results[\"boxes\"])\n",
        "\n",
        "  if len_of_bad_spots > 1:\n",
        "    bad_spot_results = calculate_iou_on_label(bad_spot_results, len_of_bad_spots, bad_spot_iou_thresh, device)\n",
        "  if len_of_fruit > 1:\n",
        "    fruit_results = calculate_iou_on_label(fruit_results, len_of_fruit, fruit_spot_iou_thresh, device)\n",
        "  \n",
        "  for key in results[0]: \n",
        "    if (key == \"boxes\"):\n",
        "      results[0][\"boxes\"] = torch.cat((fruit_results[\"boxes\"], bad_spot_results[\"boxes\"]), axis = 0)\n",
        "    else:\n",
        "      results[0][key] = torch.cat((fruit_results[key], bad_spot_results[key]), dim = 0)\n",
        "\n",
        "  if show_image:\n",
        "    if device == torch.device(\"cuda\"):\n",
        "      torch_image = torch_image.cpu() \n",
        "    written_image = cv2.cvtColor(draw_boxes(results[0][\"boxes\"], results[0][\"labels\"], torch_image.squeeze(), infer = True, put_text= True), cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(written_image)\n",
        "  \n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ab85jWlu5L7"
      },
      "source": [
        "results = infer_image(\"/content/tomatpred.jpg\", another_one, 0.2, [0.3, 0.1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZd4nVDuiu4M"
      },
      "source": [
        "import base64\n",
        "import html\n",
        "import io\n",
        "import time\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "def start_input():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      /* try changing the capture canvas and see what happens*/\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def take_photo(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T45EScTjJ5e"
      },
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def js_reply_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          js_reply: JavaScript object, contain image from webcam\n",
        "\n",
        "    output: \n",
        "          image_array: image array RGB size 512 x 512 from webcam\n",
        "    \"\"\"\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\n",
        "    image_array = np.array(image_PIL)\n",
        "\n",
        "    return image_array\n",
        "\n",
        "def drawing_array_to_bytes(drawing_array):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          drawing_array: image RGBA size 512 x 512 \n",
        "                              contain bounding box and text from yolo prediction, \n",
        "                              channel A value = 255 if the pixel contains drawing properties (lines, text) \n",
        "                              else channel A value = 0\n",
        "\n",
        "    output: \n",
        "          drawing_bytes: string, encoded from drawing_array\n",
        "    \"\"\"\n",
        "\n",
        "    drawing_PIL = Image.fromarray(drawing_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    drawing_PIL.save(iobuf, format='png')\n",
        "    drawing_bytes = 'data:image/png;base64,{}'.format((str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "    return drawing_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FSrjfHjgX4q"
      },
      "source": [
        "data_transforms = get_transforms(mode = \"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "j0WidBmGlktM",
        "outputId": "6e291470-abb3-45b9-8be7-c90bf01e15e3"
      },
      "source": [
        "start_input()\n",
        "label_html = 'Capturing...'\n",
        "img_data = ''\n",
        "count = 0 \n",
        "\n",
        "color=None\n",
        "label=None\n",
        "line_thickness=None\n",
        "another_one.to(device).eval();\n",
        "while True:\n",
        "    js_reply = take_photo(label_html, img_data)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    image = js_reply_to_image(js_reply)\n",
        "    prediciton = infer_image(image, another_one, 0.03, [0.3, 0.1], webcam= True, show_image = False)\n",
        "\n",
        "    drawing_array = np.zeros([512,512,4], dtype=np.uint8)\n",
        "\n",
        "    for x in prediciton[0]['boxes']:\n",
        "\n",
        "      tl = line_thickness or round(0.002 * (drawing_array.shape[0] + drawing_array.shape[1]) / 2) + 1  # line/font thickness\n",
        "      color = color or [random.randint(0, 255) for _ in range(3)]\n",
        "      c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
        "      cv2.rectangle(drawing_array, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
        "      if label:\n",
        "        tf = max(tl - 1, 1)  # font thickness\n",
        "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
        "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
        "        cv2.rectangle(drawing_array, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
        "        cv2.putText(drawing_array, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "    drawing_array[:,:,3] = (drawing_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "\n",
        "    drawing_PIL = Image.fromarray(drawing_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    drawing_PIL.save(iobuf, format='png')\n",
        "    drawing_bytes = 'data:image/png;base64,{}'.format((str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "    img_data = drawing_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      /* try changing the capture canvas and see what happens*/\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 512; //video.videoWidth;\n",
              "      captureCanvas.height = 512; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function takePhoto(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}