{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FruitObjectDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZfnWC_NF7NM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch \n",
        "from torch import nn, optim \n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import itertools\n",
        "import glob \n",
        "from PIL import Image\n",
        "import csv \n",
        "import cv2\n",
        "import re\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.ops.boxes import box_iou\n",
        "import random\n",
        "import torchvision\n",
        "\n",
        "!pip install --upgrade albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "!git clone https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
        "%cd Ranger-Deep-Learning-Optimizer\n",
        "!pip install -e .\n",
        "from ranger import Ranger  \n",
        "%cd ..\n",
        "#https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1\n",
        "!git clone https://github.com/davda54/sam.git\n",
        "%cd sam\n",
        "import sam\n",
        "print(\"Imported SAM Successfully from github .py file\")\n",
        "%cd ..\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_pckeYPGF0b",
        "outputId": "7118f53d-9262-460b-fee6-d94a5a3862fd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCS2doQYbNpf"
      },
      "source": [
        "## To Do Right Now: \n",
        "\n",
        "### Steps for fully integrating effecient det \n",
        "* Change Effecidet det printing to a certain batch size\n",
        "* Fix epoch thing\n",
        "*  Change statistics that effecientdet prints out to mAP instead of loss.\n",
        "* Get a test loop that works with the same metrics \n",
        "* Play with test function using optimization techniues like Normalizing (ensuring normalizing is consistent and global), sam, ranger to see if that increase acc or decreases loss\n",
        "\n",
        "\n",
        "### Spend 2 hours labeling the other data on peaches and tomatoes label them and put them in dataset.\n",
        "\n",
        "\n",
        "### Email Evan about any new project ideas. Look at new project ideas from the DOCUMENT. Read about new ideas into agriculture and look for problems that can be solved with computer vision. Also, develop note-taking idea a little bit. \n",
        "\n",
        "#Possible Solution\n",
        "\n",
        "### go to ImageNet and get images of round classes so like human faces or dog faces or balls etc. (have no bounding boxes). Get a new loader called noise_loader and make a loss that sees how many bouding boxes are predicted in the noise_loader class.\n",
        "\n",
        "#Less Urgent Ideas in the Future:\n",
        "\n",
        "### Look into convolutional neural networks into rnns and into optical flow. Try to get an rnn running with the conv effecient det \n",
        "\n",
        "### Implement model onto hardware or rasberry pi now. Look into turning jupyter notebook into python script or deploy it to rasberry pi.\n",
        "\n",
        " \n",
        "### https://www.emerginginvestigators.org/articles?category_id=10\n",
        "\n",
        "### Train, Once I get a model with very good results. torch.save it state_dicts on my local disk. Also record results of models on results spreadsheet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ3xBjGlDiXK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4UKDVNnIqQN"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/LatestFruitDataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/EfficientDet.Pytorch-Updated.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1cWaZ4DSJp3",
        "outputId": "37396958-736e-4349-ab1c-ae76b215bba7"
      },
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "%cd EfficientDet.Pytorch-Updated/\n",
        "import math\n",
        "from models.efficientnet import EfficientNet\n",
        "from models.bifpn import BIFPN\n",
        "from models.retinahead import RetinaHead\n",
        "from models.module import RegressionModel, ClassificationModel, Anchors, ClipBoxes, BBoxTransform\n",
        "from torchvision.ops import nms\n",
        "from models.losses import FocalLoss\n",
        "from models.efficientdet import EfficientDet\n",
        "from models.losses import FocalLoss\n",
        "from datasets import VOCDetection, CocoDataset, get_augumentation, detection_collate, Resizer, Normalizer, Augmenter, collater\n",
        "from utils import EFFICIENTDET, get_state_dict\n",
        "from eval import evaluate, evaluate_coco\n",
        "%cd .."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/EfficientDet.Pytorch-Updated\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY83cUEFAjoW",
        "outputId": "5e63fee0-d58c-47ff-d7b0-338159cd484e"
      },
      "source": [
        "#For one strawberry batch please drop watermark rows\n",
        "strawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 3 Labeled/FreshStrawberryBatch3Labels.csv\", header = None)\n",
        "strawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 2 Labeled/FreshStrawberriesBatch2Labels.csv\", header = None)\n",
        "strawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 1 Labeled/Strawberrybatch1.csv\", header = None)\n",
        "rottenApple_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch1Labeled/RottenAppleBatch1Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch2Labeled/RottenApplesBatch2Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch3Labaled/RottenApplesBatch3Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch1RottenStrawBerryLabels/RottenStrawberriesBatch1Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch2RottenStrawBerryLabels/RottenStrawBerryBatch2.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch3RottenStrawberrylabel/rottenStrawberryBtch3labels.csv\", header = None)\n",
        "freshApples_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplebtch2label/FreshApplesBatch2LabelsFresh.csv\", header = None)\n",
        "freshApples_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplesBatch1Labels/FreshAppleBatch1Labels.csv\", header = None)\n",
        "\n",
        "strawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "\n",
        "#Drop some watermark data for Fresh StrawBerry Batch 1 Labeled images [59, 9, 93]\n",
        "\n",
        "# strawberry_csv_batch_1 = strawberry_csv_batch_1[Image_id not in [\"FreshStrawberries59.jpeg, FreshStrawberries9.jpeg, FreshStrawberries93.jpeg\"]]\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries59.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries9.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries93.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1 = strawberry_csv_batch_1.reset_index(drop=True)\n",
        "\n",
        "#Stack all the csv files together. \n",
        "list_of_all_dataframes = [strawberry_csv_batch_1, strawberry_csv_batch_2, strawberry_csv_batch_3, rottenApple_csv_batch_1, \n",
        "                          rottenApple_csv_batch_2, rottenApple_csv_batch_3, rottenStrawberry_csv_batch_1, rottenStrawberry_csv_batch_2, \n",
        "                          rottenStrawberry_csv_batch_3, freshApples_csv_batch_2, freshApples_csv_batch_1]\n",
        "fruit_df = pd.concat(list_of_all_dataframes, ignore_index = True)\n",
        "\n",
        "total_row_sum_check = 0 \n",
        "for dataframe in list_of_all_dataframes:\n",
        "  total_row_sum_check += dataframe.shape[0]\n",
        "print(\"Checked total rows from all the dataframes combined: {}\".format(total_row_sum_check))\n",
        "\n",
        "def run_dataframe_check():\n",
        "  assert total_row_sum_check == fruit_df.shape[0]\n",
        "  print(\"DataFrame shape: {}\".format(fruit_df.shape))\n",
        "  print(\"Unique Fruit Labels {}\".format(fruit_df[\"Fruit\"].unique()))\n",
        "  print(\"Number of Unique Images {}\".format(len(fruit_df[\"Image_id\"].unique())))\n",
        "\n",
        "run_dataframe_check()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checked total rows from all the dataframes combined: 1100\n",
            "DataFrame shape: (1100, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Bad_Spots']\n",
            "Number of Unique Images 344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KalFI9OONbjp",
        "outputId": "173130c5-b878-48fb-ef98-ccc5f6dbe357"
      },
      "source": [
        "#Specify more image types when \n",
        "def more_specific_Image_id(image_id, fruit):\n",
        "  if fruit == \"Bad_Spots\":\n",
        "    if re.search(\"RottenStrawberries\", image_id):\n",
        "      return \"Strawberry_Bad_Spot\"\n",
        "    elif re.search(\"RottenApples\", image_id):\n",
        "      return \"Apple_Bad_Spot\"\n",
        "    else:\n",
        "      raise ValueError(\"Could not find a match for some of the Image_ids\")\n",
        "\n",
        "  else:\n",
        "    return fruit\n",
        "\n",
        "fruit_df[\"Fruit\"] = fruit_df.apply(lambda row: more_specific_Image_id(row.Image_id, row.Fruit), axis = 1)\n",
        "\n",
        "run_dataframe_check()\n",
        "\n",
        "#Post Processing \n",
        "fruit_df = fruit_df[fruit_df[\"Image_id\"] != \"FreshStrawberries15.jpeg\"]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame shape: (1100, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Apple_Bad_Spot' 'Strawberry_Bad_Spot']\n",
            "Number of Unique Images 344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIoosfwlUAdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55c890e-e72b-424e-f419-c36df3823cd0"
      },
      "source": [
        "bounding_box_dict = dict()\n",
        "labels_dict = dict()\n",
        "classes = [\"Placeholder\", \"Apples\", \"Strawberry\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\"]\n",
        "classes = [\"Apples\", \"Strawberry\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\"]\n",
        "print(classes)\n",
        "# classes = [\"Bad_Spots\", \"Strawberry\", \"Apples\"]\n",
        "\n",
        "for row_index in range(len(fruit_df)): \n",
        "  current_image_file = fruit_df.iloc[row_index][\"Image_id\"]\n",
        "  if current_image_file not in bounding_box_dict:\n",
        "    bounding_box_dict[current_image_file] = list()\n",
        "    labels_dict[current_image_file] = list()\n",
        "  bounding_box_dict[current_image_file].append(fruit_df.iloc[row_index, 1:5].to_list())\n",
        "  labels_dict[current_image_file].append(classes.index(fruit_df.iloc[row_index, 0]))\n",
        "\n",
        "print(len(bounding_box_dict))\n",
        "print(len(labels_dict))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Apples', 'Strawberry', 'Apple_Bad_Spot', 'Strawberry_Bad_Spot']\n",
            "343\n",
            "343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15qhrCwGUPxp"
      },
      "source": [
        "## Class function + util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgVTPiupUTQa"
      },
      "source": [
        "def ffile_path(image_id, full_image_file_paths):\n",
        "  for image_path in full_image_file_paths:\n",
        "    if image_id in image_path:\n",
        "      return image_path\n",
        "\n",
        "def find_area_bb(bb_coord):\n",
        "  bb_coord = bb_coord.numpy()\n",
        "  area_of_each_bb = list()\n",
        "  for pair_of_coord in bb_coord:\n",
        "    area_of_each_bb.append(\n",
        "        (pair_of_coord[2] - pair_of_coord[0]) * (pair_of_coord[3] - pair_of_coord[1])\n",
        "    )\n",
        "  return torch.tensor(area_of_each_bb, dtype=torch.int32)\n",
        "\n",
        "def convert_min_max(bb_coord):\n",
        "  for pair_of_coord in bb_coord:\n",
        "    pair_of_coord[2], pair_of_coord[3] = (pair_of_coord[0] + pair_of_coord[-2]), (pair_of_coord[1] + pair_of_coord[-1])\n",
        "  return bb_coord\n",
        "\n",
        "class FruitDetectDataset(object):\n",
        "  def __init__(self, id_labels, id_bounding_boxes, transforms, mode):\n",
        "\n",
        "    assert len(id_labels) == len(id_bounding_boxes)\n",
        "    assert sorted(id_labels.keys()) == sorted(id_bounding_boxes.keys())\n",
        "    self.imgs_key = sorted(id_labels.keys())\n",
        "\n",
        "    np.random.shuffle(self.imgs_key)\n",
        "    if (mode == \"train\"):\n",
        "      self.imgs_key = self.imgs_key[:int(len(self.imgs_key) * 0.8)]\n",
        "    else:\n",
        "      self.imgs_key = self.imgs_key[int(len(self.imgs_key) * 0.8):]\n",
        "\n",
        "    self.id_labels = id_labels\n",
        "    self.id_bounding_boxes = id_bounding_boxes\n",
        "    self.full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_path = ffile_path(self.imgs_key[idx], self.full_image_file_paths) \n",
        "    img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "    boxes = convert_min_max(torch.as_tensor(self.id_bounding_boxes[self.imgs_key[idx]], dtype=torch.float32))\n",
        "    \n",
        "    labels = torch.as_tensor(self.id_labels[self.imgs_key[idx]], dtype=torch.int64)\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = find_area_bb(boxes)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    \n",
        "    #Query about transforms for labels of images\n",
        "    if self.transforms: \n",
        "      sample = {\n",
        "                'image': img,\n",
        "                'bboxes': target['boxes'],\n",
        "                'labels': labels\n",
        "            }\n",
        "\n",
        "      sample = self.transforms(**sample)\n",
        "      img = sample['image']\n",
        "      target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "    \n",
        "    \n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs_key)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDh2_pg4J2yo"
      },
      "source": [
        "#The Drawing function.\n",
        "# COLORS = [(255, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0)]\n",
        "COLORS = [(0, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0), (255, 0, 0)]\n",
        "\n",
        "def draw_boxes(boxes, labels, image, infer = False, put_text = True):\n",
        "    # read the image with OpenCV\n",
        "    image = image.permute(1, 2, 0).numpy()\n",
        "    if infer:\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    for i, box in enumerate(boxes):\n",
        "        color = COLORS[labels[i]]\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (int(box[0]), int(box[1])),\n",
        "            (int(box[2]), int(box[3])),\n",
        "            color, 2\n",
        "        )\n",
        "        if put_text:\n",
        "          cv2.putText(image, classes[labels[i]], (int(box[0]), int(box[1]-5)),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n",
        "                      lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "# Albumentations\n",
        "def get_transforms(mode):\n",
        "  if (mode == \"train\"):\n",
        "    return A.Compose([\n",
        "                      A.OneOf([\n",
        "                      A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                     val_shift_limit=0.2, p=0.9),\n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "                      A.HorizontalFlip(),\n",
        "                      A.VerticalFlip(), \n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"test\"):\n",
        "    return A.Compose([\n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"effdet_train\"):\n",
        "    return A.Compose([\n",
        "                      A.OneOf([\n",
        "                      A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                     val_shift_limit=0.2, p=0.9),\n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "                      A.HorizontalFlip(),\n",
        "                      A.VerticalFlip(), \n",
        "                      A.Resize(height = 512, width=512), \n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "  else:\n",
        "    raise ValueError(\"mode is wrong value can either be train or test\")\n",
        "\n",
        "#Using this stack overflow (https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders)\n",
        "#(Suppose for example, you want to create batches of a list of varying dimension tensors. The below code pads sequences with 0 until the maximum sequence size of the batch,)\n",
        "#Collate_fn is a function that is used to process your batches before you pass it to dataloader. In my case since I have different sized images I need a way to stack batches b/c torch.stack won't work.\n",
        "#So I use zip which can accept tensors of different lengths and make them stacked with the size of the lowest length list given. Therefore stacking all the images in a batch \n",
        "#Successfully unlike torch.stack and doing that processing to every batch makes collate_fn vital since I have different image sizes.\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return tuple([list(a) for a in zip(*batch)])\n",
        "    # return tuple(zip(*batch))\n",
        "\n",
        "train_batch_size = 1\n",
        "test_batch_size = 1\n",
        "\n",
        "train_dataset = FruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"train\"), mode = \"train\")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= collate_fn)\n",
        "\n",
        "test_dataset = FruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"test\"), mode = \"test\")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = test_batch_size, shuffle = True, collate_fn= collate_fn)\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "-uf9fdpPK-SQ",
        "outputId": "b92567d7-02b2-4ae2-dc9d-c8a655fb7693"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    image = draw_boxes(labels[idx][\"boxes\"], labels[idx][\"labels\"], images[idx])\n",
        "    plt.imshow(image)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-2c30b5b4a517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A0EkOWcPSX0"
      },
      "source": [
        "## What we learned\n",
        "is that the model outputs losses when in train mode \n",
        "when in model.eval model, the model code then return only a prediction with no losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NQrBD22VWxD"
      },
      "source": [
        "def calculate_metrics(target_box,predictions_box,scores, device):\n",
        "\n",
        "    #Get most confident boxes first and least confident last\n",
        "    predictions_box = predictions_box[scores.argsort().flip(-1)]\n",
        "    iou_mat = box_iou(target_box,predictions_box)\n",
        "    #return a one by one matrix that is form (target_box, prediction_box) or (1, 1)\n",
        "    target_boxes_count, prediction_boxes_count = iou_mat.shape\n",
        "    \n",
        "    mAP_Matrix = torch.zeros_like(iou_mat)\n",
        "    # if not matrix coordinates that relate to nothing.\n",
        "    if not iou_mat[:,0].eq(0.).all():\n",
        "      index_of_biggest_iou = iou_mat[:,0].argsort()[-1]\n",
        "      mAP_Matrix[index_of_biggest_iou,0] = 1\n",
        "\n",
        "    for pr_idx in range(1,prediction_boxes_count):\n",
        "        not_assigned = torch.logical_not(mAP_Matrix[:,:pr_idx].sum(1)).long()\n",
        "        targets = not_assigned * iou_mat[:,pr_idx]\n",
        "\n",
        "        if targets.eq(0).all():\n",
        "            continue\n",
        "\n",
        "        pivot = targets.argsort()[-1]\n",
        "        mAP_Matrix[pivot,pr_idx] = 1\n",
        "\n",
        "    # mAP calculation\n",
        "    tp = mAP_Matrix.sum()\n",
        "    fp = mAP_Matrix.sum(0).eq(0).sum()\n",
        "    fn = mAP_Matrix.sum(1).eq(0).sum()\n",
        "\n",
        "    mAP = tp / (tp+fp)\n",
        "    mAR = tp / (tp+fn)\n",
        "\n",
        "    return mAP, mAR\n",
        "\n",
        "def run_metrics_for_batch(output, targets, mAP, mAR, missed_images, device):\n",
        "  for pos_in_batch, image_pred in enumerate(output):\n",
        "    assert (len(image_pred[\"boxes\"]) == len(image_pred[\"labels\"]) == len(image_pred[\"scores\"]))\n",
        "    if len(image_pred[\"boxes\"]) != 0:\n",
        "      curr_mAP, curr_mAR = calculate_metrics(targets[pos_in_batch][\"boxes\"], output[pos_in_batch][\"boxes\"], output[pos_in_batch][\"scores\"], device)\n",
        "      mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
        "    else:\n",
        "      missed_images += 1 \n",
        "  \n",
        "  return mAP, mAR, missed_images\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnukB06TZqTb"
      },
      "source": [
        "https://pypi.org/project/pytorch-warmup/ link for doing warmup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB_w3zeIOa8X"
      },
      "source": [
        "def train(net, epochs, train_loader, test_loader, lr, weight_decay, \n",
        "          print_every = 6, lo_test_dataset = len(test_dataset), lo_train_dataset = len(train_dataset)):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Check which parameters can calculate gradients. \n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "\n",
        "    # optimizer = Ranger(net.parameters(), lr = lr, weight_decay= weight_decay)\n",
        "    base_optimizer = Ranger\n",
        "    optimizer = sam.SAM(net.parameters(), base_optimizer, lr = lr, weight_decay = weight_decay)\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
        "\n",
        "    net.to(device)\n",
        "    print(\"Device: {}\".format(device))\n",
        "    print(\"Optimizer: {}\".format(optimizer))\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        \n",
        "        train_loss = steps = train_mAP = train_mAR = missed_train_images = 0\n",
        "        \n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "            net.train()\n",
        "\n",
        "            steps += 1\n",
        "            \n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            net.eval()\n",
        "            train_mAP, train_mAR, missed_train_images = run_metrics_for_batch(net(images), targets, train_mAP, train_mAR, missed_train_images, device)\n",
        "            net.train()\n",
        "\n",
        "            losses.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            optimizer.first_step(zero_grad = True)\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            losses.backward()\n",
        "            optimizer.second_step(zero_grad = True)\n",
        "\n",
        "            train_loss +=  losses.item()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (steps % print_every) == 0:\n",
        "\n",
        "              with torch.no_grad():\n",
        "                test_mAP = test_mAR = missed_test_images = test_loss = 0\n",
        "\n",
        "                for images, targets in test_loader:\n",
        "\n",
        "                  net.eval()\n",
        "                  if device == torch.device(\"cuda\"):\n",
        "                    images = [image.to(device) for image in images]\n",
        "                    targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "                  output = net(images)\n",
        "                  test_mAP, test_mAR, missed_test_images = run_metrics_for_batch(output, targets, test_mAP, test_mAR, missed_test_images, device)\n",
        "\n",
        "                  net.train()\n",
        "                  test_loss_dict = net(images, targets)\n",
        "                  test_losses = sum(loss for loss in test_loss_dict.values())\n",
        "                  test_loss += test_losses.item()\n",
        "\n",
        "                for param_group in optimizer.param_groups:\n",
        "                  learning_rate_extract = param_group[\"lr\"]\n",
        "                print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Test_loss: {:0.2f} | Test mAP: {:0.2f}% | Test mAR: {:0.2f}% | Missed Test Images: {}\".format(\n",
        "                    epoch + 1, epochs, steps, learning_rate_extract, train_loss, test_loss,  \n",
        "                    (test_mAP / float(lo_test_dataset)) * 100., (test_mAR / float(lo_test_dataset)) * 100.,missed_test_images))\n",
        "\n",
        "              assert (steps % print_every) == 0\n",
        "              train_loss = 0\n",
        "                 \n",
        "        print(\"\\n Epoch {} Final Train mAP: {:0.2f}% | Epoch {} Final Train mAR: {:0.2f}% | Epoch {} Final Missed Train Images: {} out of {} images \\n\".format(\n",
        "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, (train_mAR / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, missed_train_images, lo_train_dataset\n",
        "        ))\n",
        "    \n",
        "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))\n",
        "\n",
        "    return net"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj7pzNpCdH6j"
      },
      "source": [
        "Note check if should use tensorv2 or tensor and universal normalizing step. \n",
        "https://github.com/toandaominh1997/EfficientDet.Pytorch/blob/fbe56e58c9a2749520303d2d380427e5f01305ba/datasets/augmentation.py#L94"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjFx0ZuYFca"
      },
      "source": [
        "class EffdetFruitDetectDataset(object):\n",
        "  def __init__(self, id_labels, id_bounding_boxes, transforms, mode):\n",
        "\n",
        "    assert len(id_labels) == len(id_bounding_boxes)\n",
        "    assert sorted(id_labels.keys()) == sorted(id_bounding_boxes.keys())\n",
        "    self.imgs_key = sorted(id_labels.keys())\n",
        "\n",
        "    np.random.shuffle(self.imgs_key)\n",
        "    if (mode == \"train\"):\n",
        "      self.imgs_key = self.imgs_key[:int(len(self.imgs_key) * 0.8)]\n",
        "    else:\n",
        "      self.imgs_key = self.imgs_key[int(len(self.imgs_key) * 0.8):]\n",
        "\n",
        "    self.id_labels = id_labels\n",
        "    self.id_bounding_boxes = id_bounding_boxes\n",
        "    self.full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_path = ffile_path(self.imgs_key[idx], self.full_image_file_paths) \n",
        "    img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "    boxes = convert_min_max(torch.as_tensor(self.id_bounding_boxes[self.imgs_key[idx]], dtype=torch.float32))\n",
        "    \n",
        "    labels = torch.as_tensor(self.id_labels[self.imgs_key[idx]], dtype=torch.int64)\n",
        "    \n",
        "    #Query about transforms for labels of images\n",
        "    if self.transforms: \n",
        "      sample = {\n",
        "                'image': img,\n",
        "                'bboxes': boxes,\n",
        "                'labels': labels\n",
        "            }\n",
        "\n",
        "      sample = self.transforms(**sample)\n",
        "      img = sample['image']\n",
        "      boxes = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "\n",
        "    return {'image': img, 'bboxes': boxes, 'category_id': labels}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs_key)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmonKCf2YtY2"
      },
      "source": [
        "def detection_collate(batch):\n",
        "    imgs = [s['image'] for s in batch]\n",
        "    annots = [s['bboxes'] for s in batch]\n",
        "    labels = [s['category_id'] for s in batch]\n",
        "\n",
        "    max_num_annots = max(len(annot) for annot in annots)\n",
        "    annot_padded = np.ones((len(annots), max_num_annots, 5))*-1\n",
        "\n",
        "    if max_num_annots > 0:\n",
        "        for idx, (annot, lab) in enumerate(zip(annots, labels)):\n",
        "            if len(annot) > 0:\n",
        "                annot_padded[idx, :len(annot), :4] = annot\n",
        "                annot_padded[idx, :len(annot), 4] = lab\n",
        "    return (torch.stack(imgs, 0), torch.FloatTensor(annot_padded))\n",
        "\n",
        "train_batch_size = 1\n",
        "\n",
        "train_dataset = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"effdet_train\"), mode = \"train\")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= detection_collate)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC6a5bdEi-af"
      },
      "source": [
        "def train_effdet(train_loader, model, scheduler, optimizer, epoch):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    global iteration \n",
        "    print(\"{} epoch: \\t start training....\".format(epoch))\n",
        "    start = time.time()\n",
        "    total_loss = []\n",
        "    model.train()\n",
        "    model.module.is_training = True\n",
        "    model.module.freeze_bn()\n",
        "    optimizer.zero_grad()\n",
        "    for idx, (images, annotations) in enumerate(train_loader):\n",
        "        images = images.cuda().float()\n",
        "        annotations = annotations.cuda()\n",
        "\n",
        "        classification_loss, regression_loss = model([images, annotations])\n",
        "        classification_loss = classification_loss.mean()\n",
        "        regression_loss = regression_loss.mean()\n",
        "        loss = classification_loss + regression_loss\n",
        "        if bool(loss == 0):\n",
        "            print('loss equal zero(0)')\n",
        "            continue\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss.append(loss.item())\n",
        "        if(iteration % 50 == 0):\n",
        "            print('{} iteration: training ...'.format(iteration))\n",
        "            ans = {\n",
        "                'epoch': epoch,\n",
        "                'iteration': iteration,\n",
        "                'cls_loss': classification_loss.item(),\n",
        "                'reg_loss': regression_loss.item(),\n",
        "                'mean_loss': np.mean(total_loss)\n",
        "            }\n",
        "            for key, value in ans.items():\n",
        "                print('    {:15s}: {}'.format(str(key), value))\n",
        "        iteration += 1\n",
        "    scheduler.step(np.mean(total_loss))\n",
        "    result = {\n",
        "        'time': time.time() - start,\n",
        "        'loss': np.mean(total_loss)\n",
        "    }\n",
        "    for key, value in result.items():\n",
        "        print('    {:15s}: {}'.format(str(key), value))"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn0vmaR5aGcy"
      },
      "source": [
        "## This link shows the problem https://github.com/pytorch/vision/issues/2740\n",
        "The answer from @oke-aditya is correct. You are probably passing to the model bounding boxes in the format [xmin, ymin, width, height], while Faster R-CNN expects boxes to be in [xmin, ymin, xmax, ymax] format.\n",
        "\n",
        "Changing this should fix the issue.\n",
        "\n",
        "We have btw recently added box conversion utilities to torchvision (thanks to @oke-aditya ), they can be found in\n",
        "\n",
        "Look at box convert or doing it locally also works.\n",
        "\n",
        "### https://github.com/pytorch/vision/blob/a98e17e50146529cdfadb590ba063e6bbee71de2/torchvision/ops/boxes.py#L137-L156\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYAu4FDknTwH"
      },
      "source": [
        "### Let us try another bigger model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF1AnUSZienq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4864aed3-926b-4341-9428-abc05be0c8f1"
      },
      "source": [
        "MODEL_MAP = {\n",
        "    'efficientdet-d0': 'efficientnet-b0',\n",
        "    'efficientdet-d1': 'efficientnet-b1',\n",
        "    'efficientdet-d2': 'efficientnet-b2',\n",
        "    'efficientdet-d3': 'efficientnet-b3',\n",
        "    'efficientdet-d4': 'efficientnet-b4',\n",
        "    'efficientdet-d5': 'efficientnet-b5',\n",
        "    'efficientdet-d6': 'efficientnet-b6',\n",
        "    'efficientdet-d7': 'efficientnet-b6',\n",
        "}\n",
        "class EfficientDet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 network='efficientdet-d0',\n",
        "                 D_bifpn=3,\n",
        "                 W_bifpn=88,\n",
        "                 D_class=3,\n",
        "                 is_training=True,\n",
        "                 threshold=0.01,\n",
        "                 iou_threshold=0.5):\n",
        "        super(EfficientDet, self).__init__()\n",
        "        self.backbone = EfficientNet.from_pretrained(MODEL_MAP[network])\n",
        "        self.is_training = is_training\n",
        "        self.neck = BIFPN(in_channels=self.backbone.get_list_features()[-5:],\n",
        "                          out_channels=W_bifpn,\n",
        "                          stack=D_bifpn,\n",
        "                          num_outs=5)\n",
        "        self.bbox_head = RetinaHead(num_classes=num_classes,\n",
        "                                    in_channels=W_bifpn)\n",
        "\n",
        "        self.anchors = Anchors()\n",
        "        self.regressBoxes = BBoxTransform()\n",
        "        self.clipBoxes = ClipBoxes()\n",
        "        self.threshold = threshold\n",
        "        self.iou_threshold = iou_threshold\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "        self.freeze_bn()\n",
        "        self.criterion = FocalLoss()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.is_training:\n",
        "            inputs, annotations = inputs\n",
        "        else:\n",
        "            inputs = inputs\n",
        "        x = self.extract_feat(inputs)\n",
        "        outs = self.bbox_head(x)\n",
        "        classification = torch.cat([out for out in outs[0]], dim=1)\n",
        "        regression = torch.cat([out for out in outs[1]], dim=1)\n",
        "        anchors = self.anchors(inputs)\n",
        "        if self.is_training:\n",
        "            return self.criterion(classification, regression, anchors, annotations)\n",
        "        else:\n",
        "            transformed_anchors = self.regressBoxes(anchors, regression)\n",
        "            transformed_anchors = self.clipBoxes(transformed_anchors, inputs)\n",
        "            scores = torch.max(classification, dim=2, keepdim=True)[0]\n",
        "            scores_over_thresh = (scores > self.threshold)[0, :, 0]\n",
        "\n",
        "            if scores_over_thresh.sum() == 0:\n",
        "                print('No boxes to NMS')\n",
        "                # no boxes to NMS, just return\n",
        "                return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n",
        "            classification = classification[:, scores_over_thresh, :]\n",
        "            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n",
        "            scores = scores[:, scores_over_thresh, :]\n",
        "            anchors_nms_idx = nms(\n",
        "                transformed_anchors[0, :, :], scores[0, :, 0], iou_threshold=self.iou_threshold)\n",
        "            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(\n",
        "                dim=1)\n",
        "            return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]\n",
        "\n",
        "    def freeze_bn(self):\n",
        "        '''Freeze BatchNorm layers.'''\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.BatchNorm2d):\n",
        "                layer.eval()\n",
        "\n",
        "    def extract_feat(self, img):\n",
        "        \"\"\"\n",
        "            Directly extract features from the backbone+neck\n",
        "        \"\"\"\n",
        "        x = self.backbone(img)\n",
        "        x = self.neck(x[-5:])\n",
        "        return x\n",
        "\n",
        "model= EfficientDet(num_classes=4,is_training=True)\n",
        "model.train()\n",
        "\n",
        "model.freeze_bn()\n",
        "\n",
        "model = model.cuda()\n",
        "print('Run with DataParallel ....')\n",
        "\n",
        "## Make sure that you add this line, even though you are not using more than one \n",
        "# GPU DataParallel adds \"module\" to the start of the model structure \n",
        "# allowing for the syntax to be correct when calling \"model.module.freeze_bn()\" for example\n",
        "model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "# I am doing this here an example, you do not have to call the lines below here\n",
        "model.module.is_training = True\n",
        "model.module.freeze_bn()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Run with DataParallel ....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbYjFJSaUAPy",
        "outputId": "96b4e3b6-91d9-4a7e-8982-ba11095b0dd2"
      },
      "source": [
        "iteration = 1\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, patience=3, verbose=True)\n",
        "train_effdet(train_loader, model, scheduler, optimizer, 5)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 epoch: \t start training....\n",
            "50 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 50\n",
            "    cls_loss       : 2.302124500274658\n",
            "    reg_loss       : 1.1135210990905762\n",
            "    mean_loss      : 174.43925085544586\n",
            "100 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 100\n",
            "    cls_loss       : 2.302124500274658\n",
            "    reg_loss       : 1.0734187364578247\n",
            "    mean_loss      : 89.17448601007462\n",
            "150 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 150\n",
            "    cls_loss       : 2.302124500274658\n",
            "    reg_loss       : 1.046735405921936\n",
            "    mean_loss      : 60.59031775633494\n",
            "200 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 200\n",
            "    cls_loss       : 2.302124500274658\n",
            "    reg_loss       : 0.9701911807060242\n",
            "    mean_loss      : 46.32123940229416\n",
            "250 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 250\n",
            "    cls_loss       : 2.302124500274658\n",
            "    reg_loss       : 1.0268521308898926\n",
            "    mean_loss      : 37.74051753711701\n",
            "    time           : 36.026495695114136\n",
            "    loss           : 34.73887442063241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPmuew0CnX92"
      },
      "source": [
        "### The Mobile Net Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbDtIsBs-OWD"
      },
      "source": [
        "backbone = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
        "backbone.roi_heads.box_predictor.cls_score.out_features = 4\n",
        "backbone.roi_heads.box_predictor.bbox_pred.out_features = 16\n",
        "# backbone.roi_heads.box_predictor.cls_score.out_features = 3\n",
        "# backbone.roi_heads.box_predictor.bbox_pred.out_features = 12\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-cknP0_uy0u"
      },
      "source": [
        "# https://github.com/amdegroot/ssd.pytorch/blob/master/layers/box_utils.py#L48\n",
        "def intersect(box_a, box_b):\n",
        "\n",
        "    A = box_a.size(0)\n",
        "    B = box_b.size(0)\n",
        "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
        "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
        "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
        "    return inter[:, :, 0] * inter[:, :, 1]\n",
        "\n",
        "def jaccard_iou(box_a, box_b):\n",
        "\n",
        "    inter = intersect(box_a, box_b)\n",
        "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
        "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
        "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
        "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
        "    union = area_a + area_b - inter\n",
        "    return inter / union  # [A,B]\n",
        "\n",
        "def calculate_iou_on_label(results, len_of_results, iou_thresh, device):\n",
        "  for current_index, _ in enumerate(results[\"boxes\"]):\n",
        "    if current_index >= len_of_results:\n",
        "      break\n",
        "\n",
        "    current_index_iou = jaccard_iou(results[\"boxes\"][current_index].view(1, -1).to(device),\n",
        "                                    results[\"boxes\"].to(device))\n",
        "    \n",
        "    mask = (current_index_iou > iou_thresh) & (current_index_iou != 1)\n",
        "    mask = mask.squeeze()\n",
        "    for key in results:\n",
        "      results[key] = results[key][~mask]\n",
        "\n",
        "    len_of_results -= sum(mask)\n",
        "  \n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFhjlggXlAx7"
      },
      "source": [
        "def infer_image(image_file_path, trained_model, distance_thresh, iou_thresh, webcam = False, show_image = True):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  #Just load it up as PIL. Avoid using cv2 because do not need albumentations\n",
        "  if not webcam:\n",
        "    torch_image = F.to_tensor(Image.open(image_file_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    trained_model.to(device)\n",
        "    trained_model.eval()\n",
        "    print(\"Image Size: {}\".format(torch_image.size()))\n",
        "\n",
        "    start_time = time.time()\n",
        "    results = trained_model(torch_image)\n",
        "    end_time = time.time() - start_time\n",
        "\n",
        "    print(\"Time of Inference {:0.2f}\".format(end_time))\n",
        "  else:\n",
        "\n",
        "    torch_image = F.to_tensor(image_file_path).unsqueeze(1).to(device)\n",
        "\n",
        "    results = trained_model(torch_image)\n",
        "\n",
        "  valid_box_count = 0\n",
        "  for ii, score in enumerate(results[0][\"scores\"]):\n",
        "    if score < distance_thresh:\n",
        "      low_index_start = ii\n",
        "      break\n",
        "    else:\n",
        "      valid_box_count += 1\n",
        "\n",
        "  if valid_box_count == len(results[0][\"scores\"]):\n",
        "    low_index_start = len(results[0][\"scores\"])\n",
        "  \n",
        "  for key in results[0]:\n",
        "    results[0][key] = results[0][key][:low_index_start]\n",
        "  \n",
        "  #This is where I place the order of the list\n",
        "  fruit_spot_iou_thresh, bad_spot_iou_thresh = iou_thresh\n",
        "\n",
        "  bad_spot_index = [ii for ii, label in enumerate(results[0][\"labels\"]) if label == 3 or label == 4]\n",
        "  fruit_index = [ii for ii, _ in enumerate(results[0][\"labels\"]) if ii not in bad_spot_index]\n",
        "\n",
        "  bad_spot_results, fruit_results = dict(), dict()\n",
        "\n",
        "  for key in results[0]:\n",
        "    bad_spot_results[key], fruit_results[key] = results[0][key][[bad_spot_index]], results[0][key][[fruit_index]]\n",
        "\n",
        "  assert len(bad_spot_results[\"boxes\"]) == len(bad_spot_results[\"scores\"]) == len(bad_spot_results[\"labels\"])\n",
        "  assert len(fruit_results[\"boxes\"]) == len(fruit_results[\"scores\"]) == len(fruit_results[\"labels\"])\n",
        "\n",
        "  len_of_bad_spots, len_of_fruit = len(bad_spot_results[\"boxes\"]), len(fruit_results[\"boxes\"])\n",
        "\n",
        "  if len_of_bad_spots > 1:\n",
        "    bad_spot_results = calculate_iou_on_label(bad_spot_results, len_of_bad_spots, bad_spot_iou_thresh, device)\n",
        "  if len_of_fruit > 1:\n",
        "    fruit_results = calculate_iou_on_label(fruit_results, len_of_fruit, fruit_spot_iou_thresh, device)\n",
        "  \n",
        "  for key in results[0]: \n",
        "    if (key == \"boxes\"):\n",
        "      results[0][\"boxes\"] = torch.cat((fruit_results[\"boxes\"], bad_spot_results[\"boxes\"]), axis = 0)\n",
        "    else:\n",
        "      results[0][key] = torch.cat((fruit_results[key], bad_spot_results[key]), dim = 0)\n",
        "\n",
        "  if show_image:\n",
        "    if device == torch.device(\"cuda\"):\n",
        "      torch_image = torch_image.cpu() \n",
        "    written_image = cv2.cvtColor(draw_boxes(results[0][\"boxes\"], results[0][\"labels\"], torch_image.squeeze(), infer = True, put_text= True), cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(written_image)\n",
        "  \n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZd4nVDuiu4M"
      },
      "source": [
        "import base64\n",
        "import html\n",
        "import io\n",
        "import time\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "def start_input():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      /* try changing the capture canvas and see what happens*/\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def take_photo(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T45EScTjJ5e"
      },
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def js_reply_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          js_reply: JavaScript object, contain image from webcam\n",
        "\n",
        "    output: \n",
        "          image_array: image array RGB size 512 x 512 from webcam\n",
        "    \"\"\"\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\n",
        "    image_array = np.array(image_PIL)\n",
        "\n",
        "    return image_array\n",
        "\n",
        "def drawing_array_to_bytes(drawing_array):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          drawing_array: image RGBA size 512 x 512 \n",
        "                              contain bounding box and text from yolo prediction, \n",
        "                              channel A value = 255 if the pixel contains drawing properties (lines, text) \n",
        "                              else channel A value = 0\n",
        "\n",
        "    output: \n",
        "          drawing_bytes: string, encoded from drawing_array\n",
        "    \"\"\"\n",
        "\n",
        "    drawing_PIL = Image.fromarray(drawing_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    drawing_PIL.save(iobuf, format='png')\n",
        "    drawing_bytes = 'data:image/png;base64,{}'.format((str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "    return drawing_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FSrjfHjgX4q"
      },
      "source": [
        "data_transforms = get_transforms(mode = \"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "j0WidBmGlktM",
        "outputId": "966e0c06-eb6b-4bb4-f10a-9b5c7087947b"
      },
      "source": [
        "start_input()\n",
        "label_html = 'Capturing...'\n",
        "img_data = ''\n",
        "count = 0 \n",
        "\n",
        "color=None\n",
        "label=None\n",
        "line_thickness=None\n",
        "another_one.to(device).eval();\n",
        "while True:\n",
        "    js_reply = take_photo(label_html, img_data)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    image = js_reply_to_image(js_reply)\n",
        "    prediciton = infer_image(image, another_one, 0.2, [0.3, 0.1], webcam= True, show_image = False)\n",
        "\n",
        "    drawing_array = np.zeros([512,512,4], dtype=np.uint8)\n",
        "\n",
        "    for x in prediciton[0]['boxes']:\n",
        "\n",
        "      tl = line_thickness or round(0.002 * (drawing_array.shape[0] + drawing_array.shape[1]) / 2) + 1  # line/font thickness\n",
        "      color = color or [random.randint(0, 255) for _ in range(3)]\n",
        "      c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
        "      cv2.rectangle(drawing_array, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
        "      if label:\n",
        "        tf = max(tl - 1, 1)  # font thickness\n",
        "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
        "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
        "        cv2.rectangle(drawing_array, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
        "        cv2.putText(drawing_array, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "    drawing_array[:,:,3] = (drawing_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "\n",
        "    drawing_PIL = Image.fromarray(drawing_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    drawing_PIL.save(iobuf, format='png')\n",
        "    drawing_bytes = 'data:image/png;base64,{}'.format((str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "    img_data = drawing_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      /* try changing the capture canvas and see what happens*/\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 512; //video.videoWidth;\n",
              "      captureCanvas.height = 512; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function takePhoto(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1GzIoXeKh0v"
      },
      "source": [
        "def train(net, epochs, train_loader, test_loader, lr, weight_decay, \n",
        "          print_every = 6, lo_test_dataset = len(test_dataset), lo_train_dataset = len(train_dataset),\n",
        "          model_type_extra = None):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Check which parameters can calculate gradients. \n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "\n",
        "    optimizer = Ranger(net.parameters(), lr = lr, weight_decay= weight_decay)\n",
        "    # optimizer = sam.SAM(net.parameters(), base_optimizer, lr = lr, weight_decay = weight_decay)\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
        "\n",
        "    net.to(device)\n",
        "    print(\"Device: {}\".format(device))\n",
        "    print(\"Optimizer: {}\".format(optimizer))\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        \n",
        "        train_loss = steps = train_mAP = train_mAR = missed_train_images = 0\n",
        "        \n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "            net.train()\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            net.eval()\n",
        "            train_mAP, train_mAR, missed_train_images = run_metrics_for_batch(net(images), targets, train_mAP, train_mAR, missed_train_images, device)\n",
        "            net.train()\n",
        "\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # optimizer.first_step(zero_grad = True)\n",
        "\n",
        "            # loss_dict = net(images, targets)\n",
        "\n",
        "            # losses = sum(loss for loss in loss_dict.values())\n",
        "            # losses.backward()\n",
        "            # optimizer.second_step(zero_grad = True)\n",
        "\n",
        "            train_loss +=  losses.item()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (steps % print_every) == 0:\n",
        "\n",
        "              with torch.no_grad():\n",
        "                test_mAP = test_mAR = missed_test_images = test_loss = 0\n",
        "\n",
        "                for images, targets in test_loader:\n",
        "\n",
        "                  net.eval()\n",
        "                  if device == torch.device(\"cuda\"):\n",
        "                    images = [image.to(device) for image in images]\n",
        "                    targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "                  output = net(images)\n",
        "                  test_mAP, test_mAR, missed_test_images = run_metrics_for_batch(output, targets, test_mAP, test_mAR, missed_test_images, device)\n",
        "\n",
        "                  net.train()\n",
        "                  test_loss_dict = net(images, targets)\n",
        "                  test_losses = sum(loss for loss in test_loss_dict.values())\n",
        "                  test_loss += test_losses.item()\n",
        "\n",
        "                for param_group in optimizer.param_groups:\n",
        "                  learning_rate_extract = param_group[\"lr\"]\n",
        "                print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.2f} | Train_loss: {:0.2f} | Test_loss: {:0.2f} | Test mAP: {:0.2f}% | Test mAR: {:0.2f}% | Missed Test Images: {}\".format(\n",
        "                    epoch + 1, epochs, steps, learning_rate_extract, train_loss, test_loss,  \n",
        "                    (test_mAP / float(lo_test_dataset)) * 100., (test_mAR / float(lo_test_dataset)) * 100.,missed_test_images))\n",
        "\n",
        "              assert (steps % print_every) == 0\n",
        "              train_loss = 0\n",
        "                 \n",
        "        print(\"\\n Epoch {} Final Train mAP: {:0.2f}% | Epoch {} Final Train mAR: {:0.2f}% | Epoch {} Final Missed Train Images: {} out of {} images \\n\".format(\n",
        "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, (train_mAR / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, missed_train_images, lo_train_dataset\n",
        "        ))\n",
        "    \n",
        "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))\n",
        "\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}