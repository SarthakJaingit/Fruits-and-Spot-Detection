{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FruitObjectDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZfnWC_NF7NM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch \n",
        "from torch import nn, optim \n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import itertools\n",
        "import glob \n",
        "from PIL import Image\n",
        "import csv \n",
        "import cv2\n",
        "import re\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.ops.boxes import box_iou\n",
        "import random\n",
        "import torchvision\n",
        "import warnings\n",
        "\n",
        "!pip install --upgrade albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2, ToTensor\n",
        "!git clone https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
        "%cd Ranger-Deep-Learning-Optimizer\n",
        "!pip install -e .\n",
        "from ranger import Ranger  \n",
        "%cd ..\n",
        "#https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1\n",
        "!git clone https://github.com/davda54/sam.git\n",
        "%cd sam\n",
        "import sam\n",
        "print(\"Imported SAM Successfully from github .py file\")\n",
        "%cd ..\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_pckeYPGF0b",
        "outputId": "635c9293-ddab-4b6f-baf8-eb5c1e28f336"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCS2doQYbNpf"
      },
      "source": [
        "## To Do Right Now: \n",
        "\n",
        "### Steps for fully integrating effecient det \n",
        "* Try to understand why eff det is failing. Maybe visualize images to see if bouding boxes are correct.\n",
        "\n",
        "later: \n",
        "* Understand Effecient Det Data Parralel code\n",
        "\n",
        "\n",
        "### Labeling the other data on peaches and tomatoes label them and put them in dataset.\n",
        "\n",
        "### Look at new project ideas from the DOCUMENT. Read about new ideas into agriculture and look for problems that can be solved with computer vision.\n",
        "\n",
        "#Possible Solution\n",
        "\n",
        "### go to ImageNet and get images of round classes so like human faces or dog faces or balls etc. (have no bounding boxes). Get a new loader called noise_loader and make a loss that sees how many bouding boxes are predicted in the noise_loader class.\n",
        "\n",
        "# Goal \n",
        "Get model running on web cam on like 10 different fruits / vegetables\n",
        "\n",
        "#Less Urgent Ideas in the Future:\n",
        "\n",
        "### Idea for Increased functionality: Make code that counts fruits and fruits with bad_spots. For every fruit bounding box group fruit into no badspot, 2-4 (little amount of small badspots), big glob, huge amount of bad spots etc). \n",
        "\n",
        "### This will be in end when all my modeling, data collecting, and experimenting is done. Look into turning jupyter notebook into python script or deploy it to rasberry pi. Makes sure when converting to python script be aware of classes variable.\n",
        "\n",
        " \n",
        "### https://www.emerginginvestigators.org/articles?category_id=10\n",
        "\n",
        "### Train, Once I get a model with very good results. torch.save it state_dicts on my local disk. Also record results of models on results spreadsheet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4UKDVNnIqQN"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/LatestFruit Defects Dataset .zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/EfficientDet.Pytorch-Updated.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1cWaZ4DSJp3",
        "outputId": "0796872d-b003-44e1-953f-1d69353bd2cc"
      },
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "%cd EfficientDet.Pytorch-Updated/\n",
        "import math\n",
        "from models.efficientnet import EfficientNet\n",
        "from models.bifpn import BIFPN\n",
        "from models.retinahead import RetinaHead\n",
        "from models.module import RegressionModel, ClassificationModel, Anchors, ClipBoxes, BBoxTransform\n",
        "from torchvision.ops import nms\n",
        "from models.losses import FocalLoss\n",
        "from models.efficientdet import EfficientDet\n",
        "from models.losses import FocalLoss\n",
        "from datasets import VOCDetection, CocoDataset, get_augumentation, detection_collate, Resizer, Normalizer, Augmenter, collater\n",
        "from utils import EFFICIENTDET, get_state_dict\n",
        "from eval import evaluate, evaluate_coco\n",
        "%cd .."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/EfficientDet.Pytorch-Updated\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY83cUEFAjoW",
        "outputId": "45d1b2e9-2eab-432f-decc-f312350f4326"
      },
      "source": [
        "#For one strawberry batch please drop watermark rows\n",
        "strawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 3 Labeled/FreshStrawberryBatch3Labels.csv\", header = None)\n",
        "strawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 2 Labeled/FreshStrawberriesBatch2Labels.csv\", header = None)\n",
        "strawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 1 Labeled/Strawberrybatch1.csv\", header = None)\n",
        "rottenApple_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch1Labeled/RottenAppleBatch1Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch2Labeled/RottenApplesBatch2Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch3Labaled/RottenApplesBatch3Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch1RottenStrawBerryLabels/RottenStrawberriesBatch1Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch2RottenStrawBerryLabels/RottenStrawBerryBatch2.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch3RottenStrawberrylabel/rottenStrawberryBtch3labels.csv\", header = None)\n",
        "freshApples_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplebtch2label/FreshApplesBatch2LabelsFresh.csv\", header = None)\n",
        "freshApples_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplesBatch1Labels/FreshAppleBatch1Labels.csv\", header = None)\n",
        "rottenTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/Rotten TomatoBatch1/Batch1TomoatosLabelsBbox.csv\", header = None)\n",
        "rottenTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottnTomatoBatch2/RottenTomatyoBatch2Labelss.csv\", header = None)\n",
        "rottenTomato_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatBtch3/RottenTomatoesBatch3Labssles.csv\", header = None)\n",
        "rottenTomato_csv_batch_4 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatoesBatch4/Tomatobatch4labelssRotten.csv\", header = None)\n",
        "freshTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatoesBatch1Labelss/FreshTomatoesLabelsBatch1Labels.csv\", header = None)\n",
        "freshTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatBatch2Labessls/Batch2TomatlabelsFresh.csv\", header = None)\n",
        "\n",
        "strawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_3.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_4.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "\n",
        "#Drop some watermark data for Fresh StrawBerry Batch 1 Labeled images [59, 9, 93]\n",
        "\n",
        "# strawberry_csv_batch_1 = strawberry_csv_batch_1[Image_id not in [\"FreshStrawberries59.jpeg, FreshStrawberries9.jpeg, FreshStrawberries93.jpeg\"]]\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries59.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries9.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries93.jpeg\"].index, inplace = True)\n",
        "freshTomato_csv_batch_1.drop(freshTomato_csv_batch_1[freshTomato_csv_batch_1[\"Image_id\"] == \"Fresh Tomatoes66AddonPart1.jpeg\"].index, inplace = True)\n",
        "\n",
        "strawberry_csv_batch_1 = strawberry_csv_batch_1.reset_index(drop=True)\n",
        "freshTomato_csv_batch_1 = freshTomato_csv_batch_1.reset_index(drop = True)\n",
        "\n",
        "#Stack all the csv files together. \n",
        "list_of_all_dataframes = [strawberry_csv_batch_1, strawberry_csv_batch_2, strawberry_csv_batch_3, rottenApple_csv_batch_1, \n",
        "                          rottenApple_csv_batch_2, rottenApple_csv_batch_3, rottenStrawberry_csv_batch_1, rottenStrawberry_csv_batch_2, \n",
        "                          rottenStrawberry_csv_batch_3, freshApples_csv_batch_2, freshApples_csv_batch_1, rottenTomato_csv_batch_1, \n",
        "                          rottenTomato_csv_batch_2, rottenTomato_csv_batch_3, rottenTomato_csv_batch_4, freshTomato_csv_batch_1, \n",
        "                          freshTomato_csv_batch_2]\n",
        "fruit_df = pd.concat(list_of_all_dataframes, ignore_index = True)\n",
        "\n",
        "total_row_sum_check = 0 \n",
        "for dataframe in list_of_all_dataframes:\n",
        "  total_row_sum_check += dataframe.shape[0]\n",
        "print(\"Checked total rows from all the dataframes combined: {}\".format(total_row_sum_check))\n",
        "\n",
        "def run_dataframe_check():\n",
        "  assert total_row_sum_check == fruit_df.shape[0]\n",
        "  print(\"DataFrame shape: {}\".format(fruit_df.shape))\n",
        "  print(\"Unique Fruit Labels {}\".format(fruit_df[\"Fruit\"].unique()))\n",
        "  print(\"Number of Unique Images {}\".format(len(fruit_df[\"Image_id\"].unique())))\n",
        "\n",
        "run_dataframe_check()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checked total rows from all the dataframes combined: 1882\n",
            "DataFrame shape: (1882, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Bad_Spots' 'Tomato']\n",
            "Number of Unique Images 532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KalFI9OONbjp",
        "outputId": "c65ea194-9e03-426a-be34-e2a54d7273ea"
      },
      "source": [
        "#Specify more image types when \n",
        "def more_specific_Image_id(image_id, fruit):\n",
        "  if fruit == \"Bad_Spots\":\n",
        "    if re.search(\"RottenStrawberries\", image_id):\n",
        "      return \"Strawberry_Bad_Spot\"\n",
        "    elif re.search(\"RottenApples\", image_id):\n",
        "      return \"Apple_Bad_Spot\"\n",
        "    elif re.search(\"Rotten Tomatoes\", image_id):\n",
        "      return \"Tomato_Bad_Spot\"\n",
        "    else:\n",
        "      raise ValueError(\"Could not find a match for some of the Image_ids\")\n",
        "\n",
        "  else:\n",
        "    return fruit\n",
        "\n",
        "fruit_df[\"Fruit\"] = fruit_df.apply(lambda row: more_specific_Image_id(row.Image_id, row.Fruit), axis = 1)\n",
        "\n",
        "run_dataframe_check()\n",
        "\n",
        "#Post Processing \n",
        "fruit_df = fruit_df[fruit_df[\"Image_id\"] != \"FreshStrawberries15.jpeg\"]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame shape: (1882, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Apple_Bad_Spot' 'Strawberry_Bad_Spot' 'Tomato'\n",
            " 'Tomato_Bad_Spot']\n",
            "Number of Unique Images 532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIoosfwlUAdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c6bcaf-7b20-49ee-d1bf-26892626cc72"
      },
      "source": [
        "bounding_box_dict = dict()\n",
        "labels_dict = dict()\n",
        "classes = [\"Placeholder\", \"Apples\", \"Strawberry\", \"Tomato\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\", \"Tomato_Bad_Spot\"]\n",
        "# classes = [\"Apples\", \"Strawberry\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\"]\n",
        "print(classes)\n",
        "# classes = [\"Bad_Spots\", \"Strawberry\", \"Apples\"]\n",
        "\n",
        "for row_index in range(len(fruit_df)): \n",
        "  current_image_file = fruit_df.iloc[row_index][\"Image_id\"]\n",
        "  if current_image_file not in bounding_box_dict:\n",
        "    bounding_box_dict[current_image_file] = list()\n",
        "    labels_dict[current_image_file] = list()\n",
        "  bounding_box_dict[current_image_file].append(fruit_df.iloc[row_index, 1:5].to_list())\n",
        "  labels_dict[current_image_file].append(classes.index(fruit_df.iloc[row_index, 0]))\n",
        "\n",
        "print(len(bounding_box_dict))\n",
        "print(len(labels_dict))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Placeholder', 'Apples', 'Strawberry', 'Tomato', 'Apple_Bad_Spot', 'Strawberry_Bad_Spot', 'Tomato_Bad_Spot']\n",
            "531\n",
            "531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15qhrCwGUPxp"
      },
      "source": [
        "## Class function + util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgVTPiupUTQa"
      },
      "source": [
        "def ffile_path(image_id, full_image_file_paths):\n",
        "  for image_path in full_image_file_paths:\n",
        "    if image_id in image_path:\n",
        "      return image_path\n",
        "\n",
        "def find_area_bb(bb_coord):\n",
        "  bb_coord = bb_coord.numpy()\n",
        "  area_of_each_bb = list()\n",
        "  for pair_of_coord in bb_coord:\n",
        "    area_of_each_bb.append(\n",
        "        (pair_of_coord[2] - pair_of_coord[0]) * (pair_of_coord[3] - pair_of_coord[1])\n",
        "    )\n",
        "  return torch.tensor(area_of_each_bb, dtype=torch.int32)\n",
        "\n",
        "def convert_min_max(bb_coord):\n",
        "  for pair_of_coord in bb_coord:\n",
        "    pair_of_coord[2], pair_of_coord[3] = (pair_of_coord[0] + pair_of_coord[-2]), (pair_of_coord[1] + pair_of_coord[-1])\n",
        "  return bb_coord\n",
        "\n",
        "class FruitDetectDataset(object):\n",
        "  def __init__(self, id_labels, id_bounding_boxes, transforms, mode):\n",
        "\n",
        "    assert len(id_labels) == len(id_bounding_boxes)\n",
        "    assert sorted(id_labels.keys()) == sorted(id_bounding_boxes.keys())\n",
        "    self.imgs_key = sorted(id_labels.keys())\n",
        "\n",
        "    np.random.shuffle(self.imgs_key)\n",
        "    if (mode == \"train\"):\n",
        "      self.imgs_key = self.imgs_key[:int(len(self.imgs_key) * 0.8)]\n",
        "    else:\n",
        "      self.imgs_key = self.imgs_key[int(len(self.imgs_key) * 0.8):]\n",
        "\n",
        "    self.id_labels = id_labels\n",
        "    self.id_bounding_boxes = id_bounding_boxes\n",
        "    self.full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_path = ffile_path(self.imgs_key[idx], self.full_image_file_paths) \n",
        "    img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "    boxes = convert_min_max(torch.as_tensor(self.id_bounding_boxes[self.imgs_key[idx]], dtype=torch.float32))\n",
        "    \n",
        "    labels = torch.as_tensor(self.id_labels[self.imgs_key[idx]], dtype=torch.int64)\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = find_area_bb(boxes)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    \n",
        "    #Query about transforms for labels of images\n",
        "    if self.transforms: \n",
        "      sample = {\n",
        "                'image': img,\n",
        "                'bboxes': target['boxes'],\n",
        "                'labels': labels\n",
        "            }\n",
        "\n",
        "      sample = self.transforms(**sample)\n",
        "      img = sample['image']\n",
        "      target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "    \n",
        "    \n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs_key)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDh2_pg4J2yo"
      },
      "source": [
        "#The Drawing function.\n",
        "# COLORS = [(255, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0)]\n",
        "COLORS = [(0, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0), (255, 0, 0)]\n",
        "\n",
        "def draw_boxes(boxes, labels, image, infer = False, put_text = True):\n",
        "    # read the image with OpenCV\n",
        "    image = image.permute(1, 2, 0).numpy()\n",
        "    if infer:\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    for i, box in enumerate(boxes):\n",
        "        color = COLORS[labels[i] % len(COLORS)]\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (int(box[0]), int(box[1])),\n",
        "            (int(box[2]), int(box[3])),\n",
        "            color, 2\n",
        "        )\n",
        "        if put_text:\n",
        "          cv2.putText(image, classes[labels[i]], (int(box[0]), int(box[1]-5)),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n",
        "                      lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "# Albumentations\n",
        "def get_transforms(mode):\n",
        "  if (mode == \"train\"):\n",
        "    return A.Compose([\n",
        "                      A.OneOf([\n",
        "                      A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                     val_shift_limit=0.2, p=0.9),\n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "                      A.HorizontalFlip(),\n",
        "                      A.VerticalFlip(), \n",
        "                      # A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), p=1),\n",
        "                      # ToTensor(),\n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"test\"):\n",
        "    return A.Compose([\n",
        "                      # A.Resize(512, 512), \n",
        "                      # A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                      # std=(0.229, 0.224, 0.225), p=1),\n",
        "                      # ToTensor()\n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"effdet_train\"):\n",
        "    return A.Compose([\n",
        "                      A.OneOf([\n",
        "                      A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                     val_shift_limit=0.2, p=0.9),\n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "                      A.HorizontalFlip(),\n",
        "                      A.VerticalFlip(), \n",
        "                      A.Resize(height = 512, width=512), \n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"effdet_test\"):\n",
        "    return A.Compose([\n",
        "                      A.Resize(height = 512, width = 512), \n",
        "                      ToTensorV2()])\n",
        "  else:\n",
        "    raise ValueError(\"mode is wrong value can either be train or test\")\n",
        "\n",
        "#Using this stack overflow (https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders)\n",
        "#(Suppose for example, you want to create batches of a list of varying dimension tensors. The below code pads sequences with 0 until the maximum sequence size of the batch,)\n",
        "#Collate_fn is a function that is used to process your batches before you pass it to dataloader. In my case since I have different sized images I need a way to stack batches b/c torch.stack won't work.\n",
        "#So I use zip which can accept tensors of different lengths and make them stacked with the size of the lowest length list given. Therefore stacking all the images in a batch \n",
        "#Successfully unlike torch.stack and doing that processing to every batch makes collate_fn vital since I have different image sizes.\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return tuple([list(a) for a in zip(*batch)])\n",
        "    # return tuple(zip(*batch))\n",
        "\n",
        "train_batch_size = 1\n",
        "test_batch_size = 1\n",
        "\n",
        "train_dataset = FruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"train\"), mode = \"train\")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= collate_fn)\n",
        "\n",
        "test_dataset = FruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"test\"), mode = \"test\")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = test_batch_size, shuffle = True, collate_fn= collate_fn)\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "-uf9fdpPK-SQ",
        "outputId": "960a7c8c-309c-4995-cc2b-23190e95b084"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    image = draw_boxes(labels[idx][\"boxes\"], labels[idx][\"labels\"], images[idx])\n",
        "    plt.imshow(image)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-2c30b5b4a517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A0EkOWcPSX0"
      },
      "source": [
        "## What we learned\n",
        "is that the model outputs losses when in train mode \n",
        "when in model.eval model, the model code then return only a prediction with no losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NQrBD22VWxD"
      },
      "source": [
        "def calculate_metrics(target_box,predictions_box,scores, device):\n",
        "\n",
        "    #Get most confident boxes first and least confident last\n",
        "    predictions_box = predictions_box[scores.argsort().flip(-1)]\n",
        "    iou_mat = box_iou(target_box,predictions_box)\n",
        "    #return a one by one matrix that is form (target_box, prediction_box) or (1, 1)\n",
        "    target_boxes_count, prediction_boxes_count = iou_mat.shape\n",
        "    \n",
        "    mAP_Matrix = torch.zeros_like(iou_mat)\n",
        "    # if not matrix coordinates that relate to nothing.\n",
        "    if not iou_mat[:,0].eq(0.).all():\n",
        "      index_of_biggest_iou = iou_mat[:,0].argsort()[-1]\n",
        "      mAP_Matrix[index_of_biggest_iou,0] = 1\n",
        "\n",
        "    for pr_idx in range(1,prediction_boxes_count):\n",
        "        not_assigned = torch.logical_not(mAP_Matrix[:,:pr_idx].sum(1)).long()\n",
        "        targets = not_assigned * iou_mat[:,pr_idx]\n",
        "\n",
        "        if targets.eq(0).all():\n",
        "            continue\n",
        "\n",
        "        pivot = targets.argsort()[-1]\n",
        "        mAP_Matrix[pivot,pr_idx] = 1\n",
        "\n",
        "    # mAP calculation\n",
        "    tp = mAP_Matrix.sum()\n",
        "    fp = mAP_Matrix.sum(0).eq(0).sum()\n",
        "    fn = mAP_Matrix.sum(1).eq(0).sum()\n",
        "\n",
        "    mAP = tp / (tp+fp)\n",
        "    mAR = tp / (tp+fn)\n",
        "\n",
        "    return mAP, mAR\n",
        "\n",
        "def run_metrics_for_batch(output, targets, mAP, mAR, missed_images, device):\n",
        "  for pos_in_batch, image_pred in enumerate(output):\n",
        "    assert (len(image_pred[\"boxes\"]) == len(image_pred[\"labels\"]) == len(image_pred[\"scores\"]))\n",
        "    if len(image_pred[\"boxes\"]) != 0:\n",
        "      curr_mAP, curr_mAR = calculate_metrics(targets[pos_in_batch][\"boxes\"], output[pos_in_batch][\"boxes\"], output[pos_in_batch][\"scores\"], device)\n",
        "      mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
        "    else:\n",
        "      missed_images += 1 \n",
        "  \n",
        "  return mAP, mAR, missed_images\n",
        "\n",
        "def run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device):\n",
        "    assert (len(scores) == len(classification) == len(transformed_anchors))\n",
        "    if len(transformed_anchors) != 0:\n",
        "      curr_mAP, curr_mAR = calculate_metrics(targets[0][:, :4], transformed_anchors, scores, device)\n",
        "      mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
        "    else:\n",
        "      missed_images += 1 \n",
        "      \n",
        "    return mAP, mAR, missed_images\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnukB06TZqTb"
      },
      "source": [
        "https://pypi.org/project/pytorch-warmup/ link for doing warmup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB_w3zeIOa8X"
      },
      "source": [
        "def train(net, epochs, train_loader, test_loader, lr, weight_decay, \n",
        "          print_every = 6, lo_test_dataset = len(test_dataset), lo_train_dataset = len(train_dataset)):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Check which parameters can calculate gradients. \n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "\n",
        "    # optimizer = Ranger(net.parameters(), lr = lr, weight_decay= weight_decay)\n",
        "    base_optimizer = Ranger\n",
        "    optimizer = sam.SAM(net.parameters(), base_optimizer, lr = lr, weight_decay = weight_decay)\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
        "\n",
        "    net.to(device)\n",
        "    print(\"Device: {}\".format(device))\n",
        "    print(\"Optimizer: {}\".format(optimizer))\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        \n",
        "        train_loss = steps = train_mAP = train_mAR = missed_train_images = 0\n",
        "        \n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "            net.train()\n",
        "\n",
        "            steps += 1\n",
        "            \n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            net.eval()\n",
        "            train_mAP, train_mAR, missed_train_images = run_metrics_for_batch(net(images), targets, train_mAP, train_mAR, missed_train_images, device)\n",
        "            net.train()\n",
        "\n",
        "            losses.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            optimizer.first_step(zero_grad = True)\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            losses.backward()\n",
        "            optimizer.second_step(zero_grad = True)\n",
        "\n",
        "            train_loss +=  losses.item()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (steps % print_every) == 0:\n",
        "\n",
        "              with torch.no_grad():\n",
        "                test_mAP = test_mAR = missed_test_images = test_loss = 0\n",
        "\n",
        "                for images, targets in test_loader:\n",
        "\n",
        "                  net.eval()\n",
        "                  if device == torch.device(\"cuda\"):\n",
        "                    images = [image.to(device) for image in images]\n",
        "                    targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "                  output = net(images)\n",
        "                  test_mAP, test_mAR, missed_test_images = run_metrics_for_batch(output, targets, test_mAP, test_mAR, missed_test_images, device)\n",
        "\n",
        "                  net.train()\n",
        "                  test_loss_dict = net(images, targets)\n",
        "                  test_losses = sum(loss for loss in test_loss_dict.values())\n",
        "                  test_loss += test_losses.item()\n",
        "\n",
        "                for param_group in optimizer.param_groups:\n",
        "                  learning_rate_extract = param_group[\"lr\"]\n",
        "                print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Test_loss: {:0.2f} | Test mAP: {:0.2f}% | Test mAR: {:0.2f}% | Missed Test Images: {}\".format(\n",
        "                    epoch + 1, epochs, steps, learning_rate_extract, train_loss, test_loss,  \n",
        "                    (test_mAP / float(lo_test_dataset)) * 100., (test_mAR / float(lo_test_dataset)) * 100.,missed_test_images))\n",
        "\n",
        "              assert (steps % print_every) == 0\n",
        "              train_loss = 0\n",
        "                 \n",
        "        print(\"\\n Epoch {} Final Train mAP: {:0.2f}% | Epoch {} Final Train mAR: {:0.2f}% | Epoch {} Final Missed Train Images: {} out of {} images \\n\".format(\n",
        "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, (train_mAR / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, missed_train_images, lo_train_dataset\n",
        "        ))\n",
        "    \n",
        "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))\n",
        "\n",
        "    return net"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYAu4FDknTwH"
      },
      "source": [
        "# Effecient Det Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjFx0ZuYFca"
      },
      "source": [
        "class EffdetFruitDetectDataset(object):\n",
        "  def __init__(self, id_labels, id_bounding_boxes, transforms, mode):\n",
        "\n",
        "    assert len(id_labels) == len(id_bounding_boxes)\n",
        "    assert sorted(id_labels.keys()) == sorted(id_bounding_boxes.keys())\n",
        "    self.imgs_key = sorted(id_labels.keys())\n",
        "\n",
        "    np.random.shuffle(self.imgs_key)\n",
        "    if (mode == \"train\"):\n",
        "      self.imgs_key = self.imgs_key[:int(len(self.imgs_key) * 0.8)]\n",
        "    else:\n",
        "      self.imgs_key = self.imgs_key[int(len(self.imgs_key) * 0.8):]\n",
        "\n",
        "    self.id_labels = id_labels\n",
        "    self.id_bounding_boxes = id_bounding_boxes\n",
        "    self.full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_path = ffile_path(self.imgs_key[idx], self.full_image_file_paths) \n",
        "    img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "    boxes = convert_min_max(torch.as_tensor(self.id_bounding_boxes[self.imgs_key[idx]], dtype=torch.float32))\n",
        "    \n",
        "    labels = torch.as_tensor(self.id_labels[self.imgs_key[idx]], dtype=torch.int64)\n",
        "    \n",
        "    #Query about transforms for labels of images\n",
        "    if self.transforms: \n",
        "      sample = {\n",
        "                'image': img,\n",
        "                'bboxes': boxes,\n",
        "                'labels': labels\n",
        "            }\n",
        "\n",
        "      sample = self.transforms(**sample)\n",
        "      img = sample['image']\n",
        "      boxes = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "\n",
        "    return {'image': img, 'bboxes': boxes, 'category_id': labels}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs_key)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmonKCf2YtY2"
      },
      "source": [
        "def detection_collate(batch):\n",
        "    imgs = [s['image'] for s in batch]\n",
        "    annots = [s['bboxes'] for s in batch]\n",
        "    labels = [s['category_id'] for s in batch]\n",
        "\n",
        "    max_num_annots = max(len(annot) for annot in annots)\n",
        "    annot_padded = np.ones((len(annots), max_num_annots, 5))*-1\n",
        "\n",
        "    if max_num_annots > 0:\n",
        "        for idx, (annot, lab) in enumerate(zip(annots, labels)):\n",
        "            if len(annot) > 0:\n",
        "                annot_padded[idx, :len(annot), :4] = annot\n",
        "                annot_padded[idx, :len(annot), 4] = lab\n",
        "    return (torch.stack(imgs, 0), torch.FloatTensor(annot_padded))\n",
        "\n",
        "train_batch_size = 1\n",
        "valid_batch_size = 1\n",
        "\n",
        "eff_train_dataset = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"effdet_train\"), mode = \"train\")\n",
        "eff_train_loader = torch.utils.data.DataLoader(eff_train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= detection_collate)\n",
        "\n",
        "eff_test_dataset = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"effdet_test\"), mode = \"test\")\n",
        "eff_test_loader = torch.utils.data.DataLoader(eff_test_dataset, batch_size = valid_batch_size, shuffle = True, collate_fn = detection_collate)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF1AnUSZienq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70505545-f831-4b73-e51b-4a8e0165d18d"
      },
      "source": [
        "MODEL_MAP = {\n",
        "    'efficientdet-d0': 'efficientnet-b0',\n",
        "    'efficientdet-d1': 'efficientnet-b1',\n",
        "    'efficientdet-d2': 'efficientnet-b2',\n",
        "    'efficientdet-d3': 'efficientnet-b3',\n",
        "    'efficientdet-d4': 'efficientnet-b4',\n",
        "    'efficientdet-d5': 'efficientnet-b5',\n",
        "    'efficientdet-d6': 'efficientnet-b6',\n",
        "    'efficientdet-d7': 'efficientnet-b6',\n",
        "}\n",
        "class EfficientDet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 network='efficientdet-d0',\n",
        "                 D_bifpn=3,\n",
        "                 W_bifpn=88,\n",
        "                 D_class=3,\n",
        "                 is_training=True,\n",
        "                 threshold=0.001, #can change this value 0.01\n",
        "                 iou_threshold=1): # can change this value 0.5\n",
        "        super(EfficientDet, self).__init__()\n",
        "        \n",
        "        self.backbone = EfficientNet.from_pretrained(MODEL_MAP[network])\n",
        "        self.is_training = is_training\n",
        "        self.neck = BIFPN(in_channels=self.backbone.get_list_features()[-5:],\n",
        "                          out_channels=W_bifpn,\n",
        "                          stack=D_bifpn,\n",
        "                          num_outs=5)\n",
        "        self.bbox_head = RetinaHead(num_classes=num_classes,\n",
        "                                    in_channels=W_bifpn)\n",
        "\n",
        "        self.anchors = Anchors()\n",
        "        self.regressBoxes = BBoxTransform()\n",
        "        self.clipBoxes = ClipBoxes()\n",
        "        self.threshold = threshold\n",
        "        self.iou_threshold = iou_threshold\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "        self.freeze_bn()\n",
        "        self.criterion = FocalLoss()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.is_training:\n",
        "            inputs, annotations = inputs\n",
        "        else:\n",
        "            inputs = inputs\n",
        "        x = self.extract_feat(inputs)\n",
        "        outs = self.bbox_head(x)\n",
        "        classification = torch.cat([out for out in outs[0]], dim=1)\n",
        "        regression = torch.cat([out for out in outs[1]], dim=1)\n",
        "        anchors = self.anchors(inputs)\n",
        "        if self.is_training:\n",
        "            return self.criterion(classification, regression, anchors, annotations)\n",
        "        else:\n",
        "            transformed_anchors = self.regressBoxes(anchors, regression)\n",
        "            transformed_anchors = self.clipBoxes(transformed_anchors, inputs)\n",
        "            scores = torch.max(classification, dim=2, keepdim=True)[0]\n",
        "            scores_over_thresh = (scores > self.threshold)[0, :, 0]\n",
        "\n",
        "            if scores_over_thresh.sum() == 0:\n",
        "                # print('No boxes to NMS')\n",
        "                # no boxes to NMS, just return\n",
        "                return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n",
        "            classification = classification[:, scores_over_thresh, :]\n",
        "            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n",
        "            scores = scores[:, scores_over_thresh, :]\n",
        "            anchors_nms_idx = nms(\n",
        "                transformed_anchors[0, :, :], scores[0, :, 0], iou_threshold=self.iou_threshold)\n",
        "            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(\n",
        "                dim=1)\n",
        "            return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]\n",
        "\n",
        "    def freeze_bn(self):\n",
        "        '''Freeze BatchNorm layers.'''\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.BatchNorm2d):\n",
        "                layer.eval()\n",
        "\n",
        "    def extract_feat(self, img):\n",
        "        \"\"\"\n",
        "            Directly extract features from the backbone+neck\n",
        "        \"\"\"\n",
        "        x = self.backbone(img)\n",
        "        x = self.neck(x[-5:])\n",
        "        return x\n",
        "\n",
        "model= EfficientDet(num_classes=len(classes),is_training=True)\n",
        "model.train()\n",
        "\n",
        "model.freeze_bn()\n",
        "\n",
        "model = model.cuda()\n",
        "print('Run with DataParallel ....')\n",
        "\n",
        "## Make sure that you add this line, even though you are not using more than one \n",
        "# GPU DataParallel adds \"module\" to the start of the model structure \n",
        "# allowing for the syntax to be correct when calling \"model.module.freeze_bn()\" for example\n",
        "model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "# I am doing this here an example, you do not have to call the lines below here\n",
        "model.module.is_training = True\n",
        "model.module.freeze_bn()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Run with DataParallel ....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS4fTCSYULYU"
      },
      "source": [
        "def train_effdet(net, epochs, train_loader, test_loader, lr, weight_decay, \n",
        "          print_every = 6, lo_test_dataset = len(test_dataset), lo_train_dataset = len(train_dataset)):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device: {}\".format(device))\n",
        "    print(\"Note: Train Accuracies are only run through one train image per batch\")\n",
        "\n",
        "    if device == torch.device(\"cpu\"):\n",
        "      warnings.warn(\"Code does not support running on CPU but only GPU\")\n",
        "\n",
        "    optimizer = optim.AdamW(net.parameters(), lr=lr, weight_decay = weight_decay)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
        "\n",
        "    start_time = time.time()\n",
        "    net.module.freeze_bn()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        net.train()\n",
        "        net.module.is_training = True\n",
        "        \n",
        "        train_loss = steps = train_mAP = train_mAR = missed_train_images = 0\n",
        "        \n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "\n",
        "            net.train()\n",
        "            net.module.is_training = True\n",
        "\n",
        "            steps += 1\n",
        "            \n",
        "            images = images.cuda().float()\n",
        "            targets = targets.cuda()\n",
        "\n",
        "            classification_loss, regression_loss = model([images, targets])\n",
        "            classification_loss = classification_loss.mean()\n",
        "            regression_loss = regression_loss.mean()\n",
        "            loss = classification_loss + regression_loss\n",
        "            if bool(loss == 0):\n",
        "              print('loss equal zero(0)')\n",
        "              continue\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            net.eval()\n",
        "            net.module.is_training = False\n",
        "            scores, classification, transformed_anchors = net(images[0].unsqueeze(0))\n",
        "            train_mAP, train_mAR, missed_train_images = run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, train_mAP, \n",
        "                                                                                     train_mAR, missed_train_images, device)\n",
        "            net.train()\n",
        "            net.module.is_training = True\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            if (steps % print_every) == 0:\n",
        "\n",
        "              with torch.no_grad():\n",
        "                test_mAP = test_mAR = missed_test_images = test_loss = 0\n",
        "\n",
        "                for images, targets in test_loader:\n",
        "\n",
        "                  if images.size(0) != 1:\n",
        "                    warning.warn(\"Only can validate fully with batch size of 1, \\\n",
        "                    bigger batch sizes risk Errors or Incomplete Validation\")\n",
        "                  \n",
        "                  net.eval()\n",
        "                  net.module.is_training = False\n",
        "\n",
        "                  if device == torch.device(\"cuda\"):\n",
        "                    images = images.cuda().float()\n",
        "                    targets = targets.cuda()\n",
        "\n",
        "                  scores, classification, transformed_anchors = net(images)\n",
        "                  test_mAP, test_mAR, missed_test_images = run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, \n",
        "                                                                                 test_mAP, test_mAR, missed_test_images, device)\n",
        "\n",
        "                  net.train()\n",
        "                  net.module.is_training = True\n",
        "\n",
        "                  classification_loss, regression_loss = model([images, targets])\n",
        "                  classification_loss = classification_loss.mean()\n",
        "                  regression_loss = regression_loss.mean()\n",
        "                  loss = classification_loss + regression_loss\n",
        "\n",
        "                  test_loss += loss.item()\n",
        "\n",
        "                for param_group in optimizer.param_groups:\n",
        "                  learning_rate_extract = param_group[\"lr\"]\n",
        "                print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Test_loss: {:0.2f} | Test mAP: {:0.2f}% | Test mAR: {:0.2f}% | Missed Test Images: {}\".format(\n",
        "                    epoch + 1, epochs, steps, learning_rate_extract, train_loss, test_loss,  \n",
        "                    (test_mAP / float(lo_test_dataset)) * 100., (test_mAR / float(lo_test_dataset)) * 100.,missed_test_images))\n",
        "\n",
        "              assert (steps % print_every) == 0\n",
        "              train_loss = 0\n",
        "              # scheduler.step(test_loss / float(lo_test_dataset))\n",
        "             \n",
        "        print(\"\\n Epoch {} Final Train mAP: {:0.2f}% | Epoch {} Final Train mAR: {:0.2f}% | Epoch {} Final Missed Train Images: {} out of {} images \\n\".format(\n",
        "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, (train_mAR / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, missed_train_images, lo_train_dataset\n",
        "        ))\n",
        "    \n",
        "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlfqIday1RLr",
        "outputId": "23f4b1a7-3950-402c-debd-bf7de01e5178"
      },
      "source": [
        "train_effdet(model, 1, eff_train_loader, eff_test_loader, 0.001, 1e-4, print_every = 20)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Note: Train Accuracies are only run through one train image per batch\n",
            "Epoch 1/1 | Batch Number: 20 | LR: 0.00099 | Train_loss: 3623.97 | Test_loss: 367.30 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 70\n",
            "Epoch 1/1 | Batch Number: 40 | LR: 0.00098 | Train_loss: 69.74 | Test_loss: 359.09 | Test mAP: 0.23% | Test mAR: 0.47% | Missed Test Images: 6\n",
            "Epoch 1/1 | Batch Number: 60 | LR: 0.00095 | Train_loss: 69.72 | Test_loss: 359.25 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 22\n",
            "Epoch 1/1 | Batch Number: 80 | LR: 0.00091 | Train_loss: 70.75 | Test_loss: 367.56 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 100 | LR: 0.00087 | Train_loss: 67.79 | Test_loss: 375.18 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 120 | LR: 0.00082 | Train_loss: 67.74 | Test_loss: 377.71 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 140 | LR: 0.00075 | Train_loss: 69.27 | Test_loss: 362.36 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 160 | LR: 0.00069 | Train_loss: 69.03 | Test_loss: 361.96 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 180 | LR: 0.00062 | Train_loss: 67.36 | Test_loss: 357.59 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 200 | LR: 0.00054 | Train_loss: 67.50 | Test_loss: 356.67 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 220 | LR: 0.00047 | Train_loss: 66.42 | Test_loss: 363.37 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 240 | LR: 0.00040 | Train_loss: 65.39 | Test_loss: 353.97 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 260 | LR: 0.00033 | Train_loss: 66.01 | Test_loss: 354.42 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 280 | LR: 0.00026 | Train_loss: 66.63 | Test_loss: 354.32 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 300 | LR: 0.00020 | Train_loss: 65.47 | Test_loss: 354.11 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 320 | LR: 0.00014 | Train_loss: 65.44 | Test_loss: 351.63 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 340 | LR: 0.00009 | Train_loss: 65.43 | Test_loss: 353.63 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 360 | LR: 0.00006 | Train_loss: 65.22 | Test_loss: 351.27 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 380 | LR: 0.00003 | Train_loss: 66.03 | Test_loss: 353.22 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 400 | LR: 0.00001 | Train_loss: 64.94 | Test_loss: 352.83 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "Epoch 1/1 | Batch Number: 420 | LR: 0.00000 | Train_loss: 64.29 | Test_loss: 352.90 | Test mAP: 0.00% | Test mAR: 0.00% | Missed Test Images: 107\n",
            "\n",
            " Epoch 1 Final Train mAP: 0.68% | Epoch 1 Final Train mAR: 2.43% | Epoch 1 Final Missed Train Images: 368 out of 424 images \n",
            "\n",
            "Time for Total Training 239.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPmuew0CnX92"
      },
      "source": [
        "# The Mobile Net Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbDtIsBs-OWD"
      },
      "source": [
        "backbone = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
        "backbone.roi_heads.box_predictor.cls_score.out_features = 6\n",
        "backbone.roi_heads.box_predictor.bbox_pred.out_features = 24\n",
        "# backbone.roi_heads.box_predictor.cls_score.out_features = 3\n",
        "# backbone.roi_heads.box_predictor.bbox_pred.out_features = 12\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjzRmGEPmjjF",
        "outputId": "970955e2-a3e5-4cc3-e5fc-c68b23acbc4b"
      },
      "source": [
        "another_one = train(backbone, 5, train_loader, test_loader, 0.001, weight_decay = 1e-4, print_every = 80)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Device: cuda\n",
            "Optimizer: SAM (\n",
            "Parameter Group 0\n",
            "    N_sma_threshhold: 5\n",
            "    alpha: 0.5\n",
            "    betas: (0.95, 0.999)\n",
            "    eps: 1e-05\n",
            "    initial_lr: 0.001\n",
            "    k: 6\n",
            "    lr: 0.001\n",
            "    rho: 0.05\n",
            "    step_counter: 0\n",
            "    weight_decay: 0.0001\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5 | Batch Number: 80 | LR: 0.00100 | Train_loss: 164.11 | Test_loss: 81.12 | Test mAP: 29.64% | Test mAR: 98.09% | Missed Test Images: 0\n",
            "Epoch 1/5 | Batch Number: 160 | LR: 0.00099 | Train_loss: 75.02 | Test_loss: 63.72 | Test mAP: 42.30% | Test mAR: 95.57% | Missed Test Images: 0\n",
            "Epoch 1/5 | Batch Number: 240 | LR: 0.00097 | Train_loss: 75.74 | Test_loss: 76.73 | Test mAP: 34.87% | Test mAR: 96.84% | Missed Test Images: 0\n",
            "Epoch 1/5 | Batch Number: 320 | LR: 0.00094 | Train_loss: 64.59 | Test_loss: 50.38 | Test mAP: 32.52% | Test mAR: 97.76% | Missed Test Images: 0\n",
            "Epoch 1/5 | Batch Number: 400 | LR: 0.00091 | Train_loss: 66.40 | Test_loss: 63.86 | Test mAP: 37.40% | Test mAR: 96.58% | Missed Test Images: 0\n",
            "\n",
            " Epoch 1 Final Train mAP: 32.52% | Epoch 1 Final Train mAR: 97.26% | Epoch 1 Final Missed Train Images: 0 out of 424 images \n",
            "\n",
            "Epoch 2/5 | Batch Number: 80 | LR: 0.00087 | Train_loss: 69.89 | Test_loss: 55.32 | Test mAP: 25.39% | Test mAR: 98.22% | Missed Test Images: 0\n",
            "Epoch 2/5 | Batch Number: 160 | LR: 0.00082 | Train_loss: 64.57 | Test_loss: 67.74 | Test mAP: 33.36% | Test mAR: 96.12% | Missed Test Images: 0\n",
            "Epoch 2/5 | Batch Number: 240 | LR: 0.00078 | Train_loss: 72.54 | Test_loss: 84.03 | Test mAP: 36.99% | Test mAR: 95.57% | Missed Test Images: 0\n",
            "Epoch 2/5 | Batch Number: 320 | LR: 0.00073 | Train_loss: 75.26 | Test_loss: 62.76 | Test mAP: 30.66% | Test mAR: 96.77% | Missed Test Images: 0\n",
            "Epoch 2/5 | Batch Number: 400 | LR: 0.00067 | Train_loss: 59.26 | Test_loss: 71.96 | Test mAP: 34.35% | Test mAR: 97.46% | Missed Test Images: 0\n",
            "\n",
            " Epoch 2 Final Train mAP: 29.08% | Epoch 2 Final Train mAR: 97.30% | Epoch 2 Final Missed Train Images: 0 out of 424 images \n",
            "\n",
            "Epoch 3/5 | Batch Number: 80 | LR: 0.00060 | Train_loss: 65.16 | Test_loss: 76.66 | Test mAP: 45.90% | Test mAR: 95.50% | Missed Test Images: 0\n",
            "Epoch 3/5 | Batch Number: 160 | LR: 0.00054 | Train_loss: 74.93 | Test_loss: 58.92 | Test mAP: 33.05% | Test mAR: 98.00% | Missed Test Images: 0\n",
            "Epoch 3/5 | Batch Number: 240 | LR: 0.00048 | Train_loss: 59.46 | Test_loss: 66.35 | Test mAP: 31.22% | Test mAR: 97.15% | Missed Test Images: 0\n",
            "Epoch 3/5 | Batch Number: 320 | LR: 0.00042 | Train_loss: 63.84 | Test_loss: 61.45 | Test mAP: 22.44% | Test mAR: 98.57% | Missed Test Images: 0\n",
            "Epoch 3/5 | Batch Number: 400 | LR: 0.00036 | Train_loss: 62.14 | Test_loss: 60.31 | Test mAP: 31.60% | Test mAR: 97.70% | Missed Test Images: 0\n",
            "\n",
            " Epoch 3 Final Train mAP: 31.36% | Epoch 3 Final Train mAR: 97.53% | Epoch 3 Final Missed Train Images: 0 out of 424 images \n",
            "\n",
            "Epoch 4/5 | Batch Number: 80 | LR: 0.00029 | Train_loss: 72.81 | Test_loss: 51.81 | Test mAP: 30.31% | Test mAR: 98.40% | Missed Test Images: 0\n",
            "Epoch 4/5 | Batch Number: 160 | LR: 0.00024 | Train_loss: 58.51 | Test_loss: 65.10 | Test mAP: 37.46% | Test mAR: 98.11% | Missed Test Images: 0\n",
            "Epoch 4/5 | Batch Number: 240 | LR: 0.00019 | Train_loss: 56.30 | Test_loss: 52.56 | Test mAP: 32.45% | Test mAR: 98.39% | Missed Test Images: 0\n",
            "Epoch 4/5 | Batch Number: 320 | LR: 0.00015 | Train_loss: 60.40 | Test_loss: 64.15 | Test mAP: 39.17% | Test mAR: 97.17% | Missed Test Images: 0\n",
            "Epoch 4/5 | Batch Number: 400 | LR: 0.00011 | Train_loss: 64.98 | Test_loss: 57.06 | Test mAP: 35.93% | Test mAR: 98.15% | Missed Test Images: 0\n",
            "\n",
            " Epoch 4 Final Train mAP: 37.21% | Epoch 4 Final Train mAR: 98.25% | Epoch 4 Final Missed Train Images: 0 out of 424 images \n",
            "\n",
            "Epoch 5/5 | Batch Number: 80 | LR: 0.00006 | Train_loss: 57.81 | Test_loss: 57.24 | Test mAP: 36.80% | Test mAR: 98.05% | Missed Test Images: 0\n",
            "Epoch 5/5 | Batch Number: 160 | LR: 0.00004 | Train_loss: 53.47 | Test_loss: 54.34 | Test mAP: 41.44% | Test mAR: 97.65% | Missed Test Images: 0\n",
            "Epoch 5/5 | Batch Number: 240 | LR: 0.00002 | Train_loss: 55.94 | Test_loss: 55.52 | Test mAP: 40.12% | Test mAR: 98.55% | Missed Test Images: 0\n",
            "Epoch 5/5 | Batch Number: 320 | LR: 0.00001 | Train_loss: 58.27 | Test_loss: 56.09 | Test mAP: 41.23% | Test mAR: 98.18% | Missed Test Images: 0\n",
            "Epoch 5/5 | Batch Number: 400 | LR: 0.00000 | Train_loss: 57.77 | Test_loss: 56.34 | Test mAP: 41.36% | Test mAR: 98.18% | Missed Test Images: 0\n",
            "\n",
            " Epoch 5 Final Train mAP: 41.15% | Epoch 5 Final Train mAR: 97.68% | Epoch 5 Final Missed Train Images: 0 out of 424 images \n",
            "\n",
            "Time for Total Training 379.92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-cknP0_uy0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb0228d-85de-4153-c929-b0125f70233e"
      },
      "source": [
        "# https://github.com/amdegroot/ssd.pytorch/blob/master/layers/box_utils.py#L48\n",
        "def intersect(box_a, box_b):\n",
        "\n",
        "    A = box_a.size(0)\n",
        "    B = box_b.size(0)\n",
        "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
        "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
        "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
        "    return inter[:, :, 0] * inter[:, :, 1]\n",
        "\n",
        "def jaccard_iou(box_a, box_b):\n",
        "\n",
        "    inter = intersect(box_a, box_b)\n",
        "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
        "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
        "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
        "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
        "    union = area_a + area_b - inter\n",
        "    return inter / union  # [A,B]\n",
        "\n",
        "def calculate_iou_on_label(results, len_of_results, iou_thresh, device):\n",
        "  for current_index, _ in enumerate(results[\"boxes\"]):\n",
        "    if current_index >= len_of_results:\n",
        "      break\n",
        "\n",
        "    current_index_iou = jaccard_iou(results[\"boxes\"][current_index].view(1, -1).to(device),\n",
        "                                    results[\"boxes\"].to(device))\n",
        "    \n",
        "    mask = (current_index_iou > iou_thresh) & (current_index_iou != 1)\n",
        "    mask = mask.squeeze()\n",
        "    for key in results:\n",
        "      results[key] = results[key][~mask]\n",
        "\n",
        "    len_of_results -= sum(mask)\n",
        "  \n",
        "  return results\n",
        "\n",
        "def get_labels_categ(classes, want):\n",
        "  fruit_index_list, bad_spot_index_list = list(), list()\n",
        "  for ii, name in enumerate(classes):\n",
        "    if re.search(\"Spot\", name):\n",
        "      bad_spot_index_list.append(ii)\n",
        "    elif re.search(\"Placeholder\", name):\n",
        "      continue\n",
        "    else:\n",
        "      fruit_index_list.append(ii)\n",
        "  \n",
        "  if want == \"fruit\":\n",
        "    return fruit_index_list\n",
        "  elif want == \"bad_spot\":\n",
        "    return bad_spot_index_list\n",
        "  else:\n",
        "    raise ValueError(\"want Type not applicable [fruit or bad_spot only]\")\n",
        "\n",
        "print(classes)\n",
        "get_labels_categ(classes, \"bad_spot\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Placeholder', 'Apples', 'Strawberry', 'Tomato', 'Apple_Bad_Spot', 'Strawberry_Bad_Spot', 'Tomato_Bad_Spot']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 5, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFhjlggXlAx7"
      },
      "source": [
        "def infer_image(image_file_path, trained_model, distance_thresh, iou_thresh, webcam = False, show_image = True):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  #Just load it up as PIL. Avoid using cv2 because do not need albumentations\n",
        "  if not webcam:\n",
        "    torch_image = F.to_tensor(Image.open(image_file_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    trained_model.to(device)\n",
        "    trained_model.eval()\n",
        "    print(\"Image Size: {}\".format(torch_image.size()))\n",
        "\n",
        "    start_time = time.time()\n",
        "    results = trained_model(torch_image)\n",
        "    end_time = time.time() - start_time\n",
        "\n",
        "    print(\"Time of Inference {:0.2f}\".format(end_time))\n",
        "  else:\n",
        "\n",
        "    torch_image = F.to_tensor(image_file_path).unsqueeze(1).to(device)\n",
        "\n",
        "    results = trained_model(torch_image)\n",
        "\n",
        "  valid_box_count = 0\n",
        "  for ii, score in enumerate(results[0][\"scores\"]):\n",
        "    if score < distance_thresh:\n",
        "      low_index_start = ii\n",
        "      break\n",
        "    else:\n",
        "      valid_box_count += 1\n",
        "\n",
        "  if valid_box_count == len(results[0][\"scores\"]):\n",
        "    low_index_start = len(results[0][\"scores\"])\n",
        "  \n",
        "  for key in results[0]:\n",
        "    results[0][key] = results[0][key][:low_index_start]\n",
        "  \n",
        "  #This is where I place the order of the list\n",
        "  fruit_spot_iou_thresh, bad_spot_iou_thresh = iou_thresh\n",
        "\n",
        "  #Update when I get more data of fruits and when running for script beware of classes.\n",
        "  bad_spot_index = [ii for ii, label in enumerate(results[0][\"labels\"]) if label in get_labels_categ(classes, \"bad_spot\")]\n",
        "  fruit_index = [ii for ii, _ in enumerate(results[0][\"labels\"]) if ii not in bad_spot_index]\n",
        "\n",
        "  bad_spot_results, fruit_results = dict(), dict()\n",
        "\n",
        "  for key in results[0]:\n",
        "    bad_spot_results[key], fruit_results[key] = results[0][key][[bad_spot_index]], results[0][key][[fruit_index]]\n",
        "\n",
        "  assert len(bad_spot_results[\"boxes\"]) == len(bad_spot_results[\"scores\"]) == len(bad_spot_results[\"labels\"])\n",
        "  assert len(fruit_results[\"boxes\"]) == len(fruit_results[\"scores\"]) == len(fruit_results[\"labels\"])\n",
        "\n",
        "  len_of_bad_spots, len_of_fruit = len(bad_spot_results[\"boxes\"]), len(fruit_results[\"boxes\"])\n",
        "\n",
        "  if len_of_bad_spots > 1:\n",
        "    bad_spot_results = calculate_iou_on_label(bad_spot_results, len_of_bad_spots, bad_spot_iou_thresh, device)\n",
        "  if len_of_fruit > 1:\n",
        "    fruit_results = calculate_iou_on_label(fruit_results, len_of_fruit, fruit_spot_iou_thresh, device)\n",
        "  \n",
        "  for key in results[0]: \n",
        "    if (key == \"boxes\"):\n",
        "      results[0][\"boxes\"] = torch.cat((fruit_results[\"boxes\"], bad_spot_results[\"boxes\"]), axis = 0)\n",
        "    else:\n",
        "      results[0][key] = torch.cat((fruit_results[key], bad_spot_results[key]), dim = 0)\n",
        "\n",
        "  if show_image:\n",
        "    if device == torch.device(\"cuda\"):\n",
        "      torch_image = torch_image.cpu() \n",
        "    written_image = cv2.cvtColor(draw_boxes(results[0][\"boxes\"], results[0][\"labels\"], torch_image.squeeze(), infer = True, put_text= True), cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(written_image)\n",
        "  \n",
        "  return results"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZd4nVDuiu4M"
      },
      "source": [
        "import base64\n",
        "import html\n",
        "import io\n",
        "import time\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "def start_input():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      /* try changing the capture canvas and see what happens*/\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def take_photo(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T45EScTjJ5e"
      },
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def js_reply_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          js_reply: JavaScript object, contain image from webcam\n",
        "\n",
        "    output: \n",
        "          image_array: image array RGB size 512 x 512 from webcam\n",
        "    \"\"\"\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\n",
        "    image_array = np.array(image_PIL)\n",
        "\n",
        "    return image_array\n",
        "\n",
        "def drawing_array_to_bytes(drawing_array):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          drawing_array: image RGBA size 512 x 512 \n",
        "                              contain bounding box and text from yolo prediction, \n",
        "                              channel A value = 255 if the pixel contains drawing properties (lines, text) \n",
        "                              else channel A value = 0\n",
        "\n",
        "    output: \n",
        "          drawing_bytes: string, encoded from drawing_array\n",
        "    \"\"\"\n",
        "\n",
        "    drawing_PIL = Image.fromarray(drawing_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    drawing_PIL.save(iobuf, format='png')\n",
        "    drawing_bytes = 'data:image/png;base64,{}'.format((str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "    return drawing_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FSrjfHjgX4q"
      },
      "source": [
        "data_transforms = get_transforms(mode = \"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "j0WidBmGlktM",
        "outputId": "966e0c06-eb6b-4bb4-f10a-9b5c7087947b"
      },
      "source": [
        "start_input()\n",
        "label_html = 'Capturing...'\n",
        "img_data = ''\n",
        "count = 0 \n",
        "\n",
        "color=None\n",
        "label=None\n",
        "line_thickness=None\n",
        "another_one.to(device).eval();\n",
        "while True:\n",
        "    js_reply = take_photo(label_html, img_data)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    image = js_reply_to_image(js_reply)\n",
        "    prediciton = infer_image(image, another_one, 0.2, [0.3, 0.1], webcam= True, show_image = False)\n",
        "\n",
        "    drawing_array = np.zeros([512,512,4], dtype=np.uint8)\n",
        "\n",
        "    for x in prediciton[0]['boxes']:\n",
        "\n",
        "      tl = line_thickness or round(0.002 * (drawing_array.shape[0] + drawing_array.shape[1]) / 2) + 1  # line/font thickness\n",
        "      color = color or [random.randint(0, 255) for _ in range(3)]\n",
        "      c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
        "      cv2.rectangle(drawing_array, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
        "      if label:\n",
        "        tf = max(tl - 1, 1)  # font thickness\n",
        "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
        "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
        "        cv2.rectangle(drawing_array, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
        "        cv2.putText(drawing_array, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "    drawing_array[:,:,3] = (drawing_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "\n",
        "    drawing_PIL = Image.fromarray(drawing_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    drawing_PIL.save(iobuf, format='png')\n",
        "    drawing_bytes = 'data:image/png;base64,{}'.format((str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "    img_data = drawing_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      /* try changing the capture canvas and see what happens*/\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 512; //video.videoWidth;\n",
              "      captureCanvas.height = 512; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function takePhoto(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}