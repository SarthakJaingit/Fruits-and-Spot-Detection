{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FruitObjectDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZfnWC_NF7NM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch \n",
        "from torch import nn, optim \n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import itertools\n",
        "import glob \n",
        "from PIL import Image\n",
        "import csv \n",
        "import cv2\n",
        "import re\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.ops.boxes import box_iou\n",
        "import random\n",
        "import torchvision\n",
        "import warnings\n",
        "\n",
        "!pip install --upgrade albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2, ToTensor\n",
        "!git clone https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
        "%cd Ranger-Deep-Learning-Optimizer\n",
        "!pip install -e .\n",
        "from ranger import Ranger  \n",
        "%cd ..\n",
        "#https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1\n",
        "!git clone https://github.com/davda54/sam.git\n",
        "%cd sam\n",
        "import sam\n",
        "print(\"Imported SAM Successfully from github .py file\")\n",
        "%cd ..\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_pckeYPGF0b",
        "outputId": "ccb6e8c5-23f2-4ad5-8284-70dc1d9a71ea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount = True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCS2doQYbNpf"
      },
      "source": [
        "## To Do Right Now: \n",
        "\n",
        "# Coding/ Technical Stuff to add to paper\n",
        "### Figure out why the effecient det infers bad values for bounding boxes but still gets good mAP on train/valid func. Print out the preds in valid loop and caluclate iou on them and see what happens. Compare to the infer phenomena.\n",
        "\n",
        "### Once finished with effecient det can create ensemble model that only output boxes that are both predicted or nearly predicted by both models. If mobile net outputs 0 bboxes than run eff det and see if any boudning boxes have a good score.\n",
        "https://github.com/rwightman/efficientdet-pytorch\n",
        "https://github.com/rwightman/efficientdet-pytorch/blob/abba1d5a3611471ac88d49a473f993f72f9e1aba/effdet/config/model_config.py\n",
        "\n",
        "### Find links to put saved model and weights into rasberry pi model\n",
        "\n",
        "# Paper\n",
        "\n",
        "### Work on creating a slide presentation to keep the paper contents.\n",
        "\n",
        "### Move to .py script\n",
        "\n",
        "### Labeling the other data. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4UKDVNnIqQN"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/LatestFruit Defects Dataset .zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/noisy_dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY83cUEFAjoW",
        "outputId": "c2fd8cf5-5eed-48e6-fe38-3c94c1b9fdee"
      },
      "source": [
        "#For one strawberry batch please drop watermark rows\n",
        "strawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 3 Labeled/FreshStrawberryBatch3Labels.csv\", header = None)\n",
        "strawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 2 Labeled/FreshStrawberriesBatch2Labels.csv\", header = None)\n",
        "strawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 1 Labeled/Strawberrybatch1.csv\", header = None)\n",
        "rottenApple_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch1Labeled/RottenAppleBatch1Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch2Labeled/RottenApplesBatch2Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch3Labaled/RottenApplesBatch3Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch1RottenStrawBerryLabels/RottenStrawberriesBatch1Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch2RottenStrawBerryLabels/RottenStrawBerryBatch2.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch3RottenStrawberrylabel/rottenStrawberryBtch3labels.csv\", header = None)\n",
        "freshApples_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplebtch2label/FreshApplesBatch2LabelsFresh.csv\", header = None)\n",
        "freshApples_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplesBatch1Labels/FreshAppleBatch1Labels.csv\", header = None)\n",
        "rottenTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/Rotten TomatoBatch1/Batch1TomoatosLabelsBbox.csv\", header = None)\n",
        "rottenTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottnTomatoBatch2/RottenTomatyoBatch2Labelss.csv\", header = None)\n",
        "rottenTomato_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatBtch3/RottenTomatoesBatch3Labssles.csv\", header = None)\n",
        "rottenTomato_csv_batch_4 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatoesBatch4/Tomatobatch4labelssRotten.csv\", header = None)\n",
        "freshTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatoesBatch1Labelss/FreshTomatoesLabelsBatch1Labels.csv\", header = None)\n",
        "freshTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatBatch2Labessls/Batch2TomatlabelsFresh.csv\", header = None)\n",
        "\n",
        "strawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_3.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_4.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "\n",
        "#Drop some watermark data for Fresh StrawBerry Batch 1 Labeled images [59, 9, 93]\n",
        "\n",
        "# strawberry_csv_batch_1 = strawberry_csv_batch_1[Image_id not in [\"FreshStrawberries59.jpeg, FreshStrawberries9.jpeg, FreshStrawberries93.jpeg\"]]\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries59.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries9.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries93.jpeg\"].index, inplace = True)\n",
        "freshTomato_csv_batch_1.drop(freshTomato_csv_batch_1[freshTomato_csv_batch_1[\"Image_id\"] == \"Fresh Tomatoes66AddonPart1.jpeg\"].index, inplace = True)\n",
        "\n",
        "strawberry_csv_batch_1 = strawberry_csv_batch_1.reset_index(drop=True)\n",
        "freshTomato_csv_batch_1 = freshTomato_csv_batch_1.reset_index(drop = True)\n",
        "\n",
        "#Stack all the csv files together. \n",
        "list_of_all_dataframes = [strawberry_csv_batch_1, strawberry_csv_batch_2, strawberry_csv_batch_3, rottenApple_csv_batch_1, \n",
        "                          rottenApple_csv_batch_2, rottenApple_csv_batch_3, rottenStrawberry_csv_batch_1, rottenStrawberry_csv_batch_2, \n",
        "                          rottenStrawberry_csv_batch_3, freshApples_csv_batch_2, freshApples_csv_batch_1, rottenTomato_csv_batch_1, \n",
        "                          rottenTomato_csv_batch_2, rottenTomato_csv_batch_3, rottenTomato_csv_batch_4, freshTomato_csv_batch_1, \n",
        "                          freshTomato_csv_batch_2]\n",
        "fruit_df = pd.concat(list_of_all_dataframes, ignore_index = True)\n",
        "\n",
        "total_row_sum_check = 0 \n",
        "for dataframe in list_of_all_dataframes:\n",
        "  total_row_sum_check += dataframe.shape[0]\n",
        "print(\"Checked total rows from all the dataframes combined: {}\".format(total_row_sum_check))\n",
        "\n",
        "def run_dataframe_check():\n",
        "  assert total_row_sum_check == fruit_df.shape[0]\n",
        "  print(\"DataFrame shape: {}\".format(fruit_df.shape))\n",
        "  print(\"Unique Fruit Labels {}\".format(fruit_df[\"Fruit\"].unique()))\n",
        "  print(\"Number of Unique Images {}\".format(len(fruit_df[\"Image_id\"].unique())))\n",
        "\n",
        "run_dataframe_check()\n",
        "\n",
        "#Specify more image types when \n",
        "def more_specific_Image_id(image_id, fruit):\n",
        "  if fruit == \"Bad_Spots\":\n",
        "    if re.search(\"RottenStrawberries\", image_id):\n",
        "      return \"Strawberry_Bad_Spot\"\n",
        "    elif re.search(\"RottenApples\", image_id):\n",
        "      return \"Apple_Bad_Spot\"\n",
        "    elif re.search(\"Rotten Tomatoes\", image_id):\n",
        "      return \"Tomato_Bad_Spot\"\n",
        "    else:\n",
        "      raise ValueError(\"Could not find a match for some of the Image_ids\")\n",
        "\n",
        "  else:\n",
        "    return fruit\n",
        "\n",
        "fruit_df[\"Fruit\"] = fruit_df.apply(lambda row: more_specific_Image_id(row.Image_id, row.Fruit), axis = 1)\n",
        "\n",
        "run_dataframe_check()\n",
        "\n",
        "#Post Processing \n",
        "fruit_df = fruit_df[fruit_df[\"Image_id\"] != \"FreshStrawberries15.jpeg\"]\n",
        "\n",
        "bounding_box_dict = dict()\n",
        "labels_dict = dict()\n",
        "classes = [\"Placeholder\", \"Apples\", \"Strawberry\", \"Tomato\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\", \"Tomato_Bad_Spot\"]\n",
        "# classes = [\"Apples\", \"Strawberry\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\"]\n",
        "print(classes)\n",
        "# classes = [\"Bad_Spots\", \"Strawberry\", \"Apples\"]\n",
        "\n",
        "for row_index in range(len(fruit_df)): \n",
        "  current_image_file = fruit_df.iloc[row_index][\"Image_id\"]\n",
        "  if current_image_file not in bounding_box_dict:\n",
        "    bounding_box_dict[current_image_file] = list()\n",
        "    labels_dict[current_image_file] = list()\n",
        "  bounding_box_dict[current_image_file].append(fruit_df.iloc[row_index, 1:5].to_list())\n",
        "  labels_dict[current_image_file].append(classes.index(fruit_df.iloc[row_index, 0]))\n",
        "\n",
        "print(len(bounding_box_dict))\n",
        "print(len(labels_dict))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checked total rows from all the dataframes combined: 1882\n",
            "DataFrame shape: (1882, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Bad_Spots' 'Tomato']\n",
            "Number of Unique Images 532\n",
            "DataFrame shape: (1882, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Apple_Bad_Spot' 'Strawberry_Bad_Spot' 'Tomato'\n",
            " 'Tomato_Bad_Spot']\n",
            "Number of Unique Images 532\n",
            "['Placeholder', 'Apples', 'Strawberry', 'Tomato', 'Apple_Bad_Spot', 'Strawberry_Bad_Spot', 'Tomato_Bad_Spot']\n",
            "531\n",
            "531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15qhrCwGUPxp"
      },
      "source": [
        "## Class function + util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgVTPiupUTQa"
      },
      "source": [
        "def ffile_path(image_id, full_image_file_paths):\n",
        "  for image_path in full_image_file_paths:\n",
        "    if image_id in image_path:\n",
        "      return image_path\n",
        "\n",
        "def find_area_bb(bb_coord):\n",
        "  bb_coord = bb_coord.numpy()\n",
        "  area_of_each_bb = list()\n",
        "  for pair_of_coord in bb_coord:\n",
        "    area_of_each_bb.append(\n",
        "        (pair_of_coord[2] - pair_of_coord[0]) * (pair_of_coord[3] - pair_of_coord[1])\n",
        "    )\n",
        "  return torch.tensor(area_of_each_bb, dtype=torch.int32)\n",
        "\n",
        "def convert_min_max(bb_coord):\n",
        "  for pair_of_coord in bb_coord:\n",
        "    pair_of_coord[2], pair_of_coord[3] = (pair_of_coord[0] + pair_of_coord[-2]), (pair_of_coord[1] + pair_of_coord[-1])\n",
        "  return bb_coord\n",
        "\n",
        "class FruitDetectDataset(object):\n",
        "  def __init__(self, id_labels, id_bounding_boxes, transforms, mode, noisy_dataset_path = None):\n",
        "\n",
        "    assert len(id_labels) == len(id_bounding_boxes)\n",
        "    assert sorted(id_labels.keys()) == sorted(id_bounding_boxes.keys())\n",
        "    self.imgs_key = sorted(id_labels.keys())\n",
        "\n",
        "    if noisy_dataset_path:\n",
        "      self.noisy_fp = [fp for fp in glob.glob(os.path.join(noisy_dataset_path, \"*.JPEG\"))]\n",
        "      \n",
        "      print(\"Noisy Has been subsetted\")\n",
        "      #Go to this code if you want to subset.\n",
        "      self.noisy_fp = self.noisy_fp[:40]\n",
        "      \n",
        "    else:\n",
        "      print(\"Dataset getting configured without noise loader\")\n",
        "      self.noisy_fp = list()\n",
        "\n",
        "    # np.random.shuffle(self.imgs_key)\n",
        "    if (mode == \"train\"):\n",
        "      self.imgs_key = self.imgs_key[:int(len(self.imgs_key) * 0.8)]\n",
        "      if noisy_dataset_path:\n",
        "        print(\"Extended {} noisy images to train set\".format(int(len(self.noisy_fp) * 0.8)))\n",
        "        self.imgs_key.extend(self.noisy_fp[:int(len(self.noisy_fp) * 0.8)])\n",
        "    elif (mode == \"test\"):\n",
        "      self.imgs_key = self.imgs_key[int(len(self.imgs_key) * 0.8):]\n",
        "      if noisy_dataset_path:\n",
        "        print(\"Extended {} noisy images to test set\".format(int(len(self.noisy_fp) * 0.2)))\n",
        "        self.imgs_key.extend(self.noisy_fp[int(len(self.noisy_fp) * 0.8):])\n",
        "    else:\n",
        "      raise ValueError(\"Invalid Mode choose from train or test\")\n",
        "\n",
        "    self.id_labels = id_labels\n",
        "    self.id_bounding_boxes = id_bounding_boxes\n",
        "    self.full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_key = self.imgs_key[idx]\n",
        "    if img_key in self.noisy_fp:\n",
        "      img_path = img_key\n",
        "      boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "      labels = torch.as_tensor([], dtype = torch.int64)\n",
        "    else:\n",
        "      img_path = ffile_path(self.imgs_key[idx], self.full_image_file_paths) \n",
        "      boxes = convert_min_max(torch.as_tensor(self.id_bounding_boxes[self.imgs_key[idx]], dtype=torch.float32))\n",
        "      labels = torch.as_tensor(self.id_labels[self.imgs_key[idx]], dtype=torch.int64)\n",
        "    \n",
        "    img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = find_area_bb(boxes)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    \n",
        "    #Query about transforms for labels of images\n",
        "    if self.transforms: \n",
        "      sample = {\n",
        "                'image': img,\n",
        "                'bboxes': target['boxes'],\n",
        "                'labels': labels\n",
        "            }\n",
        "\n",
        "      sample = self.transforms(**sample)\n",
        "      img = sample['image']\n",
        "\n",
        "      if img_key not in self.noisy_fp:\n",
        "        target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "    \n",
        "    \n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs_key)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDh2_pg4J2yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6123368c-ee7d-4a71-8a8c-713f759c80a4"
      },
      "source": [
        "\n",
        "#The Drawing function.\n",
        "# COLORS = [(255, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0)]\n",
        "COLORS = [(0, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0), (255, 0, 0)]\n",
        "\n",
        "def draw_boxes(boxes, labels, image, infer = False, put_text = True):\n",
        "    # read the image with OpenCV\n",
        "    image = image.permute(1, 2, 0).numpy()\n",
        "    if infer:\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    for i, box in enumerate(boxes):\n",
        "        color = COLORS[labels[i] % len(COLORS)]\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (int(box[0]), int(box[1])),\n",
        "            (int(box[2]), int(box[3])),\n",
        "            color, 2\n",
        "        )\n",
        "        if put_text:\n",
        "          cv2.putText(image, classes[labels[i]], (int(box[0]), int(box[1]-5)),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n",
        "                      lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "# Albumentations\n",
        "def get_transforms(mode):\n",
        "  if (mode == \"train\"):\n",
        "    return A.Compose([\n",
        "                      A.OneOf([\n",
        "                      A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                     val_shift_limit=0.2, p=0.9),\n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "                      A.HorizontalFlip(),\n",
        "                      A.VerticalFlip(), \n",
        "                      # A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), p=1),\n",
        "                      # ToTensor(),\n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"test\"):\n",
        "    return A.Compose([\n",
        "                      # A.Resize(512, 512), \n",
        "                      # A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                      # std=(0.229, 0.224, 0.225), p=1),\n",
        "                      # ToTensor()\n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"effdet_train\"):\n",
        "    return A.Compose([\n",
        "                      A.OneOf([\n",
        "                      A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                     val_shift_limit=0.2, p=0.9),\n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "                      A.HorizontalFlip(),\n",
        "                      A.VerticalFlip(), \n",
        "                      A.Resize(height = 512, width=512), \n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"effdet_test\"):\n",
        "    return A.Compose([\n",
        "                      A.Resize(height = 512, width = 512), \n",
        "                      ToTensorV2()])\n",
        "  else:\n",
        "    raise ValueError(\"mode is wrong value can either be train or test\")\n",
        "\n",
        "class NoiseDataset(object):\n",
        "\n",
        "  def __init__(self, noise_file_path, size, camera_size):\n",
        "\n",
        "    self.size = size\n",
        "    self.noise_file_path = [fp for fp in glob.glob(os.path.join(noise_file_path, \"*.JPEG\"))]\n",
        "    self.transforms = transforms.Compose([\n",
        "                                          transforms.Resize((camera_size, camera_size)), \n",
        "                                          transforms.ToTensor()])\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    current_file_path = self.noise_file_path[idx]\n",
        "    img = Image.open(current_file_path).convert(\"RGB\")\n",
        "\n",
        "    img = self.transforms(img)\n",
        "    return img\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.size:\n",
        "      return self.size\n",
        "    return len(self.noise_file_path)\n",
        "    \n",
        "\n",
        "#Using this stack overflow (https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders)\n",
        "#(Suppose for example, you want to create batches of a list of varying dimension tensors. The below code pads sequences with 0 until the maximum sequence size of the batch,)\n",
        "#Collate_fn is a function that is used to process your batches before you pass it to dataloader. In my case since I have different sized images I need a way to stack batches b/c torch.stack won't work.\n",
        "#So I use zip which can accept tensors of different lengths and make them stacked with the size of the lowest length list given. Therefore stacking all the images in a batch \n",
        "#Successfully unlike torch.stack and doing that processing to every batch makes collate_fn vital since I have different image sizes.\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return tuple([list(a) for a in zip(*batch)])\n",
        "    # return tuple(zip(*batch))\n",
        "\n",
        "train_batch_size = 2\n",
        "test_batch_size = 2\n",
        "noise_path = \"/content/noisy_dataset\"\n",
        "\n",
        "train_dataset = FruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"train\"), mode = \"train\", noisy_dataset_path=noise_path)                               \n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= collate_fn)\n",
        "\n",
        "test_dataset = FruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"test\"), mode = \"test\", noisy_dataset_path=noise_path)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = test_batch_size, shuffle = True, collate_fn= collate_fn)\n",
        "\n",
        "noise_dataset = NoiseDataset(noise_path, 100, 512)\n",
        "noise_loader = torch.utils.data.DataLoader(noise_dataset, batch_size = test_batch_size, shuffle = True)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Noisy Has been subsetted\n",
            "Extended 32 noisy images to train set\n",
            "Noisy Has been subsetted\n",
            "Extended 8 noisy images to test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "-uf9fdpPK-SQ",
        "outputId": "ea570e96-b5be-48b8-a228-99089489a49a"
      },
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(2):\n",
        "    ax = fig.add_subplot(2, 2/2, idx+1, xticks=[], yticks=[])\n",
        "    image = draw_boxes(labels[idx][\"boxes\"], labels[idx][\"labels\"], images[idx])\n",
        "    plt.imshow(image)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A0EkOWcPSX0"
      },
      "source": [
        "## What we learned\n",
        "is that the model outputs losses when in train mode \n",
        "when in model.eval model, the model code then return only a prediction with no losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NQrBD22VWxD"
      },
      "source": [
        "def calculate_metrics(target_box,predictions_box,scores, device):\n",
        "\n",
        "    #Get most confident boxes first and least confident last\n",
        "    predictions_box = predictions_box[scores.argsort().flip(-1)]\n",
        "    iou_mat = box_iou(target_box,predictions_box)\n",
        "    #return a one by one matrix that is form (target_box, prediction_box) or (1, 1)\n",
        "    target_boxes_count, prediction_boxes_count = iou_mat.shape\n",
        "    \n",
        "    mAP_Matrix = torch.zeros_like(iou_mat)\n",
        "    # if not matrix coordinates that relate to nothing.\n",
        "    if not iou_mat[:,0].eq(0.).all():\n",
        "      index_of_biggest_iou = iou_mat[:,0].argsort()[-1]\n",
        "      mAP_Matrix[index_of_biggest_iou,0] = 1\n",
        "\n",
        "    for pr_idx in range(1,prediction_boxes_count):\n",
        "        not_assigned = torch.logical_not(mAP_Matrix[:,:pr_idx].sum(1)).long()\n",
        "        targets = not_assigned * iou_mat[:,pr_idx]\n",
        "\n",
        "        if targets.eq(0).all():\n",
        "            continue\n",
        "\n",
        "        pivot = targets.argsort()[-1]\n",
        "        mAP_Matrix[pivot,pr_idx] = 1\n",
        "\n",
        "    # mAP calculation\n",
        "    tp = mAP_Matrix.sum()\n",
        "    fp = mAP_Matrix.sum(0).eq(0).sum()\n",
        "    fn = mAP_Matrix.sum(1).eq(0).sum()\n",
        "\n",
        "    mAP = tp / (tp+fp)\n",
        "    mAR = tp / (tp+fn)\n",
        "\n",
        "    return mAP, mAR\n",
        "\n",
        "def run_metrics_for_batch(output, targets, mAP, mAR, missed_images, device):\n",
        "  for pos_in_batch, image_pred in enumerate(output):\n",
        "    assert (len(image_pred[\"boxes\"]) == len(image_pred[\"labels\"]) == len(image_pred[\"scores\"]))\n",
        "    if len(image_pred[\"boxes\"]) != 0:\n",
        "      curr_mAP, curr_mAR = calculate_metrics(targets[pos_in_batch][\"boxes\"], output[pos_in_batch][\"boxes\"], output[pos_in_batch][\"scores\"], device)\n",
        "      mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
        "    else:\n",
        "      missed_images += 1 \n",
        "  \n",
        "  return mAP, mAR, missed_images\n",
        "\n",
        "# def run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device):\n",
        "#     assert (len(scores) == len(classification) == len(transformed_anchors))\n",
        "#     if len(transformed_anchors) != 0:\n",
        "#       curr_mAP, curr_mAR = calculate_metrics(targets[0][:, :4], transformed_anchors, scores, device)\n",
        "#       mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
        "#     else:\n",
        "#       missed_images += 1 \n",
        "      \n",
        "#     return mAP, mAR, missed_images\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB_w3zeIOa8X"
      },
      "source": [
        "def train(net, epochs, train_loader, test_loader, noise_loader, lr, weight_decay, \n",
        "          print_every = 6, lo_test_dataset = len(test_dataset), lo_train_dataset = len(train_dataset),\n",
        "          lo_noise_dataset = len(noise_dataset)):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Check which parameters can calculate gradients. \n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "\n",
        "    # optimizer = Ranger(net.parameters(), lr = lr, weight_decay= weight_decay)\n",
        "    base_optimizer = Ranger\n",
        "    optimizer = sam.SAM(net.parameters(), base_optimizer, lr = lr, weight_decay = weight_decay)\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
        "\n",
        "    net.to(device)\n",
        "    print(\"Device: {}\".format(device))\n",
        "    print(\"Optimizer: {}\".format(optimizer))\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        \n",
        "        train_loss = steps = train_mAP = train_mAR = missed_train_images = 0\n",
        "        \n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "            net.train()\n",
        "\n",
        "            steps += 1\n",
        "            \n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            net.eval()\n",
        "            train_mAP, train_mAR, missed_train_images = run_metrics_for_batch(net(images), targets, train_mAP, train_mAR, missed_train_images, device)\n",
        "            net.train()\n",
        "\n",
        "            losses.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            optimizer.first_step(zero_grad = True)\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            losses.backward()\n",
        "            optimizer.second_step(zero_grad = True)\n",
        "\n",
        "            train_loss +=  losses.item()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (steps % print_every) == 0:\n",
        "\n",
        "              with torch.no_grad():\n",
        "                test_mAP = test_mAR = missed_test_images = test_loss = correct_missed_images = 0\n",
        "\n",
        "                for noise_images in noise_loader:\n",
        "                  \n",
        "                  net.eval()\n",
        "                  if device == torch.device(\"cuda\"):\n",
        "                    noise_images = [noise_image.to(device) for noise_image in noise_images]\n",
        "                  \n",
        "                  output = net(noise_images)\n",
        "\n",
        "                  for ii in range(len(output)):\n",
        "                    if len(output[ii][\"boxes\"]) == 0:\n",
        "                      correct_missed_images += 1\n",
        "\n",
        "                for images, targets in test_loader:\n",
        "\n",
        "                  net.eval()\n",
        "                  if device == torch.device(\"cuda\"):\n",
        "                    images = [image.to(device) for image in images]\n",
        "                    targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "                  output = net(images)\n",
        "                  test_mAP, test_mAR, missed_test_images = run_metrics_for_batch(output, targets, test_mAP, test_mAR, missed_test_images, device)\n",
        "\n",
        "                  net.train()\n",
        "                  test_loss_dict = net(images, targets)\n",
        "                  test_losses = sum(loss for loss in test_loss_dict.values())\n",
        "                  test_loss += test_losses.item()\n",
        "\n",
        "                for param_group in optimizer.param_groups:\n",
        "                  learning_rate_extract = param_group[\"lr\"]\n",
        "                print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Test_loss: {:0.2f} | Test mAP: {:0.2f}% | Missed Test Images: {} | Seperate Noise Loader: {} / {}\".format(\n",
        "                    epoch + 1, epochs, steps, learning_rate_extract, train_loss, test_loss,  \n",
        "                    (test_mAP / float(lo_test_dataset)) * 100.,missed_test_images,\n",
        "                    correct_missed_images, lo_noise_dataset))\n",
        "\n",
        "              assert (steps % print_every) == 0\n",
        "              train_loss = 0\n",
        "                 \n",
        "        print(\"\\n Epoch {} Final Train mAP: {:0.2f}% | Epoch {} Final Missed Train Images: {} out of {} images \\n\".format(\n",
        "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, missed_train_images, lo_train_dataset\n",
        "        ))\n",
        "    \n",
        "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))\n",
        "\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYAu4FDknTwH"
      },
      "source": [
        "# Effecient Det Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWHkD1yEt1LY"
      },
      "source": [
        "!pip install timm\n",
        "!pip install effdet\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, unwrap_bench\n",
        "from effdet.efficientdet import HeadNet\n",
        "from effdet.data.transforms import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EHpxq96t-Ii"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "MAX_NUM_INSTANCES = 100\n",
        "class DetectionFastCollate:\n",
        "    \"\"\" A detection specific, optimized collate function w/ a bit of state.\n",
        "    Optionally performs anchor labelling. Doing this here offloads some work from the\n",
        "    GPU and the main training process thread and increases the load on the dataloader\n",
        "    threads.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            instance_keys=None,\n",
        "            instance_shapes=None,\n",
        "            instance_fill=-1,\n",
        "            max_instances=MAX_NUM_INSTANCES,\n",
        "            anchor_labeler=None,\n",
        "    ):\n",
        "        instance_keys = instance_keys or {'bbox', 'bbox_ignore', 'cls'}\n",
        "        instance_shapes = instance_shapes or dict(\n",
        "            bbox=(max_instances, 4), bbox_ignore=(max_instances, 4), cls=(max_instances,))\n",
        "        self.instance_info = {k: dict(fill=instance_fill, shape=instance_shapes[k]) for k in instance_keys}\n",
        "        self.max_instances = max_instances\n",
        "        self.anchor_labeler = anchor_labeler\n",
        "\n",
        "    def __call__(self, batch):\n",
        "\n",
        "        # print(batch[0][2])\n",
        "        # print(type(batch[0][2]))\n",
        "        \n",
        "        batch_size = len(batch)\n",
        "        target = dict()\n",
        "        labeler_outputs = dict()\n",
        "        img_tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n",
        "        mAP_translated_boxes = list()\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            img_tensor[i] += torch.from_numpy(batch[i][0])\n",
        "            mAP_translated_boxes.append(torch.from_numpy(batch[i][2]))\n",
        "            labeler_inputs = {}\n",
        "            for tk, tv in batch[i][1].items():\n",
        "                instance_info = self.instance_info.get(tk, None)\n",
        "                if instance_info is not None:\n",
        "                    # target tensor is associated with a detection instance\n",
        "                    tv = torch.from_numpy(tv).to(dtype=torch.float32)\n",
        "                    if self.anchor_labeler is None:\n",
        "                        if i == 0:\n",
        "                            shape = (batch_size,) + instance_info['shape']\n",
        "                            target_tensor = torch.full(shape, instance_info['fill'], dtype=torch.float32)\n",
        "                            target[tk] = target_tensor\n",
        "                        else:\n",
        "                            target_tensor = target[tk]\n",
        "                        num_elem = min(tv.shape[0], self.max_instances)\n",
        "                        target_tensor[i, 0:num_elem] = tv[0:num_elem]\n",
        "                    else:\n",
        "                        # no need to pass gt tensors through when labeler in use\n",
        "                        if tk in ('bbox', 'cls'):\n",
        "                            labeler_inputs[tk] = tv\n",
        "                else:\n",
        "                    # target tensor is an image-level annotation / metadata\n",
        "                    if i == 0:\n",
        "                        # first batch elem, create destination tensors\n",
        "                        if isinstance(tv, (tuple, list)):\n",
        "                            # per batch elem sequence\n",
        "                            shape = (batch_size, len(tv))\n",
        "                            dtype = torch.float32 if isinstance(tv[0], (float, np.floating)) else torch.int32\n",
        "                        else:\n",
        "                            # per batch elem scalar\n",
        "                            shape = batch_size,\n",
        "                            dtype = torch.float32 if isinstance(tv, (float, np.floating)) else torch.int64\n",
        "                        target_tensor = torch.zeros(shape, dtype=dtype)\n",
        "                        target[tk] = target_tensor\n",
        "                    else:\n",
        "                        target_tensor = target[tk]\n",
        "                    target_tensor[i] = torch.tensor(tv, dtype=target_tensor.dtype)\n",
        "\n",
        "            if self.anchor_labeler is not None:\n",
        "                cls_targets, box_targets, num_positives = self.anchor_labeler.label_anchors(\n",
        "                    labeler_inputs['bbox'], labeler_inputs['cls'], filter_valid=False)\n",
        "                if i == 0:\n",
        "                    # first batch elem, create destination tensors, separate key per level\n",
        "                    for j, (ct, bt) in enumerate(zip(cls_targets, box_targets)):\n",
        "                        labeler_outputs[f'label_cls_{j}'] = torch.zeros(\n",
        "                            (batch_size,) + ct.shape, dtype=torch.int64)\n",
        "                        labeler_outputs[f'label_bbox_{j}'] = torch.zeros(\n",
        "                            (batch_size,) + bt.shape, dtype=torch.float32)\n",
        "                    labeler_outputs['label_num_positives'] = torch.zeros(batch_size)\n",
        "                for j, (ct, bt) in enumerate(zip(cls_targets, box_targets)):\n",
        "                    labeler_outputs[f'label_cls_{j}'][i] = ct\n",
        "                    labeler_outputs[f'label_bbox_{j}'][i] = bt\n",
        "                labeler_outputs['label_num_positives'][i] = num_positives\n",
        "        if labeler_outputs:\n",
        "            target.update(labeler_outputs)\n",
        "\n",
        "        return img_tensor, target, mAP_translated_boxes"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjFx0ZuYFca"
      },
      "source": [
        "class EffdetFruitDetectDataset(object):\n",
        "  def __init__(self, id_labels, id_bounding_boxes, transforms, mode, yxyx = True):\n",
        "\n",
        "    assert len(id_labels) == len(id_bounding_boxes)\n",
        "    assert sorted(id_labels.keys()) == sorted(id_bounding_boxes.keys())\n",
        "    self.imgs_key = sorted(id_labels.keys())\n",
        "\n",
        "    np.random.shuffle(self.imgs_key)\n",
        "    if (mode == \"train\"):\n",
        "      self.imgs_key = self.imgs_key[:int(len(self.imgs_key) * 0.8)]\n",
        "    else:\n",
        "      self.imgs_key = self.imgs_key[int(len(self.imgs_key) * 0.8):]\n",
        "\n",
        "    self.id_labels = id_labels\n",
        "    self.id_bounding_boxes = id_bounding_boxes\n",
        "    self.full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "\n",
        "    self.transforms = transforms\n",
        "    self.yxyx = yxyx\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_path = ffile_path(self.imgs_key[idx], self.full_image_file_paths) \n",
        "    # img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    boxes = convert_min_max(self.id_bounding_boxes[self.imgs_key[idx]])\n",
        "    #boxes in xyxy format\n",
        "    mAP_testable_bbox = np.array(boxes)\n",
        "    \n",
        "    labels = self.id_labels[self.imgs_key[idx]]\n",
        "    \n",
        "    target = {}\n",
        "    target['boxes'] = boxes\n",
        "    target['labels'] = labels\n",
        "\n",
        "    width, height = img.size\n",
        "    target = dict(img_idx=idx, img_size=(width, height))\n",
        "\n",
        "    bboxes = []\n",
        "    labels = []\n",
        "\n",
        "    ann ={}\n",
        "    ann['bbox'] = boxes\n",
        "    ann['label'] = labels\n",
        "\n",
        "    for ann in boxes:\n",
        "            ignore = False\n",
        "            x1, y1, x2, y2 = ann\n",
        "            label = 1\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            if w < 1 or h < 1:\n",
        "                ignore = True\n",
        "\n",
        "            bbox = ann\n",
        "\n",
        "            if self.yxyx:\n",
        "              bbox = [y1, x1, y2, x2]\n",
        "            \n",
        "            bboxes.append(bbox)\n",
        "            labels.append(label)\n",
        "\n",
        "\n",
        "    bboxes = np.array(bboxes, ndmin=2, dtype=np.float32) - 1\n",
        "\n",
        "\n",
        "    labels = np.array(labels, dtype=np.float32)\n",
        "\n",
        "    ann = dict(\n",
        "            bbox=bboxes.astype(np.float32),\n",
        "            cls=labels.astype(np.int64))\n",
        "\n",
        "    target.update(ann)\n",
        "\n",
        "    if self.transforms is not None:\n",
        "        img, target = self.transforms(img, target)\n",
        "    \n",
        "    #mAP_testable_bbox = target[\"bbox\"]\n",
        "\n",
        "    return img, target, mAP_testable_bbox\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs_key)\n",
        "\n",
        "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
        "IMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)\n",
        "IMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)\n",
        "\n",
        "def run_metrics_for_batch_for_effdet(output, targets, mAP, mAR, device):\n",
        "\n",
        "  for batch_num, _ in enumerate(output):\n",
        "    boxes, scores = output[batch_num][:, :4], output[batch_num][:, 4]\n",
        "    curr_target = targets[batch_num]\n",
        "    if len(boxes) == 0:\n",
        "      raise RuntimeError(\"Cannot run metrics on an empty box\")\n",
        "    \n",
        "    curr_mAP, curr_mAR = calculate_metrics(curr_target, boxes, scores, device)\n",
        "    mAP += curr_mAP\n",
        "    mAR += curr_mAR \n",
        "  \n",
        "  return mAP, mAR\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujHCSsyzvcsT"
      },
      "source": [
        "def define_effdet_parameters():\n",
        "  global image_size\n",
        "  global model_name\n",
        "  global num_cls\n",
        "  global pretrained\n",
        "  global pretrained_backbone\n",
        "  global redundant_bias\n",
        "  global label_smoothing\n",
        "  global legacy_focal \n",
        "  global jit_loss\n",
        "  global soft_nms\n",
        "  global bench_labeler\n",
        "\n",
        "  image_size = 640\n",
        "  model_name = \"efficientdet_q1\"\n",
        "  num_cls = 6\n",
        "  pretrained=True\n",
        "  pretrained_backbone=True\n",
        "  redundant_bias=None\n",
        "  label_smoothing=None\n",
        "  legacy_focal=None\n",
        "  jit_loss=None\n",
        "  soft_nms=None\n",
        "  bench_labeler=None\n",
        "\n",
        "define_effdet_parameters()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmonKCf2YtY2"
      },
      "source": [
        "from effdet.anchors import Anchors, AnchorLabeler\n",
        "from effdet.factory import create_model, create_model_from_config\n",
        "from timm.utils import *\n",
        "from contextlib import suppress\n",
        "from collections import OrderedDict\n",
        "from timm.models.layers import set_layer_config\n",
        "\n",
        "model = create_model(\n",
        "  model_name,\n",
        "  bench_task='train',\n",
        "  num_classes=num_cls,\n",
        "  pretrained=pretrained,\n",
        "  pretrained_backbone= pretrained_backbone,\n",
        "  redundant_bias=redundant_bias,\n",
        "  label_smoothing= label_smoothing,\n",
        "  legacy_focal=legacy_focal,\n",
        "  jit_loss=jit_loss,\n",
        "  soft_nms=soft_nms,\n",
        "  bench_labeler=bench_labeler,\n",
        "  checkpoint_path='',\n",
        "  max_det_per_image = 5, #This value should be changed. The amount of images effdet predicts. May affect mAP and mAR. \n",
        ")\n",
        "\n",
        "amp_autocast = suppress\n",
        "model_config = model.config\n",
        "anchor_labeler = AnchorLabeler(\n",
        "            Anchors.from_config(model_config), num_cls, match_threshold=0.5)\n",
        "\n",
        "transform = transforms_coco_eval(\n",
        "            (image_size, image_size),\n",
        "            interpolation='bilinear',\n",
        "            use_prefetcher=True,\n",
        "            fill_color='mean',\n",
        "            mean=IMAGENET_DEFAULT_MEAN,\n",
        "            std=IMAGENET_DEFAULT_STD)\n",
        "\n",
        "train_batch_size = 2\n",
        "valid_batch_size = 2\n",
        "\n",
        "eff_train_dataset = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, transform, mode = \"train\")\n",
        "eff_train_loader = torch.utils.data.DataLoader(eff_train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= DetectionFastCollate(anchor_labeler=anchor_labeler))\n",
        "\n",
        "eff_test_dataset = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, transform, mode = \"test\")\n",
        "eff_test_loader = torch.utils.data.DataLoader(eff_test_dataset, batch_size = valid_batch_size, shuffle = True, collate_fn = DetectionFastCollate(anchor_labeler=anchor_labeler))\n",
        "# eff_test_dataset_2 = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, transform, mode = \"test\", yxyx=False)\n",
        "# eff_test_loader_2 = torch.utils.data.DataLoader(eff_test_dataset_2, batch_size = valid_batch_size, shuffle = True, collate_fn = DetectionFastCollate(anchor_labeler=anchor_labeler))\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbXEJySC236L"
      },
      "source": [
        "def train_epoch(\n",
        "        epoch, model, loader, optimizer, \n",
        "        lr_scheduler=None, saver=None, output_dir='',  loss_scaler=None, model_ema=None):\n",
        "  \n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    else:\n",
        "      warnings.warn(\"Function does not support handling models on CPU\")\n",
        "\n",
        "    batch_time_m = AverageMeter()\n",
        "    data_time_m = AverageMeter()\n",
        "    losses_m = AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    last_idx = len(loader) - 1\n",
        "    num_updates = epoch * len(loader)\n",
        "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
        "    for batch_idx, (input, target, _) in enumerate(loader):\n",
        "        last_batch = batch_idx == last_idx\n",
        "        data_time_m.update(time.time() - end)\n",
        "\n",
        "        input = input.cuda().float()\n",
        "        target2={}\n",
        "        for k,v in target.items():\n",
        "          target2[k]=v.cuda()\n",
        "  \n",
        "\n",
        "        with amp_autocast():\n",
        "          output = model(input, target2)\n",
        "        loss = output['loss']\n",
        "\n",
        "        losses_m.update(loss.item(), input.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        " \n",
        "        loss.backward()\n",
        "        # optimizer.first_step(zero_grad = True)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "\n",
        "        # with amp_autocast():\n",
        "        #   output = model(input, target2)\n",
        "        # loss = output['loss']\n",
        "        # loss.backward()\n",
        "        # optimizer.second_step(zero_grad = True)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        num_updates += 1\n",
        "        # scheduler.step()\n",
        "\n",
        "        batch_time_m.update(time.time() - end)\n",
        "        if last_batch or batch_idx % 10 == 0:\n",
        "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
        "            lr = sum(lrl) / len(lrl)\n",
        "\n",
        "            print(\n",
        "                'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '\n",
        "                'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f})  '\n",
        "                'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '\n",
        "                '({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
        "                'LR: {lr:.3e}  '\n",
        "                'Data: {data_time.val:.3f} ({data_time.avg:.3f})'.format(\n",
        "                    epoch,\n",
        "                    batch_idx, len(loader),\n",
        "                    100. * batch_idx / last_idx,\n",
        "                    loss=losses_m,\n",
        "                    batch_time=batch_time_m,\n",
        "                    rate=input.size(0) * 1 / batch_time_m.val,\n",
        "                    rate_avg=input.size(0) * 1  / batch_time_m.avg,\n",
        "                    lr=lr,\n",
        "                    data_time=data_time_m))\n",
        "\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "\n",
        "\n",
        "    if hasattr(optimizer, 'sync_lookahead'):\n",
        "        optimizer.sync_lookahead()\n",
        "\n",
        "    return OrderedDict([('loss', losses_m.avg)])\n",
        "\n",
        "from itertools import chain\n",
        "\n",
        "#Changed the evaluator argument.\n",
        "def validate(bench, model, loader, valid_batch_size, args, log_suffix=''):\n",
        "    batch_time_m = AverageMeter()\n",
        "    losses_m = AverageMeter()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    last_idx = len(loader) - 1\n",
        "    with torch.no_grad():\n",
        "        valid_mAP, valid_mAR, steps = 0, 0, 0\n",
        "      #Changed this line \n",
        "        for (input, target, mAP_formatted_target) in loader:\n",
        "            steps += 1 \n",
        "            input = input.cuda().float()\n",
        "            target2={}\n",
        "            for k,v in target.items():\n",
        "              target2[k]=v.cuda()\n",
        "            \n",
        "            #Get mAP and mAR (Can possibly add some thresholding to this value)\n",
        "            pred = bench(input)\n",
        "            mAP_formatted_target = [tensor.cuda() for tensor in mAP_formatted_target]\n",
        "            valid_mAP, valid_mAR = run_metrics_for_batch_for_effdet(pred, mAP_formatted_target, valid_mAP, valid_mAR, device)\n",
        "\n",
        "            #Get Test Loss\n",
        "            output = model(input, target2)\n",
        "            loss = output['loss']\n",
        "            reduced_loss = loss.data\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "            losses_m.update(reduced_loss.item(), input.size(0))\n",
        "\n",
        "            batch_time_m.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "    num_of_images_tested = valid_batch_size * steps\n",
        "    metrics = OrderedDict([('loss', losses_m.avg), (\"Valid mAP\", str(int(valid_mAP / num_of_images_tested * 100)) + \"%\"), (\"Valid mAR\", str(int(valid_mAR / num_of_images_tested * 100)) + \"%\")])\n",
        "    return metrics\n",
        "\n",
        "def create_effdet_model(checkpoint):\n",
        "  pretrained=True\n",
        "  pretrained = pretrained or not checkpoint  # might as well try to validate something\n",
        "  with set_layer_config(scriptable='store_true'):\n",
        "    bench = create_model(\n",
        "              model_name,\n",
        "              bench_task='predict',\n",
        "              num_classes=num_cls,\n",
        "              pretrained=pretrained,\n",
        "              pretrained_backbone=pretrained_backbone,\n",
        "              redundant_bias=redundant_bias,\n",
        "              label_smoothing=label_smoothing,\n",
        "              legacy_focal=legacy_focal,\n",
        "              jit_loss=jit_loss,\n",
        "              soft_nms=soft_nms,\n",
        "              bench_labeler=bench_labeler,\n",
        "              checkpoint_path=checkpoint, \n",
        "              max_det_per_image = 5\n",
        "          )\n",
        "    \n",
        "  bench.cuda()\n",
        "  bench.eval()\n",
        "\n",
        "  return bench\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIcjkG9PQwNR"
      },
      "source": [
        "Train: 6 [ 211/212 (100%)]  Loss:  0.727989 (0.7556)  Time: 0.172s,   11.60/s  (0.175s,   11.41/s)  LR: 1.000e-04  Data: 0.029 (0.031)\n",
        "train OrderedDict([('loss', 0.7555575621015621)])\n",
        "test OrderedDict([('loss', 1.0847256957927598), ('Valid mAP', '47%'), ('Valid mAR', '77%')])\n",
        "\n",
        "\n",
        "Ranger:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edl4rQVv3Myj",
        "outputId": "be855938-3338-4021-8cd1-dff0aa005846"
      },
      "source": [
        "from timm.utils import get_outdir\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "saving_folder = \"exp14\"\n",
        "\n",
        "output_dir = get_outdir('/content', 'train', saving_folder)\n",
        "mAP_helper_checkpoints_dir = get_outdir('/content', 'mAP', saving_folder)\n",
        "eval_metric='loss'\n",
        "decreasing = True if eval_metric == 'loss' else False\n",
        "\n",
        "optimizer= torch.optim.AdamW(model.parameters(), lr=.0001)\n",
        "# base_optimizer = torch.optim.AdamW\n",
        "# optimizer = sam.SAM(model.parameters(), base_optimizer, lr = .0001)\n",
        "\n",
        "# optimizer = Ranger(model.parameters(), lr = 0.0001, weight_decay = 1e-5)\n",
        "\n",
        "saver = CheckpointSaver(\n",
        "model, optimizer, args=None, model_ema=None, amp_scaler=None,\n",
        "checkpoint_dir=output_dir, decreasing=decreasing, unwrap_fn = unwrap_bench)\n",
        "\n",
        "saver_for_mAP_calc = CheckpointSaver(\n",
        "    model, optimizer, args = None, model_ema = None, amp_scaler = None,\n",
        "    checkpoint_dir = mAP_helper_checkpoints_dir, decreasing=decreasing, unwrap_fn = unwrap_bench)\n",
        "\n",
        "\n",
        "#Change this for epochs.\n",
        "epochs = 8\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "\n",
        "  train_eval_metrics= train_epoch(\n",
        "          epoch , model, eff_train_loader, optimizer,saver=saver)\n",
        "  print('train',train_eval_metrics)\n",
        "\n",
        "  #Saved prev trained model\n",
        "  saver_for_mAP_calc.save_checkpoint(epoch = epoch, metric = train_eval_metrics[\"loss\"])\n",
        "\n",
        "  checkpoint = os.path.join(mAP_helper_checkpoints_dir,\"last.pth.tar\")\n",
        "  bench = create_effdet_model(checkpoint)\n",
        "  valid_eval_metrics = validate(bench, model, eff_test_loader, valid_batch_size, args = None)\n",
        "\n",
        "  #Make sure to uncomment this line for saving the model\n",
        "  best_metric, best_epoch = saver.save_checkpoint(epoch=epoch,metric=valid_eval_metrics['loss'])\n",
        "\n",
        "  print('test',valid_eval_metrics)\n",
        "  print('')\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 0 [   0/212 (  0%)]  Loss:  2.136421 (2.1364)  Time: 0.440s,    4.55/s  (0.440s,    4.55/s)  LR: 1.000e-04  Data: 0.050 (0.050)\n",
            "Train: 0 [  10/212 (  5%)]  Loss:  1.754273 (1.8219)  Time: 0.444s,    4.51/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.078 (0.052)\n",
            "Train: 0 [  20/212 (  9%)]  Loss:  1.662248 (1.6926)  Time: 0.427s,    4.68/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.066 (0.052)\n",
            "Train: 0 [  30/212 ( 14%)]  Loss:  1.328310 (1.6009)  Time: 0.409s,    4.88/s  (0.420s,    4.77/s)  LR: 1.000e-04  Data: 0.042 (0.051)\n",
            "Train: 0 [  40/212 ( 19%)]  Loss:  1.199301 (1.5416)  Time: 0.422s,    4.74/s  (0.419s,    4.77/s)  LR: 1.000e-04  Data: 0.042 (0.050)\n",
            "Train: 0 [  50/212 ( 24%)]  Loss:  1.091582 (1.4824)  Time: 0.412s,    4.85/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.046 (0.051)\n",
            "Train: 0 [  60/212 ( 28%)]  Loss:  1.048268 (1.4234)  Time: 0.413s,    4.85/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.051 (0.051)\n",
            "Train: 0 [  70/212 ( 33%)]  Loss:  1.108536 (1.3895)  Time: 0.408s,    4.90/s  (0.420s,    4.76/s)  LR: 1.000e-04  Data: 0.046 (0.051)\n",
            "Train: 0 [  80/212 ( 38%)]  Loss:  0.880575 (1.3460)  Time: 0.410s,    4.88/s  (0.419s,    4.77/s)  LR: 1.000e-04  Data: 0.044 (0.050)\n",
            "Train: 0 [  90/212 ( 43%)]  Loss:  0.838100 (1.3054)  Time: 0.411s,    4.87/s  (0.420s,    4.76/s)  LR: 1.000e-04  Data: 0.044 (0.050)\n",
            "Train: 0 [ 100/212 ( 47%)]  Loss:  0.791087 (1.2783)  Time: 0.414s,    4.83/s  (0.420s,    4.76/s)  LR: 1.000e-04  Data: 0.047 (0.050)\n",
            "Train: 0 [ 110/212 ( 52%)]  Loss:  1.006925 (1.2454)  Time: 0.415s,    4.82/s  (0.420s,    4.76/s)  LR: 1.000e-04  Data: 0.042 (0.050)\n",
            "Train: 0 [ 120/212 ( 57%)]  Loss:  0.704605 (1.2172)  Time: 0.412s,    4.85/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.051 (0.050)\n",
            "Train: 0 [ 130/212 ( 62%)]  Loss:  1.086039 (1.1851)  Time: 0.447s,    4.48/s  (0.420s,    4.76/s)  LR: 1.000e-04  Data: 0.073 (0.050)\n",
            "Train: 0 [ 140/212 ( 66%)]  Loss:  0.761057 (1.1613)  Time: 0.417s,    4.80/s  (0.421s,    4.76/s)  LR: 1.000e-04  Data: 0.055 (0.050)\n",
            "Train: 0 [ 150/212 ( 71%)]  Loss:  0.568847 (1.1343)  Time: 0.408s,    4.90/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.046 (0.050)\n",
            "Train: 0 [ 160/212 ( 76%)]  Loss:  0.543071 (1.1120)  Time: 0.415s,    4.82/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.049 (0.051)\n",
            "Train: 0 [ 170/212 ( 81%)]  Loss:  0.596574 (1.0963)  Time: 0.424s,    4.72/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.051 (0.051)\n",
            "Train: 0 [ 180/212 ( 85%)]  Loss:  0.754205 (1.0743)  Time: 0.440s,    4.55/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.064 (0.051)\n",
            "Train: 0 [ 190/212 ( 90%)]  Loss:  1.112764 (1.0591)  Time: 0.406s,    4.93/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.043 (0.051)\n",
            "Train: 0 [ 200/212 ( 95%)]  Loss:  0.934640 (1.0428)  Time: 0.405s,    4.94/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.051 (0.051)\n",
            "Train: 0 [ 210/212 (100%)]  Loss:  0.635514 (1.0253)  Time: 0.403s,    4.96/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.044 (0.051)\n",
            "Train: 0 [ 211/212 (100%)]  Loss:  1.153569 (1.0259)  Time: 0.427s,    4.69/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.044 (0.050)\n",
            "train OrderedDict([('loss', 1.0259039039319415)])\n",
            "test OrderedDict([('loss', 242.64670828179777), ('Valid mAP', '46%'), ('Valid mAR', '76%')])\n",
            "\n",
            "Train: 1 [   0/212 (  0%)]  Loss:  0.491466 (0.4915)  Time: 0.443s,    4.52/s  (0.443s,    4.52/s)  LR: 1.000e-04  Data: 0.058 (0.058)\n",
            "Train: 1 [  10/212 (  5%)]  Loss:  0.392813 (0.5623)  Time: 0.420s,    4.76/s  (0.420s,    4.76/s)  LR: 1.000e-04  Data: 0.052 (0.050)\n",
            "Train: 1 [  20/212 (  9%)]  Loss:  0.446205 (0.5729)  Time: 0.416s,    4.80/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.054 (0.051)\n",
            "Train: 1 [  30/212 ( 14%)]  Loss:  0.591689 (0.5727)  Time: 0.411s,    4.87/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.045 (0.050)\n",
            "Train: 1 [  40/212 ( 19%)]  Loss:  0.568759 (0.5554)  Time: 0.411s,    4.87/s  (0.422s,    4.73/s)  LR: 1.000e-04  Data: 0.042 (0.050)\n",
            "Train: 1 [  50/212 ( 24%)]  Loss:  0.345842 (0.5469)  Time: 0.408s,    4.90/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.046 (0.049)\n",
            "Train: 1 [  60/212 ( 28%)]  Loss:  0.393545 (0.5432)  Time: 0.406s,    4.93/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.043 (0.049)\n",
            "Train: 1 [  70/212 ( 33%)]  Loss:  0.493350 (0.5413)  Time: 0.436s,    4.59/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.061 (0.050)\n",
            "Train: 1 [  80/212 ( 38%)]  Loss:  0.660446 (0.5428)  Time: 0.431s,    4.64/s  (0.422s,    4.73/s)  LR: 1.000e-04  Data: 0.050 (0.050)\n",
            "Train: 1 [  90/212 ( 43%)]  Loss:  0.442570 (0.5364)  Time: 0.409s,    4.89/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.042 (0.050)\n",
            "Train: 1 [ 100/212 ( 47%)]  Loss:  0.423535 (0.5380)  Time: 0.420s,    4.77/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.049 (0.050)\n",
            "Train: 1 [ 110/212 ( 52%)]  Loss:  0.580928 (0.5318)  Time: 0.413s,    4.85/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.047 (0.050)\n",
            "Train: 1 [ 120/212 ( 57%)]  Loss:  0.658209 (0.5301)  Time: 0.446s,    4.49/s  (0.422s,    4.73/s)  LR: 1.000e-04  Data: 0.062 (0.050)\n",
            "Train: 1 [ 130/212 ( 62%)]  Loss:  0.690301 (0.5245)  Time: 0.412s,    4.85/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.050 (0.050)\n",
            "Train: 1 [ 140/212 ( 66%)]  Loss:  0.421454 (0.5175)  Time: 0.418s,    4.78/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.047 (0.050)\n",
            "Train: 1 [ 150/212 ( 71%)]  Loss:  0.423509 (0.5119)  Time: 0.414s,    4.84/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.051 (0.050)\n",
            "Train: 1 [ 160/212 ( 76%)]  Loss:  0.399809 (0.5108)  Time: 0.412s,    4.86/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.052 (0.050)\n",
            "Train: 1 [ 170/212 ( 81%)]  Loss:  0.295443 (0.5101)  Time: 0.401s,    4.98/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.044 (0.050)\n",
            "Train: 1 [ 180/212 ( 85%)]  Loss:  0.476209 (0.5056)  Time: 0.426s,    4.70/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.054 (0.050)\n",
            "Train: 1 [ 190/212 ( 90%)]  Loss:  0.406008 (0.5033)  Time: 0.407s,    4.91/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.043 (0.050)\n",
            "Train: 1 [ 200/212 ( 95%)]  Loss:  0.533253 (0.5010)  Time: 0.445s,    4.49/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.068 (0.050)\n",
            "Train: 1 [ 210/212 (100%)]  Loss:  0.414379 (0.4994)  Time: 0.422s,    4.74/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.045 (0.050)\n",
            "Train: 1 [ 211/212 (100%)]  Loss:  0.517886 (0.4995)  Time: 0.421s,    4.76/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.051 (0.050)\n",
            "train OrderedDict([('loss', 0.49951863288879395)])\n",
            "test OrderedDict([('loss', 1567.5694840625624), ('Valid mAP', '45%'), ('Valid mAR', '75%')])\n",
            "\n",
            "Train: 2 [   0/212 (  0%)]  Loss:  0.353877 (0.3539)  Time: 0.434s,    4.61/s  (0.434s,    4.61/s)  LR: 1.000e-04  Data: 0.046 (0.046)\n",
            "Train: 2 [  10/212 (  5%)]  Loss:  0.729791 (0.3612)  Time: 0.457s,    4.38/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.080 (0.049)\n",
            "Train: 2 [  20/212 (  9%)]  Loss:  0.269771 (0.3654)  Time: 0.407s,    4.91/s  (0.425s,    4.71/s)  LR: 1.000e-04  Data: 0.045 (0.051)\n",
            "Train: 2 [  30/212 ( 14%)]  Loss:  0.441047 (0.3794)  Time: 0.447s,    4.48/s  (0.425s,    4.70/s)  LR: 1.000e-04  Data: 0.067 (0.052)\n",
            "Train: 2 [  40/212 ( 19%)]  Loss:  0.333351 (0.3802)  Time: 0.410s,    4.88/s  (0.425s,    4.71/s)  LR: 1.000e-04  Data: 0.052 (0.052)\n",
            "Train: 2 [  50/212 ( 24%)]  Loss:  0.259200 (0.3810)  Time: 0.419s,    4.77/s  (0.424s,    4.72/s)  LR: 1.000e-04  Data: 0.044 (0.051)\n",
            "Train: 2 [  60/212 ( 28%)]  Loss:  0.471650 (0.3868)  Time: 0.416s,    4.81/s  (0.424s,    4.72/s)  LR: 1.000e-04  Data: 0.048 (0.051)\n",
            "Train: 2 [  70/212 ( 33%)]  Loss:  0.440310 (0.3901)  Time: 0.430s,    4.65/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.059 (0.051)\n",
            "Train: 2 [  80/212 ( 38%)]  Loss:  0.374314 (0.3867)  Time: 0.428s,    4.67/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.049 (0.050)\n",
            "Train: 2 [  90/212 ( 43%)]  Loss:  0.308056 (0.3870)  Time: 0.451s,    4.43/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.054 (0.051)\n",
            "Train: 2 [ 100/212 ( 47%)]  Loss:  0.440454 (0.3848)  Time: 0.435s,    4.60/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.058 (0.051)\n",
            "Train: 2 [ 110/212 ( 52%)]  Loss:  0.284272 (0.3809)  Time: 0.426s,    4.69/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.050 (0.051)\n",
            "Train: 2 [ 120/212 ( 57%)]  Loss:  0.231228 (0.3774)  Time: 0.410s,    4.87/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.045 (0.051)\n",
            "Train: 2 [ 130/212 ( 62%)]  Loss:  0.642747 (0.3789)  Time: 0.448s,    4.46/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.076 (0.051)\n",
            "Train: 2 [ 140/212 ( 66%)]  Loss:  0.376876 (0.3770)  Time: 0.417s,    4.80/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.042 (0.051)\n",
            "Train: 2 [ 150/212 ( 71%)]  Loss:  0.501215 (0.3768)  Time: 0.427s,    4.68/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.054 (0.051)\n",
            "Train: 2 [ 160/212 ( 76%)]  Loss:  0.294257 (0.3749)  Time: 0.423s,    4.73/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.047 (0.051)\n",
            "Train: 2 [ 170/212 ( 81%)]  Loss:  0.362004 (0.3766)  Time: 0.413s,    4.85/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.049 (0.051)\n",
            "Train: 2 [ 180/212 ( 85%)]  Loss:  0.304151 (0.3752)  Time: 0.421s,    4.75/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.056 (0.050)\n",
            "Train: 2 [ 190/212 ( 90%)]  Loss:  0.261831 (0.3722)  Time: 0.424s,    4.72/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.052 (0.051)\n",
            "Train: 2 [ 200/212 ( 95%)]  Loss:  0.304730 (0.3703)  Time: 0.419s,    4.78/s  (0.422s,    4.73/s)  LR: 1.000e-04  Data: 0.049 (0.050)\n",
            "Train: 2 [ 210/212 (100%)]  Loss:  0.394623 (0.3701)  Time: 0.420s,    4.76/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.050 (0.050)\n",
            "Train: 2 [ 211/212 (100%)]  Loss:  0.385510 (0.3702)  Time: 0.428s,    4.68/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.045 (0.050)\n",
            "train OrderedDict([('loss', 0.3702056951961427)])\n",
            "test OrderedDict([('loss', 81.13378448491899), ('Valid mAP', '53%'), ('Valid mAR', '85%')])\n",
            "\n",
            "Train: 3 [   0/212 (  0%)]  Loss:  0.244186 (0.2442)  Time: 0.439s,    4.56/s  (0.439s,    4.56/s)  LR: 1.000e-04  Data: 0.042 (0.042)\n",
            "Train: 3 [  10/212 (  5%)]  Loss:  0.251413 (0.2737)  Time: 0.411s,    4.86/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.043 (0.048)\n",
            "Train: 3 [  20/212 (  9%)]  Loss:  0.262724 (0.2903)  Time: 0.409s,    4.89/s  (0.425s,    4.71/s)  LR: 1.000e-04  Data: 0.044 (0.051)\n",
            "Train: 3 [  30/212 ( 14%)]  Loss:  0.346743 (0.3053)  Time: 0.427s,    4.68/s  (0.425s,    4.71/s)  LR: 1.000e-04  Data: 0.048 (0.051)\n",
            "Train: 3 [  40/212 ( 19%)]  Loss:  0.245794 (0.3041)  Time: 0.428s,    4.67/s  (0.425s,    4.71/s)  LR: 1.000e-04  Data: 0.053 (0.052)\n",
            "Train: 3 [  50/212 ( 24%)]  Loss:  0.323128 (0.3125)  Time: 0.411s,    4.86/s  (0.425s,    4.71/s)  LR: 1.000e-04  Data: 0.043 (0.052)\n",
            "Train: 3 [  60/212 ( 28%)]  Loss:  0.222104 (0.3089)  Time: 0.401s,    4.99/s  (0.424s,    4.72/s)  LR: 1.000e-04  Data: 0.042 (0.052)\n",
            "Train: 3 [  70/212 ( 33%)]  Loss:  0.240222 (0.3076)  Time: 0.413s,    4.84/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.043 (0.051)\n",
            "Train: 3 [  80/212 ( 38%)]  Loss:  0.447490 (0.3057)  Time: 0.419s,    4.77/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.053 (0.051)\n",
            "Train: 3 [  90/212 ( 43%)]  Loss:  0.295472 (0.3071)  Time: 0.420s,    4.77/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.045 (0.051)\n",
            "Train: 3 [ 100/212 ( 47%)]  Loss:  0.226131 (0.3078)  Time: 0.432s,    4.63/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.050 (0.051)\n",
            "Train: 3 [ 110/212 ( 52%)]  Loss:  0.293099 (0.3058)  Time: 0.413s,    4.85/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.047 (0.051)\n",
            "Train: 3 [ 120/212 ( 57%)]  Loss:  0.365161 (0.3017)  Time: 0.425s,    4.71/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.051 (0.051)\n",
            "Train: 3 [ 130/212 ( 62%)]  Loss:  0.306217 (0.3006)  Time: 0.419s,    4.77/s  (0.421s,    4.75/s)  LR: 1.000e-04  Data: 0.048 (0.050)\n",
            "Train: 3 [ 140/212 ( 66%)]  Loss:  0.254028 (0.3039)  Time: 0.430s,    4.65/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.066 (0.051)\n",
            "Train: 3 [ 150/212 ( 71%)]  Loss:  0.351833 (0.3041)  Time: 0.425s,    4.71/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.054 (0.051)\n",
            "Train: 3 [ 160/212 ( 76%)]  Loss:  0.247904 (0.3007)  Time: 0.423s,    4.73/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.044 (0.051)\n",
            "Train: 3 [ 170/212 ( 81%)]  Loss:  0.324645 (0.2977)  Time: 0.432s,    4.63/s  (0.422s,    4.73/s)  LR: 1.000e-04  Data: 0.053 (0.050)\n",
            "Train: 3 [ 180/212 ( 85%)]  Loss:  0.240731 (0.2979)  Time: 0.427s,    4.68/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.046 (0.050)\n",
            "Train: 3 [ 190/212 ( 90%)]  Loss:  0.357926 (0.2989)  Time: 0.413s,    4.85/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.050 (0.050)\n",
            "Train: 3 [ 200/212 ( 95%)]  Loss:  0.244461 (0.2986)  Time: 0.422s,    4.73/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.049 (0.050)\n",
            "Train: 3 [ 210/212 (100%)]  Loss:  0.228271 (0.2973)  Time: 0.396s,    5.05/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.041 (0.050)\n",
            "Train: 3 [ 211/212 (100%)]  Loss:  0.236708 (0.2970)  Time: 0.434s,    4.61/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.058 (0.050)\n",
            "train OrderedDict([('loss', 0.2970309085300509)])\n",
            "test OrderedDict([('loss', 53.476632720816916), ('Valid mAP', '55%'), ('Valid mAR', '88%')])\n",
            "\n",
            "Train: 4 [   0/212 (  0%)]  Loss:  0.151539 (0.1515)  Time: 0.426s,    4.69/s  (0.426s,    4.69/s)  LR: 1.000e-04  Data: 0.046 (0.046)\n",
            "Train: 4 [  10/212 (  5%)]  Loss:  0.492409 (0.2612)  Time: 0.450s,    4.45/s  (0.424s,    4.71/s)  LR: 1.000e-04  Data: 0.091 (0.052)\n",
            "Train: 4 [  20/212 (  9%)]  Loss:  0.335932 (0.2708)  Time: 0.426s,    4.69/s  (0.427s,    4.69/s)  LR: 1.000e-04  Data: 0.056 (0.052)\n",
            "Train: 4 [  30/212 ( 14%)]  Loss:  0.235376 (0.2602)  Time: 0.404s,    4.95/s  (0.424s,    4.71/s)  LR: 1.000e-04  Data: 0.043 (0.052)\n",
            "Train: 4 [  40/212 ( 19%)]  Loss:  0.158136 (0.2494)  Time: 0.410s,    4.88/s  (0.424s,    4.72/s)  LR: 1.000e-04  Data: 0.044 (0.051)\n",
            "Train: 4 [  50/212 ( 24%)]  Loss:  0.477315 (0.2501)  Time: 0.448s,    4.47/s  (0.424s,    4.72/s)  LR: 1.000e-04  Data: 0.065 (0.051)\n",
            "Train: 4 [  60/212 ( 28%)]  Loss:  0.207869 (0.2516)  Time: 0.420s,    4.77/s  (0.424s,    4.72/s)  LR: 1.000e-04  Data: 0.044 (0.051)\n",
            "Train: 4 [  70/212 ( 33%)]  Loss:  0.245405 (0.2540)  Time: 0.421s,    4.75/s  (0.424s,    4.72/s)  LR: 1.000e-04  Data: 0.054 (0.051)\n",
            "Train: 4 [  80/212 ( 38%)]  Loss:  0.237960 (0.2543)  Time: 0.425s,    4.71/s  (0.424s,    4.72/s)  LR: 1.000e-04  Data: 0.057 (0.051)\n",
            "Train: 4 [  90/212 ( 43%)]  Loss:  0.314520 (0.2554)  Time: 0.416s,    4.81/s  (0.424s,    4.72/s)  LR: 1.000e-04  Data: 0.049 (0.051)\n",
            "Train: 4 [ 100/212 ( 47%)]  Loss:  0.197383 (0.2542)  Time: 0.404s,    4.95/s  (0.424s,    4.72/s)  LR: 1.000e-04  Data: 0.044 (0.051)\n",
            "Train: 4 [ 110/212 ( 52%)]  Loss:  0.189648 (0.2541)  Time: 0.446s,    4.49/s  (0.424s,    4.72/s)  LR: 1.000e-04  Data: 0.054 (0.051)\n",
            "Train: 4 [ 120/212 ( 57%)]  Loss:  0.182045 (0.2535)  Time: 0.417s,    4.80/s  (0.424s,    4.72/s)  LR: 1.000e-04  Data: 0.048 (0.051)\n",
            "Train: 4 [ 130/212 ( 62%)]  Loss:  0.303306 (0.2550)  Time: 0.412s,    4.86/s  (0.423s,    4.72/s)  LR: 1.000e-04  Data: 0.051 (0.051)\n",
            "Train: 4 [ 140/212 ( 66%)]  Loss:  0.159034 (0.2524)  Time: 0.398s,    5.02/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.044 (0.051)\n",
            "Train: 4 [ 150/212 ( 71%)]  Loss:  0.164756 (0.2517)  Time: 0.414s,    4.83/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.046 (0.051)\n",
            "Train: 4 [ 160/212 ( 76%)]  Loss:  0.250683 (0.2536)  Time: 0.433s,    4.62/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.053 (0.051)\n",
            "Train: 4 [ 170/212 ( 81%)]  Loss:  0.183670 (0.2526)  Time: 0.405s,    4.94/s  (0.423s,    4.73/s)  LR: 1.000e-04  Data: 0.044 (0.051)\n",
            "Train: 4 [ 180/212 ( 85%)]  Loss:  0.248909 (0.2510)  Time: 0.415s,    4.82/s  (0.423s,    4.72/s)  LR: 1.000e-04  Data: 0.058 (0.051)\n",
            "Train: 4 [ 190/212 ( 90%)]  Loss:  0.224777 (0.2500)  Time: 0.434s,    4.60/s  (0.423s,    4.72/s)  LR: 1.000e-04  Data: 0.046 (0.051)\n",
            "Train: 4 [ 200/212 ( 95%)]  Loss:  0.306079 (0.2495)  Time: 0.408s,    4.90/s  (0.423s,    4.72/s)  LR: 1.000e-04  Data: 0.042 (0.051)\n",
            "Train: 4 [ 210/212 (100%)]  Loss:  0.203298 (0.2488)  Time: 0.415s,    4.82/s  (0.423s,    4.72/s)  LR: 1.000e-04  Data: 0.044 (0.051)\n",
            "Train: 4 [ 211/212 (100%)]  Loss:  0.157702 (0.2484)  Time: 0.436s,    4.58/s  (0.423s,    4.72/s)  LR: 1.000e-04  Data: 0.050 (0.051)\n",
            "train OrderedDict([('loss', 0.24836235113863675)])\n",
            "test OrderedDict([('loss', 8.690899807019768), ('Valid mAP', '56%'), ('Valid mAR', '89%')])\n",
            "\n",
            "Train: 5 [   0/212 (  0%)]  Loss:  0.167865 (0.1679)  Time: 0.449s,    4.46/s  (0.449s,    4.46/s)  LR: 1.000e-04  Data: 0.048 (0.048)\n",
            "Train: 5 [  10/212 (  5%)]  Loss:  0.258703 (0.2034)  Time: 0.425s,    4.71/s  (0.426s,    4.69/s)  LR: 1.000e-04  Data: 0.052 (0.048)\n",
            "Train: 5 [  20/212 (  9%)]  Loss:  0.203680 (0.2088)  Time: 0.446s,    4.48/s  (0.427s,    4.69/s)  LR: 1.000e-04  Data: 0.064 (0.050)\n",
            "Train: 5 [  30/212 ( 14%)]  Loss:  0.182360 (0.2015)  Time: 0.415s,    4.82/s  (0.426s,    4.69/s)  LR: 1.000e-04  Data: 0.042 (0.050)\n",
            "Train: 5 [  40/212 ( 19%)]  Loss:  0.188430 (0.2068)  Time: 0.412s,    4.86/s  (0.426s,    4.69/s)  LR: 1.000e-04  Data: 0.045 (0.050)\n",
            "Train: 5 [  50/212 ( 24%)]  Loss:  0.163193 (0.2067)  Time: 0.428s,    4.67/s  (0.426s,    4.69/s)  LR: 1.000e-04  Data: 0.048 (0.050)\n",
            "Train: 5 [  60/212 ( 28%)]  Loss:  0.366334 (0.2144)  Time: 0.435s,    4.60/s  (0.428s,    4.68/s)  LR: 1.000e-04  Data: 0.062 (0.051)\n",
            "Train: 5 [  70/212 ( 33%)]  Loss:  0.215764 (0.2177)  Time: 0.439s,    4.56/s  (0.428s,    4.68/s)  LR: 1.000e-04  Data: 0.060 (0.051)\n",
            "Train: 5 [  80/212 ( 38%)]  Loss:  0.158169 (0.2173)  Time: 0.410s,    4.88/s  (0.428s,    4.67/s)  LR: 1.000e-04  Data: 0.048 (0.052)\n",
            "Train: 5 [  90/212 ( 43%)]  Loss:  0.114747 (0.2168)  Time: 0.414s,    4.84/s  (0.428s,    4.68/s)  LR: 1.000e-04  Data: 0.045 (0.051)\n",
            "Train: 5 [ 100/212 ( 47%)]  Loss:  0.271757 (0.2178)  Time: 0.448s,    4.46/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.069 (0.051)\n",
            "Train: 5 [ 110/212 ( 52%)]  Loss:  0.160533 (0.2146)  Time: 0.421s,    4.76/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.045 (0.051)\n",
            "Train: 5 [ 120/212 ( 57%)]  Loss:  0.279194 (0.2169)  Time: 0.416s,    4.80/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.060 (0.052)\n",
            "Train: 5 [ 130/212 ( 62%)]  Loss:  0.243038 (0.2185)  Time: 0.439s,    4.56/s  (0.428s,    4.68/s)  LR: 1.000e-04  Data: 0.047 (0.052)\n",
            "Train: 5 [ 140/212 ( 66%)]  Loss:  0.336669 (0.2176)  Time: 0.438s,    4.57/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.062 (0.052)\n",
            "Train: 5 [ 150/212 ( 71%)]  Loss:  0.246823 (0.2175)  Time: 0.431s,    4.64/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.052 (0.052)\n",
            "Train: 5 [ 160/212 ( 76%)]  Loss:  0.243242 (0.2189)  Time: 0.424s,    4.72/s  (0.428s,    4.67/s)  LR: 1.000e-04  Data: 0.052 (0.052)\n",
            "Train: 5 [ 170/212 ( 81%)]  Loss:  0.406326 (0.2197)  Time: 0.447s,    4.48/s  (0.428s,    4.67/s)  LR: 1.000e-04  Data: 0.071 (0.052)\n",
            "Train: 5 [ 180/212 ( 85%)]  Loss:  0.168657 (0.2188)  Time: 0.425s,    4.71/s  (0.428s,    4.67/s)  LR: 1.000e-04  Data: 0.046 (0.052)\n",
            "Train: 5 [ 190/212 ( 90%)]  Loss:  0.179595 (0.2185)  Time: 0.441s,    4.54/s  (0.428s,    4.67/s)  LR: 1.000e-04  Data: 0.056 (0.052)\n",
            "Train: 5 [ 200/212 ( 95%)]  Loss:  0.256975 (0.2184)  Time: 0.423s,    4.73/s  (0.428s,    4.67/s)  LR: 1.000e-04  Data: 0.058 (0.052)\n",
            "Train: 5 [ 210/212 (100%)]  Loss:  0.229427 (0.2181)  Time: 0.420s,    4.76/s  (0.428s,    4.67/s)  LR: 1.000e-04  Data: 0.051 (0.052)\n",
            "Train: 5 [ 211/212 (100%)]  Loss:  0.180495 (0.2179)  Time: 0.441s,    4.53/s  (0.429s,    4.67/s)  LR: 1.000e-04  Data: 0.053 (0.052)\n",
            "train OrderedDict([('loss', 0.2178978171058983)])\n",
            "test OrderedDict([('loss', 70.2962582284045), ('Valid mAP', '57%'), ('Valid mAR', '91%')])\n",
            "\n",
            "Train: 6 [   0/212 (  0%)]  Loss:  0.132706 (0.1327)  Time: 0.427s,    4.68/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.047 (0.047)\n",
            "Train: 6 [  10/212 (  5%)]  Loss:  0.137560 (0.1660)  Time: 0.413s,    4.84/s  (0.430s,    4.65/s)  LR: 1.000e-04  Data: 0.049 (0.050)\n",
            "Train: 6 [  20/212 (  9%)]  Loss:  0.282417 (0.1824)  Time: 0.428s,    4.67/s  (0.429s,    4.66/s)  LR: 1.000e-04  Data: 0.051 (0.051)\n",
            "Train: 6 [  30/212 ( 14%)]  Loss:  0.186737 (0.1852)  Time: 0.411s,    4.87/s  (0.429s,    4.66/s)  LR: 1.000e-04  Data: 0.044 (0.051)\n",
            "Train: 6 [  40/212 ( 19%)]  Loss:  0.227989 (0.1879)  Time: 0.442s,    4.53/s  (0.429s,    4.67/s)  LR: 1.000e-04  Data: 0.063 (0.052)\n",
            "Train: 6 [  50/212 ( 24%)]  Loss:  0.093391 (0.1842)  Time: 0.421s,    4.75/s  (0.427s,    4.69/s)  LR: 1.000e-04  Data: 0.043 (0.051)\n",
            "Train: 6 [  60/212 ( 28%)]  Loss:  0.262932 (0.1823)  Time: 0.434s,    4.61/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.067 (0.051)\n",
            "Train: 6 [  70/212 ( 33%)]  Loss:  0.399249 (0.1838)  Time: 0.420s,    4.76/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.047 (0.051)\n",
            "Train: 6 [  80/212 ( 38%)]  Loss:  0.181939 (0.1828)  Time: 0.422s,    4.74/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.047 (0.051)\n",
            "Train: 6 [  90/212 ( 43%)]  Loss:  0.270221 (0.1856)  Time: 0.431s,    4.64/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.055 (0.052)\n",
            "Train: 6 [ 100/212 ( 47%)]  Loss:  0.236835 (0.1872)  Time: 0.434s,    4.61/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.057 (0.052)\n",
            "Train: 6 [ 110/212 ( 52%)]  Loss:  0.122701 (0.1863)  Time: 0.416s,    4.80/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.047 (0.052)\n",
            "Train: 6 [ 120/212 ( 57%)]  Loss:  0.251752 (0.1874)  Time: 0.417s,    4.79/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.052 (0.052)\n",
            "Train: 6 [ 130/212 ( 62%)]  Loss:  0.138845 (0.1894)  Time: 0.413s,    4.84/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.042 (0.052)\n",
            "Train: 6 [ 140/212 ( 66%)]  Loss:  0.147151 (0.1891)  Time: 0.440s,    4.55/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.056 (0.052)\n",
            "Train: 6 [ 150/212 ( 71%)]  Loss:  0.129867 (0.1907)  Time: 0.418s,    4.78/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.051 (0.052)\n",
            "Train: 6 [ 160/212 ( 76%)]  Loss:  0.124720 (0.1905)  Time: 0.421s,    4.75/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.045 (0.052)\n",
            "Train: 6 [ 170/212 ( 81%)]  Loss:  0.115547 (0.1922)  Time: 0.436s,    4.59/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.045 (0.052)\n",
            "Train: 6 [ 180/212 ( 85%)]  Loss:  0.179229 (0.1921)  Time: 0.434s,    4.61/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.052 (0.052)\n",
            "Train: 6 [ 190/212 ( 90%)]  Loss:  0.152396 (0.1920)  Time: 0.408s,    4.90/s  (0.427s,    4.69/s)  LR: 1.000e-04  Data: 0.045 (0.052)\n",
            "Train: 6 [ 200/212 ( 95%)]  Loss:  0.133018 (0.1903)  Time: 0.421s,    4.75/s  (0.426s,    4.69/s)  LR: 1.000e-04  Data: 0.044 (0.052)\n",
            "Train: 6 [ 210/212 (100%)]  Loss:  0.157390 (0.1891)  Time: 0.429s,    4.66/s  (0.426s,    4.70/s)  LR: 1.000e-04  Data: 0.056 (0.051)\n",
            "Train: 6 [ 211/212 (100%)]  Loss:  0.170786 (0.1891)  Time: 0.446s,    4.48/s  (0.426s,    4.70/s)  LR: 1.000e-04  Data: 0.052 (0.051)\n",
            "train OrderedDict([('loss', 0.18906063157713637)])\n",
            "test OrderedDict([('loss', 3.0312967924314123), ('Valid mAP', '57%'), ('Valid mAR', '90%')])\n",
            "\n",
            "Train: 7 [   0/212 (  0%)]  Loss:  0.182282 (0.1823)  Time: 0.433s,    4.62/s  (0.433s,    4.62/s)  LR: 1.000e-04  Data: 0.048 (0.048)\n",
            "Train: 7 [  10/212 (  5%)]  Loss:  0.229800 (0.1942)  Time: 0.433s,    4.62/s  (0.422s,    4.74/s)  LR: 1.000e-04  Data: 0.054 (0.052)\n",
            "Train: 7 [  20/212 (  9%)]  Loss:  0.135718 (0.1859)  Time: 0.415s,    4.82/s  (0.425s,    4.71/s)  LR: 1.000e-04  Data: 0.048 (0.052)\n",
            "Train: 7 [  30/212 ( 14%)]  Loss:  0.111998 (0.1794)  Time: 0.413s,    4.84/s  (0.426s,    4.70/s)  LR: 1.000e-04  Data: 0.046 (0.052)\n",
            "Train: 7 [  40/212 ( 19%)]  Loss:  0.114431 (0.1852)  Time: 0.420s,    4.76/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.044 (0.054)\n",
            "Train: 7 [  50/212 ( 24%)]  Loss:  0.130579 (0.1824)  Time: 0.424s,    4.72/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.048 (0.053)\n",
            "Train: 7 [  60/212 ( 28%)]  Loss:  0.225691 (0.1782)  Time: 0.427s,    4.68/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.051 (0.052)\n",
            "Train: 7 [  70/212 ( 33%)]  Loss:  0.137901 (0.1802)  Time: 0.409s,    4.89/s  (0.427s,    4.68/s)  LR: 1.000e-04  Data: 0.045 (0.052)\n",
            "Train: 7 [  80/212 ( 38%)]  Loss:  0.116499 (0.1751)  Time: 0.426s,    4.69/s  (0.426s,    4.69/s)  LR: 1.000e-04  Data: 0.043 (0.052)\n",
            "Train: 7 [  90/212 ( 43%)]  Loss:  0.098069 (0.1694)  Time: 0.415s,    4.82/s  (0.426s,    4.70/s)  LR: 1.000e-04  Data: 0.043 (0.051)\n",
            "Train: 7 [ 100/212 ( 47%)]  Loss:  0.229443 (0.1683)  Time: 0.420s,    4.77/s  (0.425s,    4.70/s)  LR: 1.000e-04  Data: 0.045 (0.051)\n",
            "Train: 7 [ 110/212 ( 52%)]  Loss:  0.218347 (0.1690)  Time: 0.427s,    4.69/s  (0.425s,    4.70/s)  LR: 1.000e-04  Data: 0.060 (0.051)\n",
            "Train: 7 [ 120/212 ( 57%)]  Loss:  0.196603 (0.1681)  Time: 0.440s,    4.55/s  (0.425s,    4.70/s)  LR: 1.000e-04  Data: 0.064 (0.051)\n",
            "Train: 7 [ 130/212 ( 62%)]  Loss:  0.106022 (0.1672)  Time: 0.423s,    4.72/s  (0.425s,    4.70/s)  LR: 1.000e-04  Data: 0.051 (0.051)\n",
            "Train: 7 [ 140/212 ( 66%)]  Loss:  0.141957 (0.1690)  Time: 0.429s,    4.66/s  (0.425s,    4.70/s)  LR: 1.000e-04  Data: 0.042 (0.051)\n",
            "Train: 7 [ 150/212 ( 71%)]  Loss:  0.131683 (0.1686)  Time: 0.415s,    4.82/s  (0.425s,    4.70/s)  LR: 1.000e-04  Data: 0.050 (0.051)\n",
            "Train: 7 [ 160/212 ( 76%)]  Loss:  0.146181 (0.1686)  Time: 0.411s,    4.87/s  (0.425s,    4.70/s)  LR: 1.000e-04  Data: 0.047 (0.051)\n",
            "Train: 7 [ 170/212 ( 81%)]  Loss:  0.171476 (0.1683)  Time: 0.432s,    4.63/s  (0.425s,    4.70/s)  LR: 1.000e-04  Data: 0.052 (0.051)\n",
            "Train: 7 [ 180/212 ( 85%)]  Loss:  0.103473 (0.1693)  Time: 0.427s,    4.69/s  (0.425s,    4.70/s)  LR: 1.000e-04  Data: 0.044 (0.051)\n",
            "Train: 7 [ 190/212 ( 90%)]  Loss:  0.124462 (0.1694)  Time: 0.409s,    4.89/s  (0.426s,    4.70/s)  LR: 1.000e-04  Data: 0.047 (0.051)\n",
            "Train: 7 [ 200/212 ( 95%)]  Loss:  0.191578 (0.1688)  Time: 0.442s,    4.53/s  (0.426s,    4.70/s)  LR: 1.000e-04  Data: 0.063 (0.051)\n",
            "Train: 7 [ 210/212 (100%)]  Loss:  0.204798 (0.1696)  Time: 0.424s,    4.71/s  (0.426s,    4.70/s)  LR: 1.000e-04  Data: 0.058 (0.051)\n",
            "Train: 7 [ 211/212 (100%)]  Loss:  0.143280 (0.1695)  Time: 0.418s,    4.78/s  (0.426s,    4.70/s)  LR: 1.000e-04  Data: 0.053 (0.051)\n",
            "train OrderedDict([('loss', 0.1694816413142209)])\n",
            "test OrderedDict([('loss', 0.17890453638038903), ('Valid mAP', '57%'), ('Valid mAR', '91%')])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdeB7YZ5SQhR"
      },
      "source": [
        "best_effdet_model = create_effdet_model(\"/content/train/exp14/last.pth.tar\")\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4FNA0KuXdCr"
      },
      "source": [
        "def infer_on_effdet_output(image_file_path, trained_bench):\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  torch_image = F.resize(F.to_tensor(Image.open(image_file_path).convert(\"RGB\")).unsqueeze(0).to(device), [image_size, image_size])\n",
        "  output = trained_bench(torch_image)\n",
        "\n",
        "  results = dict()\n",
        "\n",
        "  results[\"boxes\"] = output[0, :, :4]\n",
        "  results[\"labels\"] = output[0, :, 4]\n",
        "  results[\"scores\"] = output[0, :, -1]\n",
        "\n",
        "  print(results)\n",
        "\n",
        "  # print(results)\n",
        "  # print(torch_image.size())\n",
        "\n",
        "  # torch_image = draw_boxes(results[\"boxes\"], int(results[\"labels\"]), torch_image.squeeze(0).cpu(), infer = False, put_text = True)\n",
        "  # plt.imshow(torch_image)\n",
        "\n",
        "  return results"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceghlSQXdDOD",
        "outputId": "59ec7988-3871-4195-d046-2718ea3301e9"
      },
      "source": [
        "infer_on_effdet_output(\"/content/Fruit Defects Dataset /Predict_Examples/test_image.jpg\", best_effdet_model)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'boxes': tensor([[-2.8757e+04, -6.2381e+05, -2.8757e+04,  4.3569e+05],\n",
            "        [-2.0561e+04,        -inf, -2.0561e+04,         inf],\n",
            "        [-1.6797e+05,        -inf, -1.6797e+05,         inf],\n",
            "        [-2.5736e+04, -9.5589e+04, -2.5736e+04, -8.7420e+03],\n",
            "        [-1.2306e+04, -2.7851e+28, -1.2306e+04,  2.7851e+28]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward>), 'labels': tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SelectBackward>), 'scores': tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SelectBackward>)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': tensor([[-2.8757e+04, -6.2381e+05, -2.8757e+04,  4.3569e+05],\n",
              "         [-2.0561e+04,        -inf, -2.0561e+04,         inf],\n",
              "         [-1.6797e+05,        -inf, -1.6797e+05,         inf],\n",
              "         [-2.5736e+04, -9.5589e+04, -2.5736e+04, -8.7420e+03],\n",
              "         [-1.2306e+04, -2.7851e+28, -1.2306e+04,  2.7851e+28]], device='cuda:0',\n",
              "        grad_fn=<SliceBackward>),\n",
              " 'labels': tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SelectBackward>),\n",
              " 'scores': tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SelectBackward>)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPmuew0CnX92"
      },
      "source": [
        "# The Mobile Net Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbDtIsBs-OWD"
      },
      "source": [
        "backbone = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
        "backbone.roi_heads.box_predictor.cls_score.out_features = 6\n",
        "backbone.roi_heads.box_predictor.bbox_pred.out_features = 24\n",
        "# backbone.roi_heads.box_predictor.cls_score.out_features = 3\n",
        "# backbone.roi_heads.box_predictor.bbox_pred.out_features = 12\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPCVMZBY_GaP",
        "outputId": "92a6ebba-be7e-417d-c799-99baed4270e6"
      },
      "source": [
        "another_one_1 = train(backbone, 5, train_loader, test_loader, noise_loader, 0.001, weight_decay = 1e-4, print_every = 80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Device: cuda\n",
            "Optimizer: SAM (\n",
            "Parameter Group 0\n",
            "    N_sma_threshhold: 5\n",
            "    alpha: 0.5\n",
            "    betas: (0.95, 0.999)\n",
            "    eps: 1e-05\n",
            "    initial_lr: 0.001\n",
            "    k: 6\n",
            "    lr: 0.001\n",
            "    rho: 0.05\n",
            "    step_counter: 0\n",
            "    weight_decay: 0.0001\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5 | Batch Number: 80 | LR: 0.00099 | Train_loss: 106.89 | Test_loss: 62.96 | Test mAP: 43.39% | Missed Test Images: 0 | Seperate Noise Loader: 5 / 100\n",
            "Epoch 1/5 | Batch Number: 160 | LR: 0.00095 | Train_loss: 66.04 | Test_loss: 51.57 | Test mAP: 41.02% | Missed Test Images: 0 | Seperate Noise Loader: 3 / 100\n",
            "\n",
            " Epoch 1 Final Train mAP: 29.55% | Epoch 1 Final Missed Train Images: 1 out of 456 images \n",
            "\n",
            "Epoch 2/5 | Batch Number: 80 | LR: 0.00083 | Train_loss: 54.52 | Test_loss: 58.23 | Test mAP: 45.46% | Missed Test Images: 1 | Seperate Noise Loader: 8 / 100\n",
            "Epoch 2/5 | Batch Number: 160 | LR: 0.00074 | Train_loss: 54.63 | Test_loss: 65.66 | Test mAP: 45.21% | Missed Test Images: 2 | Seperate Noise Loader: 19 / 100\n",
            "\n",
            " Epoch 2 Final Train mAP: 33.25% | Epoch 2 Final Missed Train Images: 4 out of 456 images \n",
            "\n",
            "Epoch 3/5 | Batch Number: 80 | LR: 0.00055 | Train_loss: 59.18 | Test_loss: 57.74 | Test mAP: 29.91% | Missed Test Images: 1 | Seperate Noise Loader: 28 / 100\n",
            "Epoch 3/5 | Batch Number: 160 | LR: 0.00044 | Train_loss: 57.17 | Test_loss: 50.26 | Test mAP: 32.64% | Missed Test Images: 1 | Seperate Noise Loader: 27 / 100\n",
            "\n",
            " Epoch 3 Final Train mAP: 33.48% | Epoch 3 Final Missed Train Images: 7 out of 456 images \n",
            "\n",
            "Epoch 4/5 | Batch Number: 80 | LR: 0.00025 | Train_loss: 49.01 | Test_loss: 85.06 | Test mAP: 52.32% | Missed Test Images: 5 | Seperate Noise Loader: 60 / 100\n",
            "Epoch 4/5 | Batch Number: 160 | LR: 0.00016 | Train_loss: 59.35 | Test_loss: 52.93 | Test mAP: 41.71% | Missed Test Images: 2 | Seperate Noise Loader: 47 / 100\n",
            "\n",
            " Epoch 4 Final Train mAP: 40.42% | Epoch 4 Final Missed Train Images: 19 out of 456 images \n",
            "\n",
            "Epoch 5/5 | Batch Number: 80 | LR: 0.00004 | Train_loss: 56.69 | Test_loss: 76.69 | Test mAP: 46.52% | Missed Test Images: 3 | Seperate Noise Loader: 53 / 100\n",
            "Epoch 5/5 | Batch Number: 160 | LR: 0.00001 | Train_loss: 53.90 | Test_loss: 77.58 | Test mAP: 45.39% | Missed Test Images: 4 | Seperate Noise Loader: 59 / 100\n",
            "\n",
            " Epoch 5 Final Train mAP: 45.58% | Epoch 5 Final Missed Train Images: 20 out of 456 images \n",
            "\n",
            "Time for Total Training 214.29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjzRmGEPmjjF",
        "outputId": "2f32c1c8-573b-4684-b150-bf1c74948b83"
      },
      "source": [
        "another_one = train(backbone, 10, train_loader, test_loader, 0.001, weight_decay = 1e-4, print_every = 80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Device: cuda\n",
            "Optimizer: SAM (\n",
            "Parameter Group 0\n",
            "    N_sma_threshhold: 5\n",
            "    alpha: 0.5\n",
            "    betas: (0.95, 0.999)\n",
            "    eps: 1e-05\n",
            "    initial_lr: 0.001\n",
            "    k: 6\n",
            "    lr: 0.001\n",
            "    rho: 0.05\n",
            "    step_counter: 0\n",
            "    weight_decay: 0.0001\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 | Batch Number: 80 | LR: 0.00100 | Train_loss: 109.40 | Test_loss: 50.10 | Test mAP: 34.56% | Test mAR: nan% | Missed Test Images: 0\n",
            "Epoch 1/10 | Batch Number: 160 | LR: 0.00099 | Train_loss: 60.02 | Test_loss: 55.98 | Test mAP: 37.12% | Test mAR: nan% | Missed Test Images: 0\n",
            "\n",
            " Epoch 1 Final Train mAP: 30.74% | Epoch 1 Final Train mAR: nan% | Epoch 1 Final Missed Train Images: 0 out of 456 images \n",
            "\n",
            "Epoch 2/10 | Batch Number: 80 | LR: 0.00096 | Train_loss: 58.38 | Test_loss: 57.53 | Test mAP: 60.33% | Test mAR: nan% | Missed Test Images: 1\n",
            "Epoch 2/10 | Batch Number: 160 | LR: 0.00093 | Train_loss: 63.52 | Test_loss: 53.23 | Test mAP: 34.23% | Test mAR: nan% | Missed Test Images: 1\n",
            "\n",
            " Epoch 2 Final Train mAP: 31.06% | Epoch 2 Final Train mAR: nan% | Epoch 2 Final Missed Train Images: 6 out of 456 images \n",
            "\n",
            "Epoch 3/10 | Batch Number: 80 | LR: 0.00087 | Train_loss: 59.26 | Test_loss: 83.11 | Test mAP: 47.73% | Test mAR: nan% | Missed Test Images: 3\n",
            "Epoch 3/10 | Batch Number: 160 | LR: 0.00083 | Train_loss: 61.76 | Test_loss: 55.83 | Test mAP: 27.57% | Test mAR: nan% | Missed Test Images: 1\n",
            "\n",
            " Epoch 3 Final Train mAP: 30.96% | Epoch 3 Final Train mAR: nan% | Epoch 3 Final Missed Train Images: 6 out of 456 images \n",
            "\n",
            "Epoch 4/10 | Batch Number: 80 | LR: 0.00075 | Train_loss: 58.68 | Test_loss: 79.41 | Test mAP: 38.63% | Test mAR: nan% | Missed Test Images: 4\n",
            "Epoch 4/10 | Batch Number: 160 | LR: 0.00070 | Train_loss: 61.69 | Test_loss: 66.05 | Test mAP: 38.82% | Test mAR: nan% | Missed Test Images: 3\n",
            "\n",
            " Epoch 4 Final Train mAP: 33.09% | Epoch 4 Final Train mAR: nan% | Epoch 4 Final Missed Train Images: 8 out of 456 images \n",
            "\n",
            "Epoch 5/10 | Batch Number: 80 | LR: 0.00060 | Train_loss: 58.89 | Test_loss: 80.11 | Test mAP: 38.94% | Test mAR: nan% | Missed Test Images: 1\n",
            "Epoch 5/10 | Batch Number: 160 | LR: 0.00055 | Train_loss: 56.70 | Test_loss: 78.22 | Test mAP: 35.02% | Test mAR: nan% | Missed Test Images: 1\n",
            "\n",
            " Epoch 5 Final Train mAP: 34.09% | Epoch 5 Final Train mAR: nan% | Epoch 5 Final Missed Train Images: 7 out of 456 images \n",
            "\n",
            "Epoch 6/10 | Batch Number: 80 | LR: 0.00044 | Train_loss: 52.62 | Test_loss: 86.22 | Test mAP: 43.91% | Test mAR: nan% | Missed Test Images: 2\n",
            "Epoch 6/10 | Batch Number: 160 | LR: 0.00039 | Train_loss: 59.55 | Test_loss: 93.01 | Test mAP: 46.96% | Test mAR: nan% | Missed Test Images: 3\n",
            "\n",
            " Epoch 6 Final Train mAP: 37.65% | Epoch 6 Final Train mAR: nan% | Epoch 6 Final Missed Train Images: 20 out of 456 images \n",
            "\n",
            "Epoch 7/10 | Batch Number: 80 | LR: 0.00029 | Train_loss: 54.05 | Test_loss: 99.96 | Test mAP: 39.45% | Test mAR: nan% | Missed Test Images: 4\n",
            "Epoch 7/10 | Batch Number: 160 | LR: 0.00025 | Train_loss: 58.93 | Test_loss: 79.80 | Test mAP: 34.77% | Test mAR: nan% | Missed Test Images: 1\n",
            "\n",
            " Epoch 7 Final Train mAP: 38.81% | Epoch 7 Final Train mAR: nan% | Epoch 7 Final Missed Train Images: 18 out of 456 images \n",
            "\n",
            "Epoch 8/10 | Batch Number: 80 | LR: 0.00016 | Train_loss: 58.33 | Test_loss: 90.27 | Test mAP: 38.50% | Test mAR: nan% | Missed Test Images: 3\n",
            "Epoch 8/10 | Batch Number: 160 | LR: 0.00012 | Train_loss: 52.78 | Test_loss: 92.37 | Test mAP: 45.57% | Test mAR: nan% | Missed Test Images: 3\n",
            "\n",
            " Epoch 8 Final Train mAP: 43.94% | Epoch 8 Final Train mAR: nan% | Epoch 8 Final Missed Train Images: 24 out of 456 images \n",
            "\n",
            "Epoch 9/10 | Batch Number: 80 | LR: 0.00007 | Train_loss: 52.34 | Test_loss: 99.14 | Test mAP: 48.14% | Test mAR: nan% | Missed Test Images: 4\n",
            "Epoch 9/10 | Batch Number: 160 | LR: 0.00004 | Train_loss: 59.78 | Test_loss: 97.68 | Test mAP: 42.97% | Test mAR: nan% | Missed Test Images: 4\n",
            "\n",
            " Epoch 9 Final Train mAP: 47.44% | Epoch 9 Final Train mAR: nan% | Epoch 9 Final Missed Train Images: 29 out of 456 images \n",
            "\n",
            "Epoch 10/10 | Batch Number: 80 | LR: 0.00001 | Train_loss: 53.61 | Test_loss: 105.94 | Test mAP: 44.46% | Test mAR: nan% | Missed Test Images: 4\n",
            "Epoch 10/10 | Batch Number: 160 | LR: 0.00000 | Train_loss: 53.82 | Test_loss: 105.05 | Test mAP: 45.50% | Test mAR: nan% | Missed Test Images: 4\n",
            "\n",
            " Epoch 10 Final Train mAP: 51.20% | Epoch 10 Final Train mAR: nan% | Epoch 10 Final Missed Train Images: 27 out of 456 images \n",
            "\n",
            "Time for Total Training 381.80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-cknP0_uy0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "916917ab-36c0-4f8a-fd3c-0fcfe2fa1754"
      },
      "source": [
        "# https://github.com/amdegroot/ssd.pytorch/blob/master/layers/box_utils.py#L48\n",
        "def intersect(box_a, box_b):\n",
        "\n",
        "    A = box_a.size(0)\n",
        "    B = box_b.size(0)\n",
        "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
        "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
        "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
        "    return inter[:, :, 0] * inter[:, :, 1]\n",
        "\n",
        "def jaccard_iou(box_a, box_b):\n",
        "\n",
        "    inter = intersect(box_a, box_b)\n",
        "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
        "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
        "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
        "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
        "    union = area_a + area_b - inter\n",
        "    return inter / union  # [A,B]\n",
        "\n",
        "def calculate_iou_on_label(results, len_of_results, iou_thresh, device):\n",
        "  for current_index, _ in enumerate(results[\"boxes\"]):\n",
        "    if current_index >= len_of_results:\n",
        "      break\n",
        "\n",
        "    current_index_iou = jaccard_iou(results[\"boxes\"][current_index].view(1, -1).to(device),\n",
        "                                    results[\"boxes\"].to(device))\n",
        "    \n",
        "    mask = (current_index_iou > iou_thresh) & (current_index_iou != 1)\n",
        "    mask = mask.squeeze()\n",
        "    for key in results:\n",
        "      results[key] = results[key][~mask]\n",
        "\n",
        "    len_of_results -= sum(mask)\n",
        "  \n",
        "  return results\n",
        "\n",
        "def get_labels_categ(classes, want):\n",
        "  fruit_index_list, bad_spot_index_list = list(), list()\n",
        "  for ii, name in enumerate(classes):\n",
        "    if re.search(\"Spot\", name):\n",
        "      bad_spot_index_list.append(ii)\n",
        "    elif re.search(\"Placeholder\", name):\n",
        "      continue\n",
        "    else:\n",
        "      fruit_index_list.append(ii)\n",
        "  \n",
        "  if want == \"fruit\":\n",
        "    return fruit_index_list\n",
        "  elif want == \"bad_spot\":\n",
        "    return bad_spot_index_list\n",
        "  else:\n",
        "    raise ValueError(\"want Type not applicable [fruit or bad_spot only]\")\n",
        "\n",
        "print(classes)\n",
        "get_labels_categ(classes, \"bad_spot\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Placeholder', 'Apples', 'Strawberry', 'Tomato', 'Apple_Bad_Spot', 'Strawberry_Bad_Spot', 'Tomato_Bad_Spot']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 5, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFhjlggXlAx7"
      },
      "source": [
        "def infer_image(image_file_path, trained_model, distance_thresh, iou_thresh, webcam = False, show_image = True):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  #Just load it up as PIL. Avoid using cv2 because do not need albumentations\n",
        "  if not webcam:\n",
        "    torch_image = F.to_tensor(Image.open(image_file_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    trained_model.to(device)\n",
        "    trained_model.eval()\n",
        "    print(\"Image Size: {}\".format(torch_image.size()))\n",
        "\n",
        "    start_time = time.time()\n",
        "    results = trained_model(torch_image)\n",
        "    end_time = time.time() - start_time\n",
        "\n",
        "    print(\"Time of Inference {:0.2f}\".format(end_time))\n",
        "  else:\n",
        "\n",
        "    torch_image = F.to_tensor(image_file_path).unsqueeze(1).to(device)\n",
        "\n",
        "    results = trained_model(torch_image)\n",
        "\n",
        "  valid_box_count = 0\n",
        "  for ii, score in enumerate(results[0][\"scores\"]):\n",
        "    if score < distance_thresh:\n",
        "      low_index_start = ii\n",
        "      break\n",
        "    else:\n",
        "      valid_box_count += 1\n",
        "\n",
        "  if valid_box_count == len(results[0][\"scores\"]):\n",
        "    low_index_start = len(results[0][\"scores\"])\n",
        "  \n",
        "  for key in results[0]:\n",
        "    results[0][key] = results[0][key][:low_index_start]\n",
        "  \n",
        "  #This is where I place the order of the list\n",
        "  fruit_spot_iou_thresh, bad_spot_iou_thresh = iou_thresh\n",
        "\n",
        "  #Update when I get more data of fruits and when running for script beware of classes.\n",
        "  bad_spot_index = [ii for ii, label in enumerate(results[0][\"labels\"]) if label in get_labels_categ(classes, \"bad_spot\")]\n",
        "  fruit_index = [ii for ii, _ in enumerate(results[0][\"labels\"]) if ii not in bad_spot_index]\n",
        "\n",
        "  bad_spot_results, fruit_results = dict(), dict()\n",
        "\n",
        "  for key in results[0]:\n",
        "    bad_spot_results[key], fruit_results[key] = results[0][key][[bad_spot_index]], results[0][key][[fruit_index]]\n",
        "\n",
        "  assert len(bad_spot_results[\"boxes\"]) == len(bad_spot_results[\"scores\"]) == len(bad_spot_results[\"labels\"])\n",
        "  assert len(fruit_results[\"boxes\"]) == len(fruit_results[\"scores\"]) == len(fruit_results[\"labels\"])\n",
        "\n",
        "  len_of_bad_spots, len_of_fruit = len(bad_spot_results[\"boxes\"]), len(fruit_results[\"boxes\"])\n",
        "\n",
        "  if len_of_bad_spots > 1:\n",
        "    bad_spot_results = calculate_iou_on_label(bad_spot_results, len_of_bad_spots, bad_spot_iou_thresh, device)\n",
        "  if len_of_fruit > 1:\n",
        "    fruit_results = calculate_iou_on_label(fruit_results, len_of_fruit, fruit_spot_iou_thresh, device)\n",
        "  \n",
        "  for key in results[0]: \n",
        "    if (key == \"boxes\"):\n",
        "      results[0][\"boxes\"] = torch.cat((fruit_results[\"boxes\"], bad_spot_results[\"boxes\"]), axis = 0)\n",
        "    else:\n",
        "      results[0][key] = torch.cat((fruit_results[key], bad_spot_results[key]), dim = 0)\n",
        "\n",
        "  if show_image:\n",
        "    if device == torch.device(\"cuda\"):\n",
        "      torch_image = torch_image.cpu() \n",
        "    written_image = cv2.cvtColor(draw_boxes(results[0][\"boxes\"], results[0][\"labels\"], torch_image.squeeze(), infer = True, put_text= True), cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(written_image)\n",
        "  \n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ab85jWlu5L7"
      },
      "source": [
        "results = infer_image(\"/content/tomatpred.jpg\", another_one, 0.2, [0.3, 0.1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZd4nVDuiu4M"
      },
      "source": [
        "import base64\n",
        "import html\n",
        "import io\n",
        "import time\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "def start_input():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      /* try changing the capture canvas and see what happens*/\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def take_photo(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T45EScTjJ5e"
      },
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def js_reply_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          js_reply: JavaScript object, contain image from webcam\n",
        "\n",
        "    output: \n",
        "          image_array: image array RGB size 512 x 512 from webcam\n",
        "    \"\"\"\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\n",
        "    image_array = np.array(image_PIL)\n",
        "\n",
        "    return image_array\n",
        "\n",
        "def drawing_array_to_bytes(drawing_array):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          drawing_array: image RGBA size 512 x 512 \n",
        "                              contain bounding box and text from yolo prediction, \n",
        "                              channel A value = 255 if the pixel contains drawing properties (lines, text) \n",
        "                              else channel A value = 0\n",
        "\n",
        "    output: \n",
        "          drawing_bytes: string, encoded from drawing_array\n",
        "    \"\"\"\n",
        "\n",
        "    drawing_PIL = Image.fromarray(drawing_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    drawing_PIL.save(iobuf, format='png')\n",
        "    drawing_bytes = 'data:image/png;base64,{}'.format((str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "    return drawing_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FSrjfHjgX4q"
      },
      "source": [
        "data_transforms = get_transforms(mode = \"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "j0WidBmGlktM",
        "outputId": "6e291470-abb3-45b9-8be7-c90bf01e15e3"
      },
      "source": [
        "start_input()\n",
        "label_html = 'Capturing...'\n",
        "img_data = ''\n",
        "count = 0 \n",
        "\n",
        "color=None\n",
        "label=None\n",
        "line_thickness=None\n",
        "another_one.to(device).eval();\n",
        "while True:\n",
        "    js_reply = take_photo(label_html, img_data)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    image = js_reply_to_image(js_reply)\n",
        "    prediciton = infer_image(image, another_one, 0.03, [0.3, 0.1], webcam= True, show_image = False)\n",
        "\n",
        "    drawing_array = np.zeros([512,512,4], dtype=np.uint8)\n",
        "\n",
        "    for x in prediciton[0]['boxes']:\n",
        "\n",
        "      tl = line_thickness or round(0.002 * (drawing_array.shape[0] + drawing_array.shape[1]) / 2) + 1  # line/font thickness\n",
        "      color = color or [random.randint(0, 255) for _ in range(3)]\n",
        "      c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
        "      cv2.rectangle(drawing_array, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
        "      if label:\n",
        "        tf = max(tl - 1, 1)  # font thickness\n",
        "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
        "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
        "        cv2.rectangle(drawing_array, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
        "        cv2.putText(drawing_array, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "    drawing_array[:,:,3] = (drawing_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "\n",
        "    drawing_PIL = Image.fromarray(drawing_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    drawing_PIL.save(iobuf, format='png')\n",
        "    drawing_bytes = 'data:image/png;base64,{}'.format((str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "    img_data = drawing_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      /* try changing the capture canvas and see what happens*/\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 512; //video.videoWidth;\n",
              "      captureCanvas.height = 512; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function takePhoto(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}