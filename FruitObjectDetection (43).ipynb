{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FruitObjectDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZfnWC_NF7NM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch \n",
        "from torch import nn, optim \n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import itertools\n",
        "import glob \n",
        "from PIL import Image\n",
        "import csv \n",
        "import cv2\n",
        "import re\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.ops.boxes import box_iou\n",
        "import random\n",
        "import torchvision\n",
        "import warnings\n",
        "\n",
        "!pip install --upgrade albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2, ToTensor\n",
        "!git clone https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
        "%cd Ranger-Deep-Learning-Optimizer\n",
        "!pip install -e .\n",
        "from ranger import Ranger  \n",
        "%cd ..\n",
        "#https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1\n",
        "!git clone https://github.com/davda54/sam.git\n",
        "%cd sam\n",
        "import sam\n",
        "print(\"Imported SAM Successfully from github .py file\")\n",
        "%cd ..\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_pckeYPGF0b",
        "outputId": "3d4288a6-2380-40e1-d143-a75c99b13634"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCS2doQYbNpf"
      },
      "source": [
        "## To Do Right Now: \n",
        "\n",
        "# Coding/ Technical Stuff to add to paper\n",
        "### Fix the effecient predicting in validation loop for mAP. \n",
        "\n",
        "```\n",
        "valid_metrics = train_model(model) \n",
        "def train_model(...):\n",
        "  return valid_metrics\n",
        "checkpoint = last.tar.gz\n",
        "bench = create_model(last.tar.gz)\n",
        "mAP metircs = validate(bench, model)\n",
        "def validate(...):\n",
        "  out = bench(image)\n",
        "  mAP_calc(pred, out)\n",
        "\n",
        "  #currently doing with valid/train func\n",
        "  loss = model(images, target2)\n",
        "\n",
        "  return mAP_metrics, loss\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "### XYXY and YXYX issue\n",
        "```\n",
        "out = bench(image)\n",
        "#out form = [x, y, x, y]\n",
        "\n",
        "mAP_testable_boxes = [x, y, x, y]\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### Fix effecient det infer problem and also try to optimize the model for better accuracies new variants of model. Find through this link.\n",
        "\n",
        "### Once finished with effecient det can create ensemble model that only output boxes that are both predicted or nearly predicted by both models. \n",
        "https://github.com/rwightman/efficientdet-pytorch\n",
        "https://github.com/rwightman/efficientdet-pytorch/blob/abba1d5a3611471ac88d49a473f993f72f9e1aba/effdet/config/model_config.py\n",
        "\n",
        "### Find links to put saved model and weights into rasberry pi model\n",
        "\n",
        "# Paper\n",
        "\n",
        "### Work on creating a slide presentation to keep the paper contents.\n",
        "\n",
        "### Move to .py script\n",
        "\n",
        "### Labeling the other data. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4UKDVNnIqQN"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/LatestFruit Defects Dataset .zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/noisy_dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY83cUEFAjoW",
        "outputId": "bc2007a9-501c-4426-fa4d-9d2dcac05161"
      },
      "source": [
        "#For one strawberry batch please drop watermark rows\n",
        "strawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 3 Labeled/FreshStrawberryBatch3Labels.csv\", header = None)\n",
        "strawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 2 Labeled/FreshStrawberriesBatch2Labels.csv\", header = None)\n",
        "strawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 1 Labeled/Strawberrybatch1.csv\", header = None)\n",
        "rottenApple_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch1Labeled/RottenAppleBatch1Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch2Labeled/RottenApplesBatch2Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch3Labaled/RottenApplesBatch3Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch1RottenStrawBerryLabels/RottenStrawberriesBatch1Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch2RottenStrawBerryLabels/RottenStrawBerryBatch2.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch3RottenStrawberrylabel/rottenStrawberryBtch3labels.csv\", header = None)\n",
        "freshApples_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplebtch2label/FreshApplesBatch2LabelsFresh.csv\", header = None)\n",
        "freshApples_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplesBatch1Labels/FreshAppleBatch1Labels.csv\", header = None)\n",
        "rottenTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/Rotten TomatoBatch1/Batch1TomoatosLabelsBbox.csv\", header = None)\n",
        "rottenTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottnTomatoBatch2/RottenTomatyoBatch2Labelss.csv\", header = None)\n",
        "rottenTomato_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatBtch3/RottenTomatoesBatch3Labssles.csv\", header = None)\n",
        "rottenTomato_csv_batch_4 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatoesBatch4/Tomatobatch4labelssRotten.csv\", header = None)\n",
        "freshTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatoesBatch1Labelss/FreshTomatoesLabelsBatch1Labels.csv\", header = None)\n",
        "freshTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatBatch2Labessls/Batch2TomatlabelsFresh.csv\", header = None)\n",
        "\n",
        "strawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_3.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_4.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "\n",
        "#Drop some watermark data for Fresh StrawBerry Batch 1 Labeled images [59, 9, 93]\n",
        "\n",
        "# strawberry_csv_batch_1 = strawberry_csv_batch_1[Image_id not in [\"FreshStrawberries59.jpeg, FreshStrawberries9.jpeg, FreshStrawberries93.jpeg\"]]\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries59.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries9.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries93.jpeg\"].index, inplace = True)\n",
        "freshTomato_csv_batch_1.drop(freshTomato_csv_batch_1[freshTomato_csv_batch_1[\"Image_id\"] == \"Fresh Tomatoes66AddonPart1.jpeg\"].index, inplace = True)\n",
        "\n",
        "strawberry_csv_batch_1 = strawberry_csv_batch_1.reset_index(drop=True)\n",
        "freshTomato_csv_batch_1 = freshTomato_csv_batch_1.reset_index(drop = True)\n",
        "\n",
        "#Stack all the csv files together. \n",
        "list_of_all_dataframes = [strawberry_csv_batch_1, strawberry_csv_batch_2, strawberry_csv_batch_3, rottenApple_csv_batch_1, \n",
        "                          rottenApple_csv_batch_2, rottenApple_csv_batch_3, rottenStrawberry_csv_batch_1, rottenStrawberry_csv_batch_2, \n",
        "                          rottenStrawberry_csv_batch_3, freshApples_csv_batch_2, freshApples_csv_batch_1, rottenTomato_csv_batch_1, \n",
        "                          rottenTomato_csv_batch_2, rottenTomato_csv_batch_3, rottenTomato_csv_batch_4, freshTomato_csv_batch_1, \n",
        "                          freshTomato_csv_batch_2]\n",
        "fruit_df = pd.concat(list_of_all_dataframes, ignore_index = True)\n",
        "\n",
        "total_row_sum_check = 0 \n",
        "for dataframe in list_of_all_dataframes:\n",
        "  total_row_sum_check += dataframe.shape[0]\n",
        "print(\"Checked total rows from all the dataframes combined: {}\".format(total_row_sum_check))\n",
        "\n",
        "def run_dataframe_check():\n",
        "  assert total_row_sum_check == fruit_df.shape[0]\n",
        "  print(\"DataFrame shape: {}\".format(fruit_df.shape))\n",
        "  print(\"Unique Fruit Labels {}\".format(fruit_df[\"Fruit\"].unique()))\n",
        "  print(\"Number of Unique Images {}\".format(len(fruit_df[\"Image_id\"].unique())))\n",
        "\n",
        "run_dataframe_check()\n",
        "\n",
        "#Specify more image types when \n",
        "def more_specific_Image_id(image_id, fruit):\n",
        "  if fruit == \"Bad_Spots\":\n",
        "    if re.search(\"RottenStrawberries\", image_id):\n",
        "      return \"Strawberry_Bad_Spot\"\n",
        "    elif re.search(\"RottenApples\", image_id):\n",
        "      return \"Apple_Bad_Spot\"\n",
        "    elif re.search(\"Rotten Tomatoes\", image_id):\n",
        "      return \"Tomato_Bad_Spot\"\n",
        "    else:\n",
        "      raise ValueError(\"Could not find a match for some of the Image_ids\")\n",
        "\n",
        "  else:\n",
        "    return fruit\n",
        "\n",
        "fruit_df[\"Fruit\"] = fruit_df.apply(lambda row: more_specific_Image_id(row.Image_id, row.Fruit), axis = 1)\n",
        "\n",
        "run_dataframe_check()\n",
        "\n",
        "#Post Processing \n",
        "fruit_df = fruit_df[fruit_df[\"Image_id\"] != \"FreshStrawberries15.jpeg\"]\n",
        "\n",
        "bounding_box_dict = dict()\n",
        "labels_dict = dict()\n",
        "classes = [\"Placeholder\", \"Apples\", \"Strawberry\", \"Tomato\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\", \"Tomato_Bad_Spot\"]\n",
        "# classes = [\"Apples\", \"Strawberry\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\"]\n",
        "print(classes)\n",
        "# classes = [\"Bad_Spots\", \"Strawberry\", \"Apples\"]\n",
        "\n",
        "for row_index in range(len(fruit_df)): \n",
        "  current_image_file = fruit_df.iloc[row_index][\"Image_id\"]\n",
        "  if current_image_file not in bounding_box_dict:\n",
        "    bounding_box_dict[current_image_file] = list()\n",
        "    labels_dict[current_image_file] = list()\n",
        "  bounding_box_dict[current_image_file].append(fruit_df.iloc[row_index, 1:5].to_list())\n",
        "  labels_dict[current_image_file].append(classes.index(fruit_df.iloc[row_index, 0]))\n",
        "\n",
        "print(len(bounding_box_dict))\n",
        "print(len(labels_dict))\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checked total rows from all the dataframes combined: 1882\n",
            "DataFrame shape: (1882, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Bad_Spots' 'Tomato']\n",
            "Number of Unique Images 532\n",
            "DataFrame shape: (1882, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Apple_Bad_Spot' 'Strawberry_Bad_Spot' 'Tomato'\n",
            " 'Tomato_Bad_Spot']\n",
            "Number of Unique Images 532\n",
            "['Placeholder', 'Apples', 'Strawberry', 'Tomato', 'Apple_Bad_Spot', 'Strawberry_Bad_Spot', 'Tomato_Bad_Spot']\n",
            "531\n",
            "531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15qhrCwGUPxp"
      },
      "source": [
        "## Class function + util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgVTPiupUTQa"
      },
      "source": [
        "def ffile_path(image_id, full_image_file_paths):\n",
        "  for image_path in full_image_file_paths:\n",
        "    if image_id in image_path:\n",
        "      return image_path\n",
        "\n",
        "def find_area_bb(bb_coord):\n",
        "  bb_coord = bb_coord.numpy()\n",
        "  area_of_each_bb = list()\n",
        "  for pair_of_coord in bb_coord:\n",
        "    area_of_each_bb.append(\n",
        "        (pair_of_coord[2] - pair_of_coord[0]) * (pair_of_coord[3] - pair_of_coord[1])\n",
        "    )\n",
        "  return torch.tensor(area_of_each_bb, dtype=torch.int32)\n",
        "\n",
        "def convert_min_max(bb_coord):\n",
        "  for pair_of_coord in bb_coord:\n",
        "    pair_of_coord[2], pair_of_coord[3] = (pair_of_coord[0] + pair_of_coord[-2]), (pair_of_coord[1] + pair_of_coord[-1])\n",
        "  return bb_coord\n",
        "\n",
        "class FruitDetectDataset(object):\n",
        "  def __init__(self, id_labels, id_bounding_boxes, transforms, mode, noisy_dataset_path = None):\n",
        "\n",
        "    assert len(id_labels) == len(id_bounding_boxes)\n",
        "    assert sorted(id_labels.keys()) == sorted(id_bounding_boxes.keys())\n",
        "    self.imgs_key = sorted(id_labels.keys())\n",
        "\n",
        "    if noisy_dataset_path:\n",
        "      self.noisy_fp = [fp for fp in glob.glob(os.path.join(noisy_dataset_path, \"*.JPEG\"))]\n",
        "      \n",
        "      print(\"Noisy Has been subsetted\")\n",
        "      #Go to this code if you want to subset.\n",
        "      self.noisy_fp = self.noisy_fp[:40]\n",
        "      \n",
        "    else:\n",
        "      print(\"Dataset getting configured without noise loader\")\n",
        "      self.noisy_fp = list()\n",
        "\n",
        "    # np.random.shuffle(self.imgs_key)\n",
        "    if (mode == \"train\"):\n",
        "      self.imgs_key = self.imgs_key[:int(len(self.imgs_key) * 0.8)]\n",
        "      if noisy_dataset_path:\n",
        "        print(\"Extended {} noisy images to train set\".format(int(len(self.noisy_fp) * 0.8)))\n",
        "        self.imgs_key.extend(self.noisy_fp[:int(len(self.noisy_fp) * 0.8)])\n",
        "    elif (mode == \"test\"):\n",
        "      self.imgs_key = self.imgs_key[int(len(self.imgs_key) * 0.8):]\n",
        "      if noisy_dataset_path:\n",
        "        print(\"Extended {} noisy images to test set\".format(int(len(self.noisy_fp) * 0.2)))\n",
        "        self.imgs_key.extend(self.noisy_fp[int(len(self.noisy_fp) * 0.8):])\n",
        "    else:\n",
        "      raise ValueError(\"Invalid Mode choose from train or test\")\n",
        "\n",
        "    self.id_labels = id_labels\n",
        "    self.id_bounding_boxes = id_bounding_boxes\n",
        "    self.full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_key = self.imgs_key[idx]\n",
        "    if img_key in self.noisy_fp:\n",
        "      img_path = img_key\n",
        "      boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "      labels = torch.as_tensor([], dtype = torch.int64)\n",
        "    else:\n",
        "      img_path = ffile_path(self.imgs_key[idx], self.full_image_file_paths) \n",
        "      boxes = convert_min_max(torch.as_tensor(self.id_bounding_boxes[self.imgs_key[idx]], dtype=torch.float32))\n",
        "      labels = torch.as_tensor(self.id_labels[self.imgs_key[idx]], dtype=torch.int64)\n",
        "    \n",
        "    img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = find_area_bb(boxes)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    \n",
        "    #Query about transforms for labels of images\n",
        "    if self.transforms: \n",
        "      sample = {\n",
        "                'image': img,\n",
        "                'bboxes': target['boxes'],\n",
        "                'labels': labels\n",
        "            }\n",
        "\n",
        "      sample = self.transforms(**sample)\n",
        "      img = sample['image']\n",
        "\n",
        "      if img_key not in self.noisy_fp:\n",
        "        target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "    \n",
        "    \n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs_key)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDh2_pg4J2yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7b48d5-e085-49f8-8213-cd0e70810703"
      },
      "source": [
        "\n",
        "#The Drawing function.\n",
        "# COLORS = [(255, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0)]\n",
        "COLORS = [(0, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0), (255, 0, 0)]\n",
        "\n",
        "def draw_boxes(boxes, labels, image, infer = False, put_text = True):\n",
        "    # read the image with OpenCV\n",
        "    image = image.permute(1, 2, 0).numpy()\n",
        "    if infer:\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    for i, box in enumerate(boxes):\n",
        "        color = COLORS[labels[i] % len(COLORS)]\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (int(box[0]), int(box[1])),\n",
        "            (int(box[2]), int(box[3])),\n",
        "            color, 2\n",
        "        )\n",
        "        if put_text:\n",
        "          cv2.putText(image, classes[labels[i]], (int(box[0]), int(box[1]-5)),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n",
        "                      lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "# Albumentations\n",
        "def get_transforms(mode):\n",
        "  if (mode == \"train\"):\n",
        "    return A.Compose([\n",
        "                      A.OneOf([\n",
        "                      A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                     val_shift_limit=0.2, p=0.9),\n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "                      A.HorizontalFlip(),\n",
        "                      A.VerticalFlip(), \n",
        "                      # A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), p=1),\n",
        "                      # ToTensor(),\n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"test\"):\n",
        "    return A.Compose([\n",
        "                      # A.Resize(512, 512), \n",
        "                      # A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                      # std=(0.229, 0.224, 0.225), p=1),\n",
        "                      # ToTensor()\n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"effdet_train\"):\n",
        "    return A.Compose([\n",
        "                      A.OneOf([\n",
        "                      A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                     val_shift_limit=0.2, p=0.9),\n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "                      A.HorizontalFlip(),\n",
        "                      A.VerticalFlip(), \n",
        "                      A.Resize(height = 512, width=512), \n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"effdet_test\"):\n",
        "    return A.Compose([\n",
        "                      A.Resize(height = 512, width = 512), \n",
        "                      ToTensorV2()])\n",
        "  else:\n",
        "    raise ValueError(\"mode is wrong value can either be train or test\")\n",
        "\n",
        "class NoiseDataset(object):\n",
        "\n",
        "  def __init__(self, noise_file_path, size, camera_size):\n",
        "\n",
        "    self.size = size\n",
        "    self.noise_file_path = [fp for fp in glob.glob(os.path.join(noise_file_path, \"*.JPEG\"))]\n",
        "    self.transforms = transforms.Compose([\n",
        "                                          transforms.Resize((camera_size, camera_size)), \n",
        "                                          transforms.ToTensor()])\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    current_file_path = self.noise_file_path[idx]\n",
        "    img = Image.open(current_file_path).convert(\"RGB\")\n",
        "\n",
        "    img = self.transforms(img)\n",
        "    return img\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.size:\n",
        "      return self.size\n",
        "    return len(self.noise_file_path)\n",
        "    \n",
        "\n",
        "#Using this stack overflow (https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders)\n",
        "#(Suppose for example, you want to create batches of a list of varying dimension tensors. The below code pads sequences with 0 until the maximum sequence size of the batch,)\n",
        "#Collate_fn is a function that is used to process your batches before you pass it to dataloader. In my case since I have different sized images I need a way to stack batches b/c torch.stack won't work.\n",
        "#So I use zip which can accept tensors of different lengths and make them stacked with the size of the lowest length list given. Therefore stacking all the images in a batch \n",
        "#Successfully unlike torch.stack and doing that processing to every batch makes collate_fn vital since I have different image sizes.\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return tuple([list(a) for a in zip(*batch)])\n",
        "    # return tuple(zip(*batch))\n",
        "\n",
        "train_batch_size = 2\n",
        "test_batch_size = 2\n",
        "noise_path = \"/content/noisy_dataset\"\n",
        "\n",
        "train_dataset = FruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"train\"), mode = \"train\", noisy_dataset_path=noise_path)                               \n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= collate_fn)\n",
        "\n",
        "test_dataset = FruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"test\"), mode = \"test\", noisy_dataset_path=noise_path)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = test_batch_size, shuffle = True, collate_fn= collate_fn)\n",
        "\n",
        "noise_dataset = NoiseDataset(noise_path, 100, 512)\n",
        "noise_loader = torch.utils.data.DataLoader(noise_dataset, batch_size = test_batch_size, shuffle = True)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Noisy Has been subsetted\n",
            "Extended 32 noisy images to train set\n",
            "Noisy Has been subsetted\n",
            "Extended 8 noisy images to test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "-uf9fdpPK-SQ",
        "outputId": "23370503-5771-40ec-920c-317cabc5040b"
      },
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(2):\n",
        "    ax = fig.add_subplot(2, 2/2, idx+1, xticks=[], yticks=[])\n",
        "    image = draw_boxes(labels[idx][\"boxes\"], labels[idx][\"labels\"], images[idx])\n",
        "    plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A0EkOWcPSX0"
      },
      "source": [
        "## What we learned\n",
        "is that the model outputs losses when in train mode \n",
        "when in model.eval model, the model code then return only a prediction with no losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NQrBD22VWxD"
      },
      "source": [
        "def calculate_metrics(target_box,predictions_box,scores, device):\n",
        "\n",
        "    #Get most confident boxes first and least confident last\n",
        "    predictions_box = predictions_box[scores.argsort().flip(-1)]\n",
        "    iou_mat = box_iou(target_box,predictions_box)\n",
        "    #return a one by one matrix that is form (target_box, prediction_box) or (1, 1)\n",
        "    target_boxes_count, prediction_boxes_count = iou_mat.shape\n",
        "    \n",
        "    mAP_Matrix = torch.zeros_like(iou_mat)\n",
        "    # if not matrix coordinates that relate to nothing.\n",
        "    if not iou_mat[:,0].eq(0.).all():\n",
        "      index_of_biggest_iou = iou_mat[:,0].argsort()[-1]\n",
        "      mAP_Matrix[index_of_biggest_iou,0] = 1\n",
        "\n",
        "    for pr_idx in range(1,prediction_boxes_count):\n",
        "        not_assigned = torch.logical_not(mAP_Matrix[:,:pr_idx].sum(1)).long()\n",
        "        targets = not_assigned * iou_mat[:,pr_idx]\n",
        "\n",
        "        if targets.eq(0).all():\n",
        "            continue\n",
        "\n",
        "        pivot = targets.argsort()[-1]\n",
        "        mAP_Matrix[pivot,pr_idx] = 1\n",
        "\n",
        "    # mAP calculation\n",
        "    tp = mAP_Matrix.sum()\n",
        "    fp = mAP_Matrix.sum(0).eq(0).sum()\n",
        "    fn = mAP_Matrix.sum(1).eq(0).sum()\n",
        "\n",
        "    mAP = tp / (tp+fp)\n",
        "    mAR = tp / (tp+fn)\n",
        "\n",
        "    return mAP, mAR\n",
        "\n",
        "def run_metrics_for_batch(output, targets, mAP, mAR, missed_images, device):\n",
        "  for pos_in_batch, image_pred in enumerate(output):\n",
        "    assert (len(image_pred[\"boxes\"]) == len(image_pred[\"labels\"]) == len(image_pred[\"scores\"]))\n",
        "    if len(image_pred[\"boxes\"]) != 0:\n",
        "      curr_mAP, curr_mAR = calculate_metrics(targets[pos_in_batch][\"boxes\"], output[pos_in_batch][\"boxes\"], output[pos_in_batch][\"scores\"], device)\n",
        "      mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
        "    else:\n",
        "      missed_images += 1 \n",
        "  \n",
        "  return mAP, mAR, missed_images\n",
        "\n",
        "# def run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device):\n",
        "#     assert (len(scores) == len(classification) == len(transformed_anchors))\n",
        "#     if len(transformed_anchors) != 0:\n",
        "#       curr_mAP, curr_mAR = calculate_metrics(targets[0][:, :4], transformed_anchors, scores, device)\n",
        "#       mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
        "#     else:\n",
        "#       missed_images += 1 \n",
        "      \n",
        "#     return mAP, mAR, missed_images\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB_w3zeIOa8X"
      },
      "source": [
        "def train(net, epochs, train_loader, test_loader, noise_loader, lr, weight_decay, \n",
        "          print_every = 6, lo_test_dataset = len(test_dataset), lo_train_dataset = len(train_dataset),\n",
        "          lo_noise_dataset = len(noise_dataset)):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Check which parameters can calculate gradients. \n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "\n",
        "    # optimizer = Ranger(net.parameters(), lr = lr, weight_decay= weight_decay)\n",
        "    base_optimizer = Ranger\n",
        "    optimizer = sam.SAM(net.parameters(), base_optimizer, lr = lr, weight_decay = weight_decay)\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
        "\n",
        "    net.to(device)\n",
        "    print(\"Device: {}\".format(device))\n",
        "    print(\"Optimizer: {}\".format(optimizer))\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        \n",
        "        train_loss = steps = train_mAP = train_mAR = missed_train_images = 0\n",
        "        \n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "            net.train()\n",
        "\n",
        "            steps += 1\n",
        "            \n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            net.eval()\n",
        "            train_mAP, train_mAR, missed_train_images = run_metrics_for_batch(net(images), targets, train_mAP, train_mAR, missed_train_images, device)\n",
        "            net.train()\n",
        "\n",
        "            losses.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            optimizer.first_step(zero_grad = True)\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            losses.backward()\n",
        "            optimizer.second_step(zero_grad = True)\n",
        "\n",
        "            train_loss +=  losses.item()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (steps % print_every) == 0:\n",
        "\n",
        "              with torch.no_grad():\n",
        "                test_mAP = test_mAR = missed_test_images = test_loss = correct_missed_images = 0\n",
        "\n",
        "                for noise_images in noise_loader:\n",
        "                  \n",
        "                  net.eval()\n",
        "                  if device == torch.device(\"cuda\"):\n",
        "                    noise_images = [noise_image.to(device) for noise_image in noise_images]\n",
        "                  \n",
        "                  output = net(noise_images)\n",
        "\n",
        "                  for ii in range(len(output)):\n",
        "                    if len(output[ii][\"boxes\"]) == 0:\n",
        "                      correct_missed_images += 1\n",
        "\n",
        "                for images, targets in test_loader:\n",
        "\n",
        "                  net.eval()\n",
        "                  if device == torch.device(\"cuda\"):\n",
        "                    images = [image.to(device) for image in images]\n",
        "                    targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "                  output = net(images)\n",
        "                  test_mAP, test_mAR, missed_test_images = run_metrics_for_batch(output, targets, test_mAP, test_mAR, missed_test_images, device)\n",
        "\n",
        "                  net.train()\n",
        "                  test_loss_dict = net(images, targets)\n",
        "                  test_losses = sum(loss for loss in test_loss_dict.values())\n",
        "                  test_loss += test_losses.item()\n",
        "\n",
        "                for param_group in optimizer.param_groups:\n",
        "                  learning_rate_extract = param_group[\"lr\"]\n",
        "                print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Test_loss: {:0.2f} | Test mAP: {:0.2f}% | Missed Test Images: {} | Seperate Noise Loader: {} / {}\".format(\n",
        "                    epoch + 1, epochs, steps, learning_rate_extract, train_loss, test_loss,  \n",
        "                    (test_mAP / float(lo_test_dataset)) * 100.,missed_test_images,\n",
        "                    correct_missed_images, lo_noise_dataset))\n",
        "\n",
        "              assert (steps % print_every) == 0\n",
        "              train_loss = 0\n",
        "                 \n",
        "        print(\"\\n Epoch {} Final Train mAP: {:0.2f}% | Epoch {} Final Missed Train Images: {} out of {} images \\n\".format(\n",
        "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, missed_train_images, lo_train_dataset\n",
        "        ))\n",
        "    \n",
        "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))\n",
        "\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYAu4FDknTwH"
      },
      "source": [
        "# Effecient Det Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWHkD1yEt1LY"
      },
      "source": [
        "!pip install timm\n",
        "!pip install effdet\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, unwrap_bench\n",
        "from effdet.efficientdet import HeadNet\n",
        "from effdet.data.transforms import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EHpxq96t-Ii"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "MAX_NUM_INSTANCES = 100\n",
        "class DetectionFastCollate:\n",
        "    \"\"\" A detection specific, optimized collate function w/ a bit of state.\n",
        "    Optionally performs anchor labelling. Doing this here offloads some work from the\n",
        "    GPU and the main training process thread and increases the load on the dataloader\n",
        "    threads.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            instance_keys=None,\n",
        "            instance_shapes=None,\n",
        "            instance_fill=-1,\n",
        "            max_instances=MAX_NUM_INSTANCES,\n",
        "            anchor_labeler=None,\n",
        "    ):\n",
        "        instance_keys = instance_keys or {'bbox', 'bbox_ignore', 'cls'}\n",
        "        instance_shapes = instance_shapes or dict(\n",
        "            bbox=(max_instances, 4), bbox_ignore=(max_instances, 4), cls=(max_instances,))\n",
        "        self.instance_info = {k: dict(fill=instance_fill, shape=instance_shapes[k]) for k in instance_keys}\n",
        "        self.max_instances = max_instances\n",
        "        self.anchor_labeler = anchor_labeler\n",
        "\n",
        "    def __call__(self, batch):\n",
        "\n",
        "        # print(batch[0][2])\n",
        "        # print(type(batch[0][2]))\n",
        "        \n",
        "        batch_size = len(batch)\n",
        "        target = dict()\n",
        "        labeler_outputs = dict()\n",
        "        img_tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n",
        "        mAP_translated_boxes = list()\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            img_tensor[i] += torch.from_numpy(batch[i][0])\n",
        "            mAP_translated_boxes.append(torch.from_numpy(batch[i][2]))\n",
        "            labeler_inputs = {}\n",
        "            for tk, tv in batch[i][1].items():\n",
        "                instance_info = self.instance_info.get(tk, None)\n",
        "                if instance_info is not None:\n",
        "                    # target tensor is associated with a detection instance\n",
        "                    tv = torch.from_numpy(tv).to(dtype=torch.float32)\n",
        "                    if self.anchor_labeler is None:\n",
        "                        if i == 0:\n",
        "                            shape = (batch_size,) + instance_info['shape']\n",
        "                            target_tensor = torch.full(shape, instance_info['fill'], dtype=torch.float32)\n",
        "                            target[tk] = target_tensor\n",
        "                        else:\n",
        "                            target_tensor = target[tk]\n",
        "                        num_elem = min(tv.shape[0], self.max_instances)\n",
        "                        target_tensor[i, 0:num_elem] = tv[0:num_elem]\n",
        "                    else:\n",
        "                        # no need to pass gt tensors through when labeler in use\n",
        "                        if tk in ('bbox', 'cls'):\n",
        "                            labeler_inputs[tk] = tv\n",
        "                else:\n",
        "                    # target tensor is an image-level annotation / metadata\n",
        "                    if i == 0:\n",
        "                        # first batch elem, create destination tensors\n",
        "                        if isinstance(tv, (tuple, list)):\n",
        "                            # per batch elem sequence\n",
        "                            shape = (batch_size, len(tv))\n",
        "                            dtype = torch.float32 if isinstance(tv[0], (float, np.floating)) else torch.int32\n",
        "                        else:\n",
        "                            # per batch elem scalar\n",
        "                            shape = batch_size,\n",
        "                            dtype = torch.float32 if isinstance(tv, (float, np.floating)) else torch.int64\n",
        "                        target_tensor = torch.zeros(shape, dtype=dtype)\n",
        "                        target[tk] = target_tensor\n",
        "                    else:\n",
        "                        target_tensor = target[tk]\n",
        "                    target_tensor[i] = torch.tensor(tv, dtype=target_tensor.dtype)\n",
        "\n",
        "            if self.anchor_labeler is not None:\n",
        "                cls_targets, box_targets, num_positives = self.anchor_labeler.label_anchors(\n",
        "                    labeler_inputs['bbox'], labeler_inputs['cls'], filter_valid=False)\n",
        "                if i == 0:\n",
        "                    # first batch elem, create destination tensors, separate key per level\n",
        "                    for j, (ct, bt) in enumerate(zip(cls_targets, box_targets)):\n",
        "                        labeler_outputs[f'label_cls_{j}'] = torch.zeros(\n",
        "                            (batch_size,) + ct.shape, dtype=torch.int64)\n",
        "                        labeler_outputs[f'label_bbox_{j}'] = torch.zeros(\n",
        "                            (batch_size,) + bt.shape, dtype=torch.float32)\n",
        "                    labeler_outputs['label_num_positives'] = torch.zeros(batch_size)\n",
        "                for j, (ct, bt) in enumerate(zip(cls_targets, box_targets)):\n",
        "                    labeler_outputs[f'label_cls_{j}'][i] = ct\n",
        "                    labeler_outputs[f'label_bbox_{j}'][i] = bt\n",
        "                labeler_outputs['label_num_positives'][i] = num_positives\n",
        "        if labeler_outputs:\n",
        "            target.update(labeler_outputs)\n",
        "\n",
        "        return img_tensor, target, mAP_translated_boxes"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjFx0ZuYFca"
      },
      "source": [
        "class EffdetFruitDetectDataset(object):\n",
        "  def __init__(self, id_labels, id_bounding_boxes, transforms, mode, yxyx = True):\n",
        "\n",
        "    assert len(id_labels) == len(id_bounding_boxes)\n",
        "    assert sorted(id_labels.keys()) == sorted(id_bounding_boxes.keys())\n",
        "    self.imgs_key = sorted(id_labels.keys())\n",
        "\n",
        "    np.random.shuffle(self.imgs_key)\n",
        "    if (mode == \"train\"):\n",
        "      self.imgs_key = self.imgs_key[:int(len(self.imgs_key) * 0.8)]\n",
        "    else:\n",
        "      self.imgs_key = self.imgs_key[int(len(self.imgs_key) * 0.8):]\n",
        "\n",
        "    self.id_labels = id_labels\n",
        "    self.id_bounding_boxes = id_bounding_boxes\n",
        "    self.full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "\n",
        "    self.transforms = transforms\n",
        "    self.yxyx = yxyx\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_path = ffile_path(self.imgs_key[idx], self.full_image_file_paths) \n",
        "    # img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    boxes = convert_min_max(self.id_bounding_boxes[self.imgs_key[idx]])\n",
        "    # mAP_testable_bbox = np.array(boxes)\n",
        "    \n",
        "    labels = self.id_labels[self.imgs_key[idx]]\n",
        "    \n",
        "    target = {}\n",
        "    target['boxes'] = boxes\n",
        "    target['labels'] = labels\n",
        "\n",
        "    width, height = img.size\n",
        "    target = dict(img_idx=idx, img_size=(width, height))\n",
        "\n",
        "    bboxes = []\n",
        "    labels = []\n",
        "\n",
        "    ann ={}\n",
        "    ann['bbox'] = boxes\n",
        "    ann['label'] = labels\n",
        "\n",
        "    for ann in boxes:\n",
        "            ignore = False\n",
        "            x1, y1, x2, y2 = ann\n",
        "            label = 1\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            if w < 1 or h < 1:\n",
        "                ignore = True\n",
        "\n",
        "            bbox = ann\n",
        "\n",
        "            if self.yxyx:\n",
        "              bbox = [y1, x1, y2, x2]\n",
        "            \n",
        "            bboxes.append(bbox)\n",
        "            labels.append(label)\n",
        "\n",
        "\n",
        "    bboxes = np.array(bboxes, ndmin=2, dtype=np.float32) - 1\n",
        "\n",
        "\n",
        "    labels = np.array(labels, dtype=np.float32)\n",
        "\n",
        "    ann = dict(\n",
        "            bbox=bboxes.astype(np.float32),\n",
        "            cls=labels.astype(np.int64))\n",
        "\n",
        "    target.update(ann)\n",
        "\n",
        "    if self.transforms is not None:\n",
        "        img, target = self.transforms(img, target)\n",
        "    \n",
        "    mAP_testable_bbox = target[\"bbox\"]\n",
        "\n",
        "    return img, target, mAP_testable_bbox\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs_key)\n",
        "\n",
        "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
        "IMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)\n",
        "IMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)\n",
        "\n",
        "def run_metrics_for_batch_for_effdet(output, targets, mAP, mAR, device):\n",
        "\n",
        "  for batch_num, _ in enumerate(output):\n",
        "    boxes, scores = output[batch_num][:, :4], output[batch_num][:, 4]\n",
        "    curr_target = targets[batch_num]\n",
        "    if len(boxes) == 0:\n",
        "      raise RuntimeError(\"Cannot run metrics on an empty box\")\n",
        "    \n",
        "    curr_mAP, curr_mAR = calculate_metrics(curr_target, boxes, scores, device)\n",
        "    mAP += curr_mAP\n",
        "    mAR += curr_mAR \n",
        "  \n",
        "  return mAP, mAR\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujHCSsyzvcsT"
      },
      "source": [
        "def define_effdet_parameters():\n",
        "  global image_size\n",
        "  global model_name\n",
        "  global num_cls\n",
        "  global pretrained\n",
        "  global pretrained_backbone\n",
        "  global redundant_bias\n",
        "  global label_smoothing\n",
        "  global legacy_focal \n",
        "  global jit_loss\n",
        "  global soft_nms\n",
        "  global bench_labeler\n",
        "\n",
        "  image_size = 640\n",
        "  model_name = \"efficientdet_q1\"\n",
        "  num_cls = 6\n",
        "  pretrained=True\n",
        "  pretrained_backbone=True\n",
        "  redundant_bias=None\n",
        "  label_smoothing=None\n",
        "  legacy_focal=None\n",
        "  jit_loss=None\n",
        "  soft_nms=None\n",
        "  bench_labeler=None\n",
        "\n",
        "define_effdet_parameters()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmonKCf2YtY2"
      },
      "source": [
        "from effdet.anchors import Anchors, AnchorLabeler\n",
        "from effdet.factory import create_model, create_model_from_config\n",
        "from timm.utils import *\n",
        "from contextlib import suppress\n",
        "from collections import OrderedDict\n",
        "\n",
        "model = create_model(\n",
        "  model_name,\n",
        "  bench_task='train',\n",
        "  num_classes=num_cls,\n",
        "  pretrained=pretrained,\n",
        "  pretrained_backbone= pretrained_backbone,\n",
        "  redundant_bias=redundant_bias,\n",
        "  label_smoothing= label_smoothing,\n",
        "  legacy_focal=legacy_focal,\n",
        "  jit_loss=jit_loss,\n",
        "  soft_nms=soft_nms,\n",
        "  bench_labeler=bench_labeler,\n",
        "  checkpoint_path='',\n",
        "  max_det_per_image = 5, #This value should be changed. The amount of images effdet predicts. May affect mAP and mAR. \n",
        ")\n",
        "\n",
        "amp_autocast = suppress\n",
        "model_config = model.config\n",
        "anchor_labeler = AnchorLabeler(\n",
        "            Anchors.from_config(model_config), num_cls, match_threshold=0.5)\n",
        "\n",
        "transform = transforms_coco_eval(\n",
        "            (image_size, image_size),\n",
        "            interpolation='bilinear',\n",
        "            use_prefetcher=True,\n",
        "            fill_color='mean',\n",
        "            mean=IMAGENET_DEFAULT_MEAN,\n",
        "            std=IMAGENET_DEFAULT_STD)\n",
        "\n",
        "train_batch_size = 2\n",
        "valid_batch_size = 2\n",
        "\n",
        "eff_train_dataset = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, transform, mode = \"train\")\n",
        "eff_train_loader = torch.utils.data.DataLoader(eff_train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= DetectionFastCollate(anchor_labeler=anchor_labeler))\n",
        "\n",
        "eff_test_dataset = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, transform, mode = \"test\")\n",
        "eff_test_loader = torch.utils.data.DataLoader(eff_test_dataset, batch_size = valid_batch_size, shuffle = True, collate_fn = DetectionFastCollate(anchor_labeler=anchor_labeler))\n",
        "# eff_test_dataset_2 = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, transform, mode = \"test\", yxyx=False)\n",
        "# eff_test_loader_2 = torch.utils.data.DataLoader(eff_test_dataset_2, batch_size = valid_batch_size, shuffle = True, collate_fn = DetectionFastCollate(anchor_labeler=anchor_labeler))\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbXEJySC236L"
      },
      "source": [
        "def train_epoch(\n",
        "        epoch, model, loader, optimizer, \n",
        "        lr_scheduler=None, saver=None, output_dir='',  loss_scaler=None, model_ema=None):\n",
        "  \n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    else:\n",
        "      warnings.warn(\"Function does not support handling models on CPU\")\n",
        "\n",
        "    batch_time_m = AverageMeter()\n",
        "    data_time_m = AverageMeter()\n",
        "    losses_m = AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    last_idx = len(loader) - 1\n",
        "    num_updates = epoch * len(loader)\n",
        "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
        "    for batch_idx, (input, target, _) in enumerate(loader):\n",
        "        last_batch = batch_idx == last_idx\n",
        "        data_time_m.update(time.time() - end)\n",
        "\n",
        "        input = input.cuda().float()\n",
        "        target2={}\n",
        "        for k,v in target.items():\n",
        "          target2[k]=v.cuda()\n",
        "  \n",
        "\n",
        "        with amp_autocast():\n",
        "          output = model(input, target2)\n",
        "        loss = output['loss']\n",
        "\n",
        "        losses_m.update(loss.item(), input.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        " \n",
        "        loss.backward()\n",
        "        # optimizer.first_step(zero_grad = True)\n",
        "        optimizer.step()\n",
        "\n",
        "        # with amp_autocast():\n",
        "        #   output = model(input, target2)\n",
        "        # loss = output['loss']\n",
        "        # loss.backward()\n",
        "        # optimizer.second_step(zero_grad = True)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        num_updates += 1\n",
        "        # scheduler.step()\n",
        "\n",
        "        batch_time_m.update(time.time() - end)\n",
        "        if last_batch or batch_idx % 10 == 0:\n",
        "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
        "            lr = sum(lrl) / len(lrl)\n",
        "\n",
        "            print(\n",
        "                'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '\n",
        "                'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f})  '\n",
        "                'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '\n",
        "                '({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
        "                'LR: {lr:.3e}  '\n",
        "                'Data: {data_time.val:.3f} ({data_time.avg:.3f})'.format(\n",
        "                    epoch,\n",
        "                    batch_idx, len(loader),\n",
        "                    100. * batch_idx / last_idx,\n",
        "                    loss=losses_m,\n",
        "                    batch_time=batch_time_m,\n",
        "                    rate=input.size(0) * 1 / batch_time_m.val,\n",
        "                    rate_avg=input.size(0) * 1  / batch_time_m.avg,\n",
        "                    lr=lr,\n",
        "                    data_time=data_time_m))\n",
        "\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "\n",
        "\n",
        "    if hasattr(optimizer, 'sync_lookahead'):\n",
        "        optimizer.sync_lookahead()\n",
        "\n",
        "    return OrderedDict([('loss', losses_m.avg)])\n",
        "\n",
        "from itertools import chain\n",
        "\n",
        "#Changed the evaluator argument.\n",
        "def validate(model, loader, valid_batch_size, args, log_suffix=''):\n",
        "    batch_time_m = AverageMeter()\n",
        "    losses_m = AverageMeter()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    last_idx = len(loader) - 1\n",
        "    with torch.no_grad():\n",
        "        valid_mAP, valid_mAR, steps = 0, 0, 0\n",
        "      #Changed this line \n",
        "        for (input, target, mAP_formatted_target) in loader:\n",
        "            steps += 1 \n",
        "            input = input.cuda().float()\n",
        "            target2={}\n",
        "            for k,v in target.items():\n",
        "              target2[k]=v.cuda()\n",
        "\n",
        "            output = model(input,  target2)\n",
        "            # This is how you get access to the model detections\n",
        "            pred = output[\"detections\"]\n",
        "\n",
        "            mAP_formatted_target = [tensor.cuda() for tensor in mAP_formatted_target]\n",
        "\n",
        "            valid_mAP, valid_mAR = run_metrics_for_batch_for_effdet(pred, mAP_formatted_target, valid_mAP, valid_mAR, device)\n",
        "            loss = output['loss']\n",
        "\n",
        "            reduced_loss = loss.data\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "            losses_m.update(reduced_loss.item(), input.size(0))\n",
        "\n",
        "            batch_time_m.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "    num_of_images_tested = valid_batch_size * steps\n",
        "    metrics = OrderedDict([('loss', losses_m.avg), (\"Valid mAP\", str(int(valid_mAP / num_of_images_tested * 100)) + \"%\"), (\"Valid mAR\", str(int(valid_mAR / num_of_images_tested * 100)) + \"%\")])\n",
        "    return metrics\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIcjkG9PQwNR"
      },
      "source": [
        "Train: 6 [ 211/212 (100%)]  Loss:  0.727989 (0.7556)  Time: 0.172s,   11.60/s  (0.175s,   11.41/s)  LR: 1.000e-04  Data: 0.029 (0.031)\n",
        "train OrderedDict([('loss', 0.7555575621015621)])\n",
        "test OrderedDict([('loss', 1.0847256957927598), ('Valid mAP', '47%'), ('Valid mAR', '77%')])\n",
        "\n",
        "\n",
        "Ranger:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edl4rQVv3Myj",
        "outputId": "125f7784-fd67-43bb-871d-5d11441c66ca"
      },
      "source": [
        "from timm.utils import get_outdir\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "output_dir = get_outdir('/content', 'train', 'exp12')\n",
        "eval_metric='loss'\n",
        "decreasing = True if eval_metric == 'loss' else False\n",
        "\n",
        "optimizer= torch.optim.AdamW(model.parameters(), lr=.0001)\n",
        "# base_optimizer = torch.optim.AdamW\n",
        "# optimizer = sam.SAM(model.parameters(), base_optimizer, lr = .0001)\n",
        "\n",
        "# optimizer = Ranger(model.parameters(), lr = 0.0001, weight_decay = 1e-5)\n",
        "\n",
        "saver = CheckpointSaver(\n",
        "model, optimizer, args=None, model_ema=None, amp_scaler=None,\n",
        "checkpoint_dir=output_dir, decreasing=decreasing, unwrap_fn = unwrap_bench)\n",
        "\n",
        "\n",
        "#Change this for epochs.\n",
        "epochs = 1\n",
        "\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "\n",
        "  eval_metrics= train_epoch(\n",
        "          epoch , model, eff_train_loader, optimizer,saver=saver)\n",
        "  \n",
        "  print('train',eval_metrics)\n",
        "\n",
        "  eval_metrics = validate(model, eff_test_loader , valid_batch_size, args=None)\n",
        "  \n",
        "  #Make sure to uncomment this line for saving the model\n",
        "  best_metric, best_epoch = saver.save_checkpoint(epoch=epoch,metric=eval_metrics['loss'])\n",
        "\n",
        "  print('test',eval_metrics)\n",
        "  print('')\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 0 [   0/212 (  0%)]  Loss:  0.819832 (0.8198)  Time: 0.439s,    4.55/s  (0.439s,    4.55/s)  LR: 1.000e-04  Data: 0.039 (0.039)\n",
            "Train: 0 [  10/212 (  5%)]  Loss:  1.126829 (0.9771)  Time: 0.326s,    6.14/s  (0.343s,    5.83/s)  LR: 1.000e-04  Data: 0.042 (0.046)\n",
            "Train: 0 [  20/212 (  9%)]  Loss:  0.717627 (0.9042)  Time: 0.325s,    6.15/s  (0.335s,    5.97/s)  LR: 1.000e-04  Data: 0.041 (0.044)\n",
            "Train: 0 [  30/212 ( 14%)]  Loss:  0.908511 (0.8824)  Time: 0.336s,    5.95/s  (0.336s,    5.96/s)  LR: 1.000e-04  Data: 0.052 (0.046)\n",
            "Train: 0 [  40/212 ( 19%)]  Loss:  0.798429 (0.8650)  Time: 0.333s,    6.01/s  (0.335s,    5.97/s)  LR: 1.000e-04  Data: 0.042 (0.045)\n",
            "Train: 0 [  50/212 ( 24%)]  Loss:  0.560923 (0.8514)  Time: 0.333s,    6.00/s  (0.335s,    5.96/s)  LR: 1.000e-04  Data: 0.046 (0.046)\n",
            "Train: 0 [  60/212 ( 28%)]  Loss:  0.444410 (0.8376)  Time: 0.323s,    6.19/s  (0.336s,    5.96/s)  LR: 1.000e-04  Data: 0.039 (0.046)\n",
            "Train: 0 [  70/212 ( 33%)]  Loss:  0.752788 (0.8275)  Time: 0.331s,    6.04/s  (0.335s,    5.96/s)  LR: 1.000e-04  Data: 0.044 (0.046)\n",
            "Train: 0 [  80/212 ( 38%)]  Loss:  0.776514 (0.8208)  Time: 0.330s,    6.06/s  (0.335s,    5.98/s)  LR: 1.000e-04  Data: 0.046 (0.046)\n",
            "Train: 0 [  90/212 ( 43%)]  Loss:  0.600667 (0.8126)  Time: 0.327s,    6.12/s  (0.334s,    5.98/s)  LR: 1.000e-04  Data: 0.044 (0.046)\n",
            "Train: 0 [ 100/212 ( 47%)]  Loss:  0.389391 (0.7995)  Time: 0.335s,    5.98/s  (0.335s,    5.98/s)  LR: 1.000e-04  Data: 0.042 (0.046)\n",
            "Train: 0 [ 110/212 ( 52%)]  Loss:  0.521809 (0.7908)  Time: 0.335s,    5.97/s  (0.334s,    5.98/s)  LR: 1.000e-04  Data: 0.045 (0.046)\n",
            "Train: 0 [ 120/212 ( 57%)]  Loss:  0.609969 (0.7779)  Time: 0.336s,    5.95/s  (0.334s,    5.98/s)  LR: 1.000e-04  Data: 0.046 (0.046)\n",
            "Train: 0 [ 130/212 ( 62%)]  Loss:  0.930030 (0.7729)  Time: 0.343s,    5.83/s  (0.335s,    5.98/s)  LR: 1.000e-04  Data: 0.056 (0.046)\n",
            "Train: 0 [ 140/212 ( 66%)]  Loss:  0.457861 (0.7627)  Time: 0.339s,    5.90/s  (0.335s,    5.97/s)  LR: 1.000e-04  Data: 0.042 (0.046)\n",
            "Train: 0 [ 150/212 ( 71%)]  Loss:  1.157313 (0.7572)  Time: 0.379s,    5.27/s  (0.336s,    5.96/s)  LR: 1.000e-04  Data: 0.092 (0.047)\n",
            "Train: 0 [ 160/212 ( 76%)]  Loss:  0.444013 (0.7469)  Time: 0.349s,    5.73/s  (0.336s,    5.96/s)  LR: 1.000e-04  Data: 0.049 (0.047)\n",
            "Train: 0 [ 170/212 ( 81%)]  Loss:  0.571733 (0.7351)  Time: 0.327s,    6.11/s  (0.336s,    5.96/s)  LR: 1.000e-04  Data: 0.043 (0.047)\n",
            "Train: 0 [ 180/212 ( 85%)]  Loss:  0.732514 (0.7337)  Time: 0.333s,    6.01/s  (0.336s,    5.95/s)  LR: 1.000e-04  Data: 0.044 (0.047)\n",
            "Train: 0 [ 190/212 ( 90%)]  Loss:  0.809529 (0.7277)  Time: 0.328s,    6.09/s  (0.336s,    5.96/s)  LR: 1.000e-04  Data: 0.044 (0.047)\n",
            "Train: 0 [ 200/212 ( 95%)]  Loss:  0.505668 (0.7241)  Time: 0.334s,    5.98/s  (0.336s,    5.96/s)  LR: 1.000e-04  Data: 0.044 (0.047)\n",
            "Train: 0 [ 210/212 (100%)]  Loss:  0.837716 (0.7224)  Time: 0.330s,    6.07/s  (0.336s,    5.96/s)  LR: 1.000e-04  Data: 0.043 (0.047)\n",
            "Train: 0 [ 211/212 (100%)]  Loss:  0.787206 (0.7227)  Time: 0.338s,    5.92/s  (0.336s,    5.96/s)  LR: 1.000e-04  Data: 0.051 (0.047)\n",
            "train OrderedDict([('loss', 0.7227334427946018)])\n",
            "test OrderedDict([('loss', 2.3190799692523814), ('Valid mAP', '23%'), ('Valid mAR', '43%')])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "KmvM0cFgZ60A",
        "outputId": "279ec24d-5941-48b0-bfd1-4e8b38394117"
      },
      "source": [
        "def infer_on_effdet_output(image_file_path, trained_model):\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  trained_model.to(device)\n",
        "\n",
        "  #Creating empty paraemeter to predict in train mode\n",
        "  dummy_label = dict()\n",
        "  label_struct = {'img_idx': torch.Size([1]),\n",
        "  'img_scale': torch.Size([1]),\n",
        "  'img_size': torch.Size([1, 2]),\n",
        "  'label_bbox_0': torch.Size([1, 64, 64, 36]),\n",
        "  'label_bbox_1': torch.Size([1, 32, 32, 36]),\n",
        "  'label_bbox_2': torch.Size([1, 16, 16, 36]),\n",
        "  'label_bbox_3': torch.Size([1, 8, 8, 36]),\n",
        "  'label_bbox_4': torch.Size([1, 4, 4, 36]),\n",
        "  'label_cls_0': torch.Size([1, 64, 64, 9]),\n",
        "  'label_cls_1': torch.Size([1, 32, 32, 9]),\n",
        "  'label_cls_2': torch.Size([1, 16, 16, 9]),\n",
        "  'label_cls_3': torch.Size([1, 8, 8, 9]),\n",
        "  'label_cls_4': torch.Size([1, 4, 4, 9]),\n",
        "  'label_num_positives': torch.Size([1])}\n",
        "\n",
        "  for key in label_struct:\n",
        "    dummy_label[key]= torch.zeros(label_struct[key], dtype = torch.int64) \n",
        "  \n",
        "  #The dummy labels affects the box predictions for some reason\n",
        "  \n",
        "  torch_image = F.resize(F.to_tensor(Image.open(image_file_path).convert(\"RGB\")).unsqueeze(0).to(device), [image_size, image_size])\n",
        "\n",
        "  target2 = {}\n",
        "  for k,v in dummy_label.items():\n",
        "    target2[k]=v.to(device)\n",
        "\n",
        "  # target2 = {}\n",
        "  # for key in sample_target:\n",
        "  #   target2[key] = sample_target[key][:1]\n",
        "\n",
        "  output = trained_model(torch_image, target2)\n",
        "  results = dict()\n",
        "\n",
        "  results[\"boxes\"] = output[\"detections\"][0, :, :4]\n",
        "  results[\"labels\"] = output[\"detections\"][0, :, 4]\n",
        "  results[\"scores\"] = output[\"detections\"][0, :, -1]\n",
        "\n",
        "  #Conclusions the targets you get is effected\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "infer_on_effdet_output(\"/content/Fruit Defects Dataset /Predict_Examples/1starw.jpg\", model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-a5180f2d38c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0minfer_on_effdet_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Fruit Defects Dataset /Predict_Examples/1starw.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-a5180f2d38c6>\u001b[0m in \u001b[0;36minfer_on_effdet_output\u001b[0;34m(image_file_path, trained_model)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;31m#   target2[key] = sample_target[key][:1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/effdet/bench.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, target)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 target['bbox'], target['cls'])\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_positives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'class_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclass_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'box_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbox_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/effdet/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, cls_outputs, box_outputs, cls_targets, box_targets, num_positives)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mcls_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_positives\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             box_loss_weight=self.box_loss_weight, label_smoothing=self.label_smoothing, legacy_focal=self.legacy_focal)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/effdet/loss.py\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(cls_outputs, box_outputs, cls_targets, box_targets, num_positives, num_classes, alpha, gamma, delta, box_loss_weight, label_smoothing, legacy_focal)\u001b[0m\n\u001b[1;32m    203\u001b[0m             cls_loss = new_focal_loss(\n\u001b[1;32m    204\u001b[0m                 \u001b[0mcls_outputs_at_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_targets_at_level_oh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 alpha=alpha, gamma=gamma, normalizer=num_positives_sum, label_smoothing=label_smoothing)\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mcls_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mcls_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcls_targets_at_level\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/effdet/loss.py\u001b[0m in \u001b[0;36mnew_focal_loss\u001b[0;34m(logits, targets, alpha, gamma, normalizer, label_smoothing)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0monem_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mp_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred_prob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0monem_targets\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0malpha_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0monem_targets\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mmodulating_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (80) at non-singleton dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBX6Qw46WuH5",
        "outputId": "756a6819-304e-4e1a-92c1-b9b0636e4941"
      },
      "source": [
        "infer_on_effdet_output(\"/content/Fruit Defects Dataset /Predict_Examples/blobstarw.jpg\", model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': tensor([[8.3379e+00, 4.6880e+00, 1.1102e+02, 8.5899e+01],\n",
              "         [6.9310e+00, 1.3363e+01, 8.5534e+01, 1.3768e+02],\n",
              "         [0.0000e+00, 1.0484e+01, 5.1661e+01, 1.0359e+02],\n",
              "         [2.1244e+01, 1.7404e+01, 1.8353e+02, 1.2771e+02],\n",
              "         [2.4328e+00, 6.5647e-02, 4.5054e+01, 6.6978e+01]], device='cuda:0',\n",
              "        grad_fn=<SliceBackward>),\n",
              " 'labels': tensor([0.1525, 0.1084, 0.0977, 0.0859, 0.0814], device='cuda:0',\n",
              "        grad_fn=<SelectBackward>),\n",
              " 'scores': tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SelectBackward>)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBYTpidCcppc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3x1IFQdqaTq"
      },
      "source": [
        "#### Save Model weights and load it into predict Model for Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vV4BRikqXkv"
      },
      "source": [
        "from timm.models.layers import set_layer_config\n",
        "\n",
        "checkpoint= '/content/train/exp12/model_best.pth.tar'\n",
        "pretrained=True\n",
        "\n",
        "pretrained = pretrained or not checkpoint  # might as well try to validate something\n",
        "\n",
        "with set_layer_config(scriptable='store_true'):\n",
        "  bench = create_model(\n",
        "            model_name,\n",
        "            bench_task='predict',\n",
        "            num_classes=num_cls,\n",
        "            pretrained=pretrained,\n",
        "            pretrained_backbone=pretrained_backbone,\n",
        "            redundant_bias=redundant_bias,\n",
        "            label_smoothing=label_smoothing,\n",
        "            legacy_focal=legacy_focal,\n",
        "            jit_loss=jit_loss,\n",
        "            soft_nms=soft_nms,\n",
        "            bench_labeler=bench_labeler,\n",
        "            checkpoint_path=checkpoint, \n",
        "            max_det_per_image = 5\n",
        "        )\n",
        "  \n",
        "bench.cuda()\n",
        "bench.eval()\n",
        "batch_time = AverageMeter()\n",
        "end = time.time()\n",
        "last_idx = len(eff_test_loader) - 1\n",
        "# with torch.no_grad():\n",
        "#     for i, (input, target, mAP_formatted_target) in enumerate(eff_test_loader):\n",
        "#         input = input.cuda().float()\n",
        "#         target2={}\n",
        "#         for k,v in target.items():\n",
        "#           target2[k]=v.cuda()\n",
        "#         with amp_autocast():\n",
        "#             output = bench(input)\n",
        "\n",
        "#         print(output)\n",
        "\n",
        "# mean_ap = 0."
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPmuew0CnX92"
      },
      "source": [
        "# The Mobile Net Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbDtIsBs-OWD"
      },
      "source": [
        "backbone = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
        "backbone.roi_heads.box_predictor.cls_score.out_features = 6\n",
        "backbone.roi_heads.box_predictor.bbox_pred.out_features = 24\n",
        "# backbone.roi_heads.box_predictor.cls_score.out_features = 3\n",
        "# backbone.roi_heads.box_predictor.bbox_pred.out_features = 12\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPCVMZBY_GaP",
        "outputId": "92a6ebba-be7e-417d-c799-99baed4270e6"
      },
      "source": [
        "another_one_1 = train(backbone, 5, train_loader, test_loader, noise_loader, 0.001, weight_decay = 1e-4, print_every = 80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Device: cuda\n",
            "Optimizer: SAM (\n",
            "Parameter Group 0\n",
            "    N_sma_threshhold: 5\n",
            "    alpha: 0.5\n",
            "    betas: (0.95, 0.999)\n",
            "    eps: 1e-05\n",
            "    initial_lr: 0.001\n",
            "    k: 6\n",
            "    lr: 0.001\n",
            "    rho: 0.05\n",
            "    step_counter: 0\n",
            "    weight_decay: 0.0001\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5 | Batch Number: 80 | LR: 0.00099 | Train_loss: 106.89 | Test_loss: 62.96 | Test mAP: 43.39% | Missed Test Images: 0 | Seperate Noise Loader: 5 / 100\n",
            "Epoch 1/5 | Batch Number: 160 | LR: 0.00095 | Train_loss: 66.04 | Test_loss: 51.57 | Test mAP: 41.02% | Missed Test Images: 0 | Seperate Noise Loader: 3 / 100\n",
            "\n",
            " Epoch 1 Final Train mAP: 29.55% | Epoch 1 Final Missed Train Images: 1 out of 456 images \n",
            "\n",
            "Epoch 2/5 | Batch Number: 80 | LR: 0.00083 | Train_loss: 54.52 | Test_loss: 58.23 | Test mAP: 45.46% | Missed Test Images: 1 | Seperate Noise Loader: 8 / 100\n",
            "Epoch 2/5 | Batch Number: 160 | LR: 0.00074 | Train_loss: 54.63 | Test_loss: 65.66 | Test mAP: 45.21% | Missed Test Images: 2 | Seperate Noise Loader: 19 / 100\n",
            "\n",
            " Epoch 2 Final Train mAP: 33.25% | Epoch 2 Final Missed Train Images: 4 out of 456 images \n",
            "\n",
            "Epoch 3/5 | Batch Number: 80 | LR: 0.00055 | Train_loss: 59.18 | Test_loss: 57.74 | Test mAP: 29.91% | Missed Test Images: 1 | Seperate Noise Loader: 28 / 100\n",
            "Epoch 3/5 | Batch Number: 160 | LR: 0.00044 | Train_loss: 57.17 | Test_loss: 50.26 | Test mAP: 32.64% | Missed Test Images: 1 | Seperate Noise Loader: 27 / 100\n",
            "\n",
            " Epoch 3 Final Train mAP: 33.48% | Epoch 3 Final Missed Train Images: 7 out of 456 images \n",
            "\n",
            "Epoch 4/5 | Batch Number: 80 | LR: 0.00025 | Train_loss: 49.01 | Test_loss: 85.06 | Test mAP: 52.32% | Missed Test Images: 5 | Seperate Noise Loader: 60 / 100\n",
            "Epoch 4/5 | Batch Number: 160 | LR: 0.00016 | Train_loss: 59.35 | Test_loss: 52.93 | Test mAP: 41.71% | Missed Test Images: 2 | Seperate Noise Loader: 47 / 100\n",
            "\n",
            " Epoch 4 Final Train mAP: 40.42% | Epoch 4 Final Missed Train Images: 19 out of 456 images \n",
            "\n",
            "Epoch 5/5 | Batch Number: 80 | LR: 0.00004 | Train_loss: 56.69 | Test_loss: 76.69 | Test mAP: 46.52% | Missed Test Images: 3 | Seperate Noise Loader: 53 / 100\n",
            "Epoch 5/5 | Batch Number: 160 | LR: 0.00001 | Train_loss: 53.90 | Test_loss: 77.58 | Test mAP: 45.39% | Missed Test Images: 4 | Seperate Noise Loader: 59 / 100\n",
            "\n",
            " Epoch 5 Final Train mAP: 45.58% | Epoch 5 Final Missed Train Images: 20 out of 456 images \n",
            "\n",
            "Time for Total Training 214.29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjzRmGEPmjjF",
        "outputId": "2f32c1c8-573b-4684-b150-bf1c74948b83"
      },
      "source": [
        "another_one = train(backbone, 10, train_loader, test_loader, 0.001, weight_decay = 1e-4, print_every = 80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Device: cuda\n",
            "Optimizer: SAM (\n",
            "Parameter Group 0\n",
            "    N_sma_threshhold: 5\n",
            "    alpha: 0.5\n",
            "    betas: (0.95, 0.999)\n",
            "    eps: 1e-05\n",
            "    initial_lr: 0.001\n",
            "    k: 6\n",
            "    lr: 0.001\n",
            "    rho: 0.05\n",
            "    step_counter: 0\n",
            "    weight_decay: 0.0001\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 | Batch Number: 80 | LR: 0.00100 | Train_loss: 109.40 | Test_loss: 50.10 | Test mAP: 34.56% | Test mAR: nan% | Missed Test Images: 0\n",
            "Epoch 1/10 | Batch Number: 160 | LR: 0.00099 | Train_loss: 60.02 | Test_loss: 55.98 | Test mAP: 37.12% | Test mAR: nan% | Missed Test Images: 0\n",
            "\n",
            " Epoch 1 Final Train mAP: 30.74% | Epoch 1 Final Train mAR: nan% | Epoch 1 Final Missed Train Images: 0 out of 456 images \n",
            "\n",
            "Epoch 2/10 | Batch Number: 80 | LR: 0.00096 | Train_loss: 58.38 | Test_loss: 57.53 | Test mAP: 60.33% | Test mAR: nan% | Missed Test Images: 1\n",
            "Epoch 2/10 | Batch Number: 160 | LR: 0.00093 | Train_loss: 63.52 | Test_loss: 53.23 | Test mAP: 34.23% | Test mAR: nan% | Missed Test Images: 1\n",
            "\n",
            " Epoch 2 Final Train mAP: 31.06% | Epoch 2 Final Train mAR: nan% | Epoch 2 Final Missed Train Images: 6 out of 456 images \n",
            "\n",
            "Epoch 3/10 | Batch Number: 80 | LR: 0.00087 | Train_loss: 59.26 | Test_loss: 83.11 | Test mAP: 47.73% | Test mAR: nan% | Missed Test Images: 3\n",
            "Epoch 3/10 | Batch Number: 160 | LR: 0.00083 | Train_loss: 61.76 | Test_loss: 55.83 | Test mAP: 27.57% | Test mAR: nan% | Missed Test Images: 1\n",
            "\n",
            " Epoch 3 Final Train mAP: 30.96% | Epoch 3 Final Train mAR: nan% | Epoch 3 Final Missed Train Images: 6 out of 456 images \n",
            "\n",
            "Epoch 4/10 | Batch Number: 80 | LR: 0.00075 | Train_loss: 58.68 | Test_loss: 79.41 | Test mAP: 38.63% | Test mAR: nan% | Missed Test Images: 4\n",
            "Epoch 4/10 | Batch Number: 160 | LR: 0.00070 | Train_loss: 61.69 | Test_loss: 66.05 | Test mAP: 38.82% | Test mAR: nan% | Missed Test Images: 3\n",
            "\n",
            " Epoch 4 Final Train mAP: 33.09% | Epoch 4 Final Train mAR: nan% | Epoch 4 Final Missed Train Images: 8 out of 456 images \n",
            "\n",
            "Epoch 5/10 | Batch Number: 80 | LR: 0.00060 | Train_loss: 58.89 | Test_loss: 80.11 | Test mAP: 38.94% | Test mAR: nan% | Missed Test Images: 1\n",
            "Epoch 5/10 | Batch Number: 160 | LR: 0.00055 | Train_loss: 56.70 | Test_loss: 78.22 | Test mAP: 35.02% | Test mAR: nan% | Missed Test Images: 1\n",
            "\n",
            " Epoch 5 Final Train mAP: 34.09% | Epoch 5 Final Train mAR: nan% | Epoch 5 Final Missed Train Images: 7 out of 456 images \n",
            "\n",
            "Epoch 6/10 | Batch Number: 80 | LR: 0.00044 | Train_loss: 52.62 | Test_loss: 86.22 | Test mAP: 43.91% | Test mAR: nan% | Missed Test Images: 2\n",
            "Epoch 6/10 | Batch Number: 160 | LR: 0.00039 | Train_loss: 59.55 | Test_loss: 93.01 | Test mAP: 46.96% | Test mAR: nan% | Missed Test Images: 3\n",
            "\n",
            " Epoch 6 Final Train mAP: 37.65% | Epoch 6 Final Train mAR: nan% | Epoch 6 Final Missed Train Images: 20 out of 456 images \n",
            "\n",
            "Epoch 7/10 | Batch Number: 80 | LR: 0.00029 | Train_loss: 54.05 | Test_loss: 99.96 | Test mAP: 39.45% | Test mAR: nan% | Missed Test Images: 4\n",
            "Epoch 7/10 | Batch Number: 160 | LR: 0.00025 | Train_loss: 58.93 | Test_loss: 79.80 | Test mAP: 34.77% | Test mAR: nan% | Missed Test Images: 1\n",
            "\n",
            " Epoch 7 Final Train mAP: 38.81% | Epoch 7 Final Train mAR: nan% | Epoch 7 Final Missed Train Images: 18 out of 456 images \n",
            "\n",
            "Epoch 8/10 | Batch Number: 80 | LR: 0.00016 | Train_loss: 58.33 | Test_loss: 90.27 | Test mAP: 38.50% | Test mAR: nan% | Missed Test Images: 3\n",
            "Epoch 8/10 | Batch Number: 160 | LR: 0.00012 | Train_loss: 52.78 | Test_loss: 92.37 | Test mAP: 45.57% | Test mAR: nan% | Missed Test Images: 3\n",
            "\n",
            " Epoch 8 Final Train mAP: 43.94% | Epoch 8 Final Train mAR: nan% | Epoch 8 Final Missed Train Images: 24 out of 456 images \n",
            "\n",
            "Epoch 9/10 | Batch Number: 80 | LR: 0.00007 | Train_loss: 52.34 | Test_loss: 99.14 | Test mAP: 48.14% | Test mAR: nan% | Missed Test Images: 4\n",
            "Epoch 9/10 | Batch Number: 160 | LR: 0.00004 | Train_loss: 59.78 | Test_loss: 97.68 | Test mAP: 42.97% | Test mAR: nan% | Missed Test Images: 4\n",
            "\n",
            " Epoch 9 Final Train mAP: 47.44% | Epoch 9 Final Train mAR: nan% | Epoch 9 Final Missed Train Images: 29 out of 456 images \n",
            "\n",
            "Epoch 10/10 | Batch Number: 80 | LR: 0.00001 | Train_loss: 53.61 | Test_loss: 105.94 | Test mAP: 44.46% | Test mAR: nan% | Missed Test Images: 4\n",
            "Epoch 10/10 | Batch Number: 160 | LR: 0.00000 | Train_loss: 53.82 | Test_loss: 105.05 | Test mAP: 45.50% | Test mAR: nan% | Missed Test Images: 4\n",
            "\n",
            " Epoch 10 Final Train mAP: 51.20% | Epoch 10 Final Train mAR: nan% | Epoch 10 Final Missed Train Images: 27 out of 456 images \n",
            "\n",
            "Time for Total Training 381.80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-cknP0_uy0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "916917ab-36c0-4f8a-fd3c-0fcfe2fa1754"
      },
      "source": [
        "# https://github.com/amdegroot/ssd.pytorch/blob/master/layers/box_utils.py#L48\n",
        "def intersect(box_a, box_b):\n",
        "\n",
        "    A = box_a.size(0)\n",
        "    B = box_b.size(0)\n",
        "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
        "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
        "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
        "    return inter[:, :, 0] * inter[:, :, 1]\n",
        "\n",
        "def jaccard_iou(box_a, box_b):\n",
        "\n",
        "    inter = intersect(box_a, box_b)\n",
        "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
        "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
        "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
        "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
        "    union = area_a + area_b - inter\n",
        "    return inter / union  # [A,B]\n",
        "\n",
        "def calculate_iou_on_label(results, len_of_results, iou_thresh, device):\n",
        "  for current_index, _ in enumerate(results[\"boxes\"]):\n",
        "    if current_index >= len_of_results:\n",
        "      break\n",
        "\n",
        "    current_index_iou = jaccard_iou(results[\"boxes\"][current_index].view(1, -1).to(device),\n",
        "                                    results[\"boxes\"].to(device))\n",
        "    \n",
        "    mask = (current_index_iou > iou_thresh) & (current_index_iou != 1)\n",
        "    mask = mask.squeeze()\n",
        "    for key in results:\n",
        "      results[key] = results[key][~mask]\n",
        "\n",
        "    len_of_results -= sum(mask)\n",
        "  \n",
        "  return results\n",
        "\n",
        "def get_labels_categ(classes, want):\n",
        "  fruit_index_list, bad_spot_index_list = list(), list()\n",
        "  for ii, name in enumerate(classes):\n",
        "    if re.search(\"Spot\", name):\n",
        "      bad_spot_index_list.append(ii)\n",
        "    elif re.search(\"Placeholder\", name):\n",
        "      continue\n",
        "    else:\n",
        "      fruit_index_list.append(ii)\n",
        "  \n",
        "  if want == \"fruit\":\n",
        "    return fruit_index_list\n",
        "  elif want == \"bad_spot\":\n",
        "    return bad_spot_index_list\n",
        "  else:\n",
        "    raise ValueError(\"want Type not applicable [fruit or bad_spot only]\")\n",
        "\n",
        "print(classes)\n",
        "get_labels_categ(classes, \"bad_spot\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Placeholder', 'Apples', 'Strawberry', 'Tomato', 'Apple_Bad_Spot', 'Strawberry_Bad_Spot', 'Tomato_Bad_Spot']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 5, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFhjlggXlAx7"
      },
      "source": [
        "def infer_image(image_file_path, trained_model, distance_thresh, iou_thresh, webcam = False, show_image = True):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  #Just load it up as PIL. Avoid using cv2 because do not need albumentations\n",
        "  if not webcam:\n",
        "    torch_image = F.to_tensor(Image.open(image_file_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    trained_model.to(device)\n",
        "    trained_model.eval()\n",
        "    print(\"Image Size: {}\".format(torch_image.size()))\n",
        "\n",
        "    start_time = time.time()\n",
        "    results = trained_model(torch_image)\n",
        "    end_time = time.time() - start_time\n",
        "\n",
        "    print(\"Time of Inference {:0.2f}\".format(end_time))\n",
        "  else:\n",
        "\n",
        "    torch_image = F.to_tensor(image_file_path).unsqueeze(1).to(device)\n",
        "\n",
        "    results = trained_model(torch_image)\n",
        "\n",
        "  valid_box_count = 0\n",
        "  for ii, score in enumerate(results[0][\"scores\"]):\n",
        "    if score < distance_thresh:\n",
        "      low_index_start = ii\n",
        "      break\n",
        "    else:\n",
        "      valid_box_count += 1\n",
        "\n",
        "  if valid_box_count == len(results[0][\"scores\"]):\n",
        "    low_index_start = len(results[0][\"scores\"])\n",
        "  \n",
        "  for key in results[0]:\n",
        "    results[0][key] = results[0][key][:low_index_start]\n",
        "  \n",
        "  #This is where I place the order of the list\n",
        "  fruit_spot_iou_thresh, bad_spot_iou_thresh = iou_thresh\n",
        "\n",
        "  #Update when I get more data of fruits and when running for script beware of classes.\n",
        "  bad_spot_index = [ii for ii, label in enumerate(results[0][\"labels\"]) if label in get_labels_categ(classes, \"bad_spot\")]\n",
        "  fruit_index = [ii for ii, _ in enumerate(results[0][\"labels\"]) if ii not in bad_spot_index]\n",
        "\n",
        "  bad_spot_results, fruit_results = dict(), dict()\n",
        "\n",
        "  for key in results[0]:\n",
        "    bad_spot_results[key], fruit_results[key] = results[0][key][[bad_spot_index]], results[0][key][[fruit_index]]\n",
        "\n",
        "  assert len(bad_spot_results[\"boxes\"]) == len(bad_spot_results[\"scores\"]) == len(bad_spot_results[\"labels\"])\n",
        "  assert len(fruit_results[\"boxes\"]) == len(fruit_results[\"scores\"]) == len(fruit_results[\"labels\"])\n",
        "\n",
        "  len_of_bad_spots, len_of_fruit = len(bad_spot_results[\"boxes\"]), len(fruit_results[\"boxes\"])\n",
        "\n",
        "  if len_of_bad_spots > 1:\n",
        "    bad_spot_results = calculate_iou_on_label(bad_spot_results, len_of_bad_spots, bad_spot_iou_thresh, device)\n",
        "  if len_of_fruit > 1:\n",
        "    fruit_results = calculate_iou_on_label(fruit_results, len_of_fruit, fruit_spot_iou_thresh, device)\n",
        "  \n",
        "  for key in results[0]: \n",
        "    if (key == \"boxes\"):\n",
        "      results[0][\"boxes\"] = torch.cat((fruit_results[\"boxes\"], bad_spot_results[\"boxes\"]), axis = 0)\n",
        "    else:\n",
        "      results[0][key] = torch.cat((fruit_results[key], bad_spot_results[key]), dim = 0)\n",
        "\n",
        "  if show_image:\n",
        "    if device == torch.device(\"cuda\"):\n",
        "      torch_image = torch_image.cpu() \n",
        "    written_image = cv2.cvtColor(draw_boxes(results[0][\"boxes\"], results[0][\"labels\"], torch_image.squeeze(), infer = True, put_text= True), cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(written_image)\n",
        "  \n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ab85jWlu5L7"
      },
      "source": [
        "results = infer_image(\"/content/tomatpred.jpg\", another_one, 0.2, [0.3, 0.1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZd4nVDuiu4M"
      },
      "source": [
        "import base64\n",
        "import html\n",
        "import io\n",
        "import time\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "def start_input():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      /* try changing the capture canvas and see what happens*/\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def take_photo(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T45EScTjJ5e"
      },
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def js_reply_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          js_reply: JavaScript object, contain image from webcam\n",
        "\n",
        "    output: \n",
        "          image_array: image array RGB size 512 x 512 from webcam\n",
        "    \"\"\"\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\n",
        "    image_array = np.array(image_PIL)\n",
        "\n",
        "    return image_array\n",
        "\n",
        "def drawing_array_to_bytes(drawing_array):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          drawing_array: image RGBA size 512 x 512 \n",
        "                              contain bounding box and text from yolo prediction, \n",
        "                              channel A value = 255 if the pixel contains drawing properties (lines, text) \n",
        "                              else channel A value = 0\n",
        "\n",
        "    output: \n",
        "          drawing_bytes: string, encoded from drawing_array\n",
        "    \"\"\"\n",
        "\n",
        "    drawing_PIL = Image.fromarray(drawing_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    drawing_PIL.save(iobuf, format='png')\n",
        "    drawing_bytes = 'data:image/png;base64,{}'.format((str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "    return drawing_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FSrjfHjgX4q"
      },
      "source": [
        "data_transforms = get_transforms(mode = \"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "j0WidBmGlktM",
        "outputId": "6e291470-abb3-45b9-8be7-c90bf01e15e3"
      },
      "source": [
        "start_input()\n",
        "label_html = 'Capturing...'\n",
        "img_data = ''\n",
        "count = 0 \n",
        "\n",
        "color=None\n",
        "label=None\n",
        "line_thickness=None\n",
        "another_one.to(device).eval();\n",
        "while True:\n",
        "    js_reply = take_photo(label_html, img_data)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    image = js_reply_to_image(js_reply)\n",
        "    prediciton = infer_image(image, another_one, 0.03, [0.3, 0.1], webcam= True, show_image = False)\n",
        "\n",
        "    drawing_array = np.zeros([512,512,4], dtype=np.uint8)\n",
        "\n",
        "    for x in prediciton[0]['boxes']:\n",
        "\n",
        "      tl = line_thickness or round(0.002 * (drawing_array.shape[0] + drawing_array.shape[1]) / 2) + 1  # line/font thickness\n",
        "      color = color or [random.randint(0, 255) for _ in range(3)]\n",
        "      c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
        "      cv2.rectangle(drawing_array, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
        "      if label:\n",
        "        tf = max(tl - 1, 1)  # font thickness\n",
        "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
        "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
        "        cv2.rectangle(drawing_array, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
        "        cv2.putText(drawing_array, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "    drawing_array[:,:,3] = (drawing_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "\n",
        "    drawing_PIL = Image.fromarray(drawing_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    drawing_PIL.save(iobuf, format='png')\n",
        "    drawing_bytes = 'data:image/png;base64,{}'.format((str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "    img_data = drawing_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      /* try changing the capture canvas and see what happens*/\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 512; //video.videoWidth;\n",
              "      captureCanvas.height = 512; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function takePhoto(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}