{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FruitObjectDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "513586f568764f66a2ec6c1ce6e48ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_efe79b0083b04ad2bd627cf8b810d2cf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c47a98cc3c6f4b168d011ca10610c4fa",
              "IPY_MODEL_ed954f85d1b04ed49258a3762b0a75d5"
            ]
          }
        },
        "efe79b0083b04ad2bd627cf8b810d2cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c47a98cc3c6f4b168d011ca10610c4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4b66dccb59fb41a593925d92f83647d0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 21388428,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 21388428,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_292f67bf7c214d0aa109969537a3b691"
          }
        },
        "ed954f85d1b04ed49258a3762b0a75d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_575144ade3e644ac927ba54651fd5427",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20.4M/20.4M [00:00&lt;00:00, 67.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c1c7aad80eaa4a00b56c01ed377d594c"
          }
        },
        "4b66dccb59fb41a593925d92f83647d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "292f67bf7c214d0aa109969537a3b691": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "575144ade3e644ac927ba54651fd5427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c1c7aad80eaa4a00b56c01ed377d594c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZfnWC_NF7NM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch \n",
        "from torch import nn, optim \n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import itertools\n",
        "import glob \n",
        "from PIL import Image\n",
        "import csv \n",
        "import cv2\n",
        "import re\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.ops.boxes import box_iou\n",
        "import random\n",
        "import torchvision\n",
        "import warnings\n",
        "\n",
        "!pip install --upgrade albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "!git clone https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
        "%cd Ranger-Deep-Learning-Optimizer\n",
        "!pip install -e .\n",
        "from ranger import Ranger  \n",
        "%cd ..\n",
        "#https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1\n",
        "!git clone https://github.com/davda54/sam.git\n",
        "%cd sam\n",
        "import sam\n",
        "print(\"Imported SAM Successfully from github .py file\")\n",
        "%cd ..\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_pckeYPGF0b",
        "outputId": "9b5f9ce3-f85e-4e1f-fc9e-39c3c63a1173"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCS2doQYbNpf"
      },
      "source": [
        "## To Do Right Now: \n",
        "\n",
        "### Steps for fully integrating effecient det \n",
        "* Update this code to run on a single image in a train batch\n",
        "* change run metrics for batch so it can understand how effdet outputs data and then develops statistics.\n",
        "* Make a validation loader\n",
        "* Play with test function using optimization techniues like Normalizing (ensuring normalizing is consistent and global), sam, ranger to see if that increase acc or decreases loss\n",
        "\n",
        "later: \n",
        "* Understand Effecient Det Data Parralel code\n",
        "\n",
        "\n",
        "### Spend 2 hours labeling the other data on peaches and tomatoes label them and put them in dataset.\n",
        "\n",
        "\n",
        "### Email Evan about any new project ideas. Look at new project ideas from the DOCUMENT. Read about new ideas into agriculture and look for problems that can be solved with computer vision. Also, develop note-taking idea a little bit. \n",
        "\n",
        "#Possible Solution\n",
        "\n",
        "### go to ImageNet and get images of round classes so like human faces or dog faces or balls etc. (have no bounding boxes). Get a new loader called noise_loader and make a loss that sees how many bouding boxes are predicted in the noise_loader class.\n",
        "\n",
        "# Goal \n",
        "Get model running on web cam on like 10 different fruits / vegetables\n",
        "\n",
        "#Less Urgent Ideas in the Future:\n",
        "\n",
        "### Look into turning jupyter notebook into python script or deploy it to rasberry pi. Makes sure when converting to python script be aware of classes variable.\n",
        "\n",
        " \n",
        "### https://www.emerginginvestigators.org/articles?category_id=10\n",
        "\n",
        "### Train, Once I get a model with very good results. torch.save it state_dicts on my local disk. Also record results of models on results spreadsheet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ3xBjGlDiXK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4UKDVNnIqQN"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/LatestFruit Defects Dataset .zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/EfficientDet.Pytorch-Updated.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1cWaZ4DSJp3",
        "outputId": "94e5a1e1-a83b-4fab-99cd-fbbe8d9c1d7a"
      },
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "%cd EfficientDet.Pytorch-Updated/\n",
        "import math\n",
        "from models.efficientnet import EfficientNet\n",
        "from models.bifpn import BIFPN\n",
        "from models.retinahead import RetinaHead\n",
        "from models.module import RegressionModel, ClassificationModel, Anchors, ClipBoxes, BBoxTransform\n",
        "from torchvision.ops import nms\n",
        "from models.losses import FocalLoss\n",
        "from models.efficientdet import EfficientDet\n",
        "from models.losses import FocalLoss\n",
        "from datasets import VOCDetection, CocoDataset, get_augumentation, detection_collate, Resizer, Normalizer, Augmenter, collater\n",
        "from utils import EFFICIENTDET, get_state_dict\n",
        "from eval import evaluate, evaluate_coco\n",
        "%cd .."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/EfficientDet.Pytorch-Updated\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY83cUEFAjoW",
        "outputId": "dc01b7bd-c750-4165-e79e-7c6ce8c8a265"
      },
      "source": [
        "#For one strawberry batch please drop watermark rows\n",
        "strawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 3 Labeled/FreshStrawberryBatch3Labels.csv\", header = None)\n",
        "strawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 2 Labeled/FreshStrawberriesBatch2Labels.csv\", header = None)\n",
        "strawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 1 Labeled/Strawberrybatch1.csv\", header = None)\n",
        "rottenApple_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch1Labeled/RottenAppleBatch1Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch2Labeled/RottenApplesBatch2Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch3Labaled/RottenApplesBatch3Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch1RottenStrawBerryLabels/RottenStrawberriesBatch1Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch2RottenStrawBerryLabels/RottenStrawBerryBatch2.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch3RottenStrawberrylabel/rottenStrawberryBtch3labels.csv\", header = None)\n",
        "freshApples_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplebtch2label/FreshApplesBatch2LabelsFresh.csv\", header = None)\n",
        "freshApples_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplesBatch1Labels/FreshAppleBatch1Labels.csv\", header = None)\n",
        "rottenTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/Rotten TomatoBatch1/Batch1TomoatosLabelsBbox.csv\", header = None)\n",
        "rottenTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottnTomatoBatch2/RottenTomatyoBatch2Labelss.csv\", header = None)\n",
        "rottenTomato_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatBtch3/RottenTomatoesBatch3Labssles.csv\", header = None)\n",
        "rottenTomato_csv_batch_4 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatoesBatch4/Tomatobatch4labelssRotten.csv\", header = None)\n",
        "freshTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatoesBatch1Labelss/FreshTomatoesLabelsBatch1Labels.csv\", header = None)\n",
        "freshTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatBatch2Labessls/Batch2TomatlabelsFresh.csv\", header = None)\n",
        "\n",
        "strawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_3.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_4.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "\n",
        "#Drop some watermark data for Fresh StrawBerry Batch 1 Labeled images [59, 9, 93]\n",
        "\n",
        "# strawberry_csv_batch_1 = strawberry_csv_batch_1[Image_id not in [\"FreshStrawberries59.jpeg, FreshStrawberries9.jpeg, FreshStrawberries93.jpeg\"]]\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries59.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries9.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries93.jpeg\"].index, inplace = True)\n",
        "freshTomato_csv_batch_1.drop(freshTomato_csv_batch_1[freshTomato_csv_batch_1[\"Image_id\"] == \"Fresh Tomatoes66AddonPart1.jpeg\"].index, inplace = True)\n",
        "\n",
        "strawberry_csv_batch_1 = strawberry_csv_batch_1.reset_index(drop=True)\n",
        "freshTomato_csv_batch_1 = freshTomato_csv_batch_1.reset_index(drop = True)\n",
        "\n",
        "#Stack all the csv files together. \n",
        "list_of_all_dataframes = [strawberry_csv_batch_1, strawberry_csv_batch_2, strawberry_csv_batch_3, rottenApple_csv_batch_1, \n",
        "                          rottenApple_csv_batch_2, rottenApple_csv_batch_3, rottenStrawberry_csv_batch_1, rottenStrawberry_csv_batch_2, \n",
        "                          rottenStrawberry_csv_batch_3, freshApples_csv_batch_2, freshApples_csv_batch_1, rottenTomato_csv_batch_1, \n",
        "                          rottenTomato_csv_batch_2, rottenTomato_csv_batch_3, rottenTomato_csv_batch_4, freshTomato_csv_batch_1, \n",
        "                          freshTomato_csv_batch_2]\n",
        "fruit_df = pd.concat(list_of_all_dataframes, ignore_index = True)\n",
        "\n",
        "total_row_sum_check = 0 \n",
        "for dataframe in list_of_all_dataframes:\n",
        "  total_row_sum_check += dataframe.shape[0]\n",
        "print(\"Checked total rows from all the dataframes combined: {}\".format(total_row_sum_check))\n",
        "\n",
        "def run_dataframe_check():\n",
        "  assert total_row_sum_check == fruit_df.shape[0]\n",
        "  print(\"DataFrame shape: {}\".format(fruit_df.shape))\n",
        "  print(\"Unique Fruit Labels {}\".format(fruit_df[\"Fruit\"].unique()))\n",
        "  print(\"Number of Unique Images {}\".format(len(fruit_df[\"Image_id\"].unique())))\n",
        "\n",
        "run_dataframe_check()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checked total rows from all the dataframes combined: 1882\n",
            "DataFrame shape: (1882, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Bad_Spots' 'Tomato']\n",
            "Number of Unique Images 532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KalFI9OONbjp",
        "outputId": "7650150d-3286-4877-b79a-dc9559c37268"
      },
      "source": [
        "#Specify more image types when \n",
        "def more_specific_Image_id(image_id, fruit):\n",
        "  if fruit == \"Bad_Spots\":\n",
        "    if re.search(\"RottenStrawberries\", image_id):\n",
        "      return \"Strawberry_Bad_Spot\"\n",
        "    elif re.search(\"RottenApples\", image_id):\n",
        "      return \"Apple_Bad_Spot\"\n",
        "    elif re.search(\"Rotten Tomatoes\", image_id):\n",
        "      return \"Tomato_Bad_Spot\"\n",
        "    else:\n",
        "      raise ValueError(\"Could not find a match for some of the Image_ids\")\n",
        "\n",
        "  else:\n",
        "    return fruit\n",
        "\n",
        "fruit_df[\"Fruit\"] = fruit_df.apply(lambda row: more_specific_Image_id(row.Image_id, row.Fruit), axis = 1)\n",
        "\n",
        "run_dataframe_check()\n",
        "\n",
        "#Post Processing \n",
        "fruit_df = fruit_df[fruit_df[\"Image_id\"] != \"FreshStrawberries15.jpeg\"]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame shape: (1882, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Apple_Bad_Spot' 'Strawberry_Bad_Spot' 'Tomato'\n",
            " 'Tomato_Bad_Spot']\n",
            "Number of Unique Images 532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIoosfwlUAdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03da8d86-831b-49be-ff07-5efa23cb5bf3"
      },
      "source": [
        "bounding_box_dict = dict()\n",
        "labels_dict = dict()\n",
        "classes = [\"Placeholder\", \"Apples\", \"Strawberry\", \"Tomato\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\", \"Tomato_Bad_Spot\"]\n",
        "# classes = [\"Apples\", \"Strawberry\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\"]\n",
        "print(classes)\n",
        "# classes = [\"Bad_Spots\", \"Strawberry\", \"Apples\"]\n",
        "\n",
        "for row_index in range(len(fruit_df)): \n",
        "  current_image_file = fruit_df.iloc[row_index][\"Image_id\"]\n",
        "  if current_image_file not in bounding_box_dict:\n",
        "    bounding_box_dict[current_image_file] = list()\n",
        "    labels_dict[current_image_file] = list()\n",
        "  bounding_box_dict[current_image_file].append(fruit_df.iloc[row_index, 1:5].to_list())\n",
        "  labels_dict[current_image_file].append(classes.index(fruit_df.iloc[row_index, 0]))\n",
        "\n",
        "print(len(bounding_box_dict))\n",
        "print(len(labels_dict))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Placeholder', 'Apples', 'Strawberry', 'Tomato', 'Apple_Bad_Spot', 'Strawberry_Bad_Spot', 'Tomato_Bad_Spot']\n",
            "531\n",
            "531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15qhrCwGUPxp"
      },
      "source": [
        "## Class function + util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgVTPiupUTQa"
      },
      "source": [
        "def ffile_path(image_id, full_image_file_paths):\n",
        "  for image_path in full_image_file_paths:\n",
        "    if image_id in image_path:\n",
        "      return image_path\n",
        "\n",
        "def find_area_bb(bb_coord):\n",
        "  bb_coord = bb_coord.numpy()\n",
        "  area_of_each_bb = list()\n",
        "  for pair_of_coord in bb_coord:\n",
        "    area_of_each_bb.append(\n",
        "        (pair_of_coord[2] - pair_of_coord[0]) * (pair_of_coord[3] - pair_of_coord[1])\n",
        "    )\n",
        "  return torch.tensor(area_of_each_bb, dtype=torch.int32)\n",
        "\n",
        "def convert_min_max(bb_coord):\n",
        "  for pair_of_coord in bb_coord:\n",
        "    pair_of_coord[2], pair_of_coord[3] = (pair_of_coord[0] + pair_of_coord[-2]), (pair_of_coord[1] + pair_of_coord[-1])\n",
        "  return bb_coord\n",
        "\n",
        "class FruitDetectDataset(object):\n",
        "  def __init__(self, id_labels, id_bounding_boxes, transforms, mode):\n",
        "\n",
        "    assert len(id_labels) == len(id_bounding_boxes)\n",
        "    assert sorted(id_labels.keys()) == sorted(id_bounding_boxes.keys())\n",
        "    self.imgs_key = sorted(id_labels.keys())\n",
        "\n",
        "    np.random.shuffle(self.imgs_key)\n",
        "    if (mode == \"train\"):\n",
        "      self.imgs_key = self.imgs_key[:int(len(self.imgs_key) * 0.8)]\n",
        "    else:\n",
        "      self.imgs_key = self.imgs_key[int(len(self.imgs_key) * 0.8):]\n",
        "\n",
        "    self.id_labels = id_labels\n",
        "    self.id_bounding_boxes = id_bounding_boxes\n",
        "    self.full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_path = ffile_path(self.imgs_key[idx], self.full_image_file_paths) \n",
        "    img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "    boxes = convert_min_max(torch.as_tensor(self.id_bounding_boxes[self.imgs_key[idx]], dtype=torch.float32))\n",
        "    \n",
        "    labels = torch.as_tensor(self.id_labels[self.imgs_key[idx]], dtype=torch.int64)\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = find_area_bb(boxes)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    \n",
        "    #Query about transforms for labels of images\n",
        "    if self.transforms: \n",
        "      sample = {\n",
        "                'image': img,\n",
        "                'bboxes': target['boxes'],\n",
        "                'labels': labels\n",
        "            }\n",
        "\n",
        "      sample = self.transforms(**sample)\n",
        "      img = sample['image']\n",
        "      target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "    \n",
        "    \n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs_key)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDh2_pg4J2yo"
      },
      "source": [
        "#The Drawing function.\n",
        "# COLORS = [(255, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0)]\n",
        "COLORS = [(0, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0), (255, 0, 0)]\n",
        "\n",
        "def draw_boxes(boxes, labels, image, infer = False, put_text = True):\n",
        "    # read the image with OpenCV\n",
        "    image = image.permute(1, 2, 0).numpy()\n",
        "    if infer:\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    for i, box in enumerate(boxes):\n",
        "        color = COLORS[labels[i] % len(COLORS)]\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (int(box[0]), int(box[1])),\n",
        "            (int(box[2]), int(box[3])),\n",
        "            color, 2\n",
        "        )\n",
        "        if put_text:\n",
        "          cv2.putText(image, classes[labels[i]], (int(box[0]), int(box[1]-5)),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n",
        "                      lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "# Albumentations\n",
        "def get_transforms(mode):\n",
        "  if (mode == \"train\"):\n",
        "    return A.Compose([\n",
        "                      A.OneOf([\n",
        "                      A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                     val_shift_limit=0.2, p=0.9),\n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "                      A.HorizontalFlip(),\n",
        "                      A.VerticalFlip(), \n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"test\"):\n",
        "    return A.Compose([\n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  elif (mode == \"effdet_train\"):\n",
        "    return A.Compose([\n",
        "                      A.OneOf([\n",
        "                      A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                     val_shift_limit=0.2, p=0.9),\n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "                      A.HorizontalFlip(),\n",
        "                      A.VerticalFlip(), \n",
        "                      A.Resize(height = 512, width=512), \n",
        "                      ToTensorV2()\n",
        "                      ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "  else:\n",
        "    raise ValueError(\"mode is wrong value can either be train or test\")\n",
        "\n",
        "#Using this stack overflow (https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders)\n",
        "#(Suppose for example, you want to create batches of a list of varying dimension tensors. The below code pads sequences with 0 until the maximum sequence size of the batch,)\n",
        "#Collate_fn is a function that is used to process your batches before you pass it to dataloader. In my case since I have different sized images I need a way to stack batches b/c torch.stack won't work.\n",
        "#So I use zip which can accept tensors of different lengths and make them stacked with the size of the lowest length list given. Therefore stacking all the images in a batch \n",
        "#Successfully unlike torch.stack and doing that processing to every batch makes collate_fn vital since I have different image sizes.\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return tuple([list(a) for a in zip(*batch)])\n",
        "    # return tuple(zip(*batch))\n",
        "\n",
        "train_batch_size = 1\n",
        "test_batch_size = 1\n",
        "\n",
        "train_dataset = FruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"train\"), mode = \"train\")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= collate_fn)\n",
        "\n",
        "test_dataset = FruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"test\"), mode = \"test\")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = test_batch_size, shuffle = True, collate_fn= collate_fn)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "-uf9fdpPK-SQ",
        "outputId": "6ab53e08-8404-482d-c242-1e2f6fb48075"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    image = draw_boxes(labels[idx][\"boxes\"], labels[idx][\"labels\"], images[idx])\n",
        "    plt.imshow(image)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2c30b5b4a517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A0EkOWcPSX0"
      },
      "source": [
        "## What we learned\n",
        "is that the model outputs losses when in train mode \n",
        "when in model.eval model, the model code then return only a prediction with no losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NQrBD22VWxD"
      },
      "source": [
        "def calculate_metrics(target_box,predictions_box,scores, device):\n",
        "\n",
        "    #Get most confident boxes first and least confident last\n",
        "    predictions_box = predictions_box[scores.argsort().flip(-1)]\n",
        "    iou_mat = box_iou(target_box,predictions_box)\n",
        "    #return a one by one matrix that is form (target_box, prediction_box) or (1, 1)\n",
        "    target_boxes_count, prediction_boxes_count = iou_mat.shape\n",
        "    \n",
        "    mAP_Matrix = torch.zeros_like(iou_mat)\n",
        "    # if not matrix coordinates that relate to nothing.\n",
        "    if not iou_mat[:,0].eq(0.).all():\n",
        "      index_of_biggest_iou = iou_mat[:,0].argsort()[-1]\n",
        "      mAP_Matrix[index_of_biggest_iou,0] = 1\n",
        "\n",
        "    for pr_idx in range(1,prediction_boxes_count):\n",
        "        not_assigned = torch.logical_not(mAP_Matrix[:,:pr_idx].sum(1)).long()\n",
        "        targets = not_assigned * iou_mat[:,pr_idx]\n",
        "\n",
        "        if targets.eq(0).all():\n",
        "            continue\n",
        "\n",
        "        pivot = targets.argsort()[-1]\n",
        "        mAP_Matrix[pivot,pr_idx] = 1\n",
        "\n",
        "    # mAP calculation\n",
        "    tp = mAP_Matrix.sum()\n",
        "    fp = mAP_Matrix.sum(0).eq(0).sum()\n",
        "    fn = mAP_Matrix.sum(1).eq(0).sum()\n",
        "\n",
        "    mAP = tp / (tp+fp)\n",
        "    mAR = tp / (tp+fn)\n",
        "\n",
        "    return mAP, mAR\n",
        "\n",
        "def run_metrics_for_batch(output, targets, mAP, mAR, missed_images, device):\n",
        "  for pos_in_batch, image_pred in enumerate(output):\n",
        "    assert (len(image_pred[\"boxes\"]) == len(image_pred[\"labels\"]) == len(image_pred[\"scores\"]))\n",
        "    if len(image_pred[\"boxes\"]) != 0:\n",
        "      curr_mAP, curr_mAR = calculate_metrics(targets[pos_in_batch][\"boxes\"], output[pos_in_batch][\"boxes\"], output[pos_in_batch][\"scores\"], device)\n",
        "      mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
        "    else:\n",
        "      missed_images += 1 \n",
        "  \n",
        "  return mAP, mAR, missed_images\n",
        "\n",
        "def run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device):\n",
        "    assert (len(scores) == len(classification) == len(transformed_anchors))\n",
        "    if len(transformed_anchors) != 0:\n",
        "      curr_mAP, curr_mAR = calculate_metrics(targets[0][:, :4], transformed_anchors, scores, device)\n",
        "      mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
        "    else:\n",
        "      missed_images += 1 \n",
        "      \n",
        "    return mAP, mAR, missed_images\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8CWCsiinNQ5",
        "outputId": "2e94e0b8-5991-4a96-9dd9-df4fbf025db4"
      },
      "source": [
        "mAp, mar, missed_im = run_metrics_for_effdet_batch(scores.cuda(), classification.cuda(), transformed_anchors.cuda(), targets.cuda(), 0, 0, 0, torch.device(\"cuda\"))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0001, device='cuda:0'), tensor(1., device='cuda:0'), 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDZ0Avfnk9pl",
        "outputId": "c85036ad-216e-4460-8870-538a17331a35"
      },
      "source": [
        "len(scores) == len(classification) == len(transformed_anchors)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VqELmnjmmtW",
        "outputId": "036ba2f0-af3b-4b5d-fe07-28a30c8255a9"
      },
      "source": [
        "scores"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 1.0000,  ..., 0.0333, 0.0309, 0.0309], device='cuda:0',\n",
              "       grad_fn=<MaxBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnukB06TZqTb"
      },
      "source": [
        "https://pypi.org/project/pytorch-warmup/ link for doing warmup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB_w3zeIOa8X"
      },
      "source": [
        "def train(net, epochs, train_loader, test_loader, lr, weight_decay, \n",
        "          print_every = 6, lo_test_dataset = len(test_dataset), lo_train_dataset = len(train_dataset)):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Check which parameters can calculate gradients. \n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "\n",
        "    # optimizer = Ranger(net.parameters(), lr = lr, weight_decay= weight_decay)\n",
        "    base_optimizer = Ranger\n",
        "    optimizer = sam.SAM(net.parameters(), base_optimizer, lr = lr, weight_decay = weight_decay)\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
        "\n",
        "    net.to(device)\n",
        "    print(\"Device: {}\".format(device))\n",
        "    print(\"Optimizer: {}\".format(optimizer))\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        \n",
        "        train_loss = steps = train_mAP = train_mAR = missed_train_images = 0\n",
        "        \n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "            net.train()\n",
        "\n",
        "            steps += 1\n",
        "            \n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            net.eval()\n",
        "            train_mAP, train_mAR, missed_train_images = run_metrics_for_batch(net(images), targets, train_mAP, train_mAR, missed_train_images, device)\n",
        "            net.train()\n",
        "\n",
        "            losses.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            optimizer.first_step(zero_grad = True)\n",
        "\n",
        "            loss_dict = net(images, targets)\n",
        "\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            losses.backward()\n",
        "            optimizer.second_step(zero_grad = True)\n",
        "\n",
        "            train_loss +=  losses.item()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (steps % print_every) == 0:\n",
        "\n",
        "              with torch.no_grad():\n",
        "                test_mAP = test_mAR = missed_test_images = test_loss = 0\n",
        "\n",
        "                for images, targets in test_loader:\n",
        "\n",
        "                  net.eval()\n",
        "                  if device == torch.device(\"cuda\"):\n",
        "                    images = [image.to(device) for image in images]\n",
        "                    targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
        "\n",
        "                  output = net(images)\n",
        "                  test_mAP, test_mAR, missed_test_images = run_metrics_for_batch(output, targets, test_mAP, test_mAR, missed_test_images, device)\n",
        "\n",
        "                  net.train()\n",
        "                  test_loss_dict = net(images, targets)\n",
        "                  test_losses = sum(loss for loss in test_loss_dict.values())\n",
        "                  test_loss += test_losses.item()\n",
        "\n",
        "                for param_group in optimizer.param_groups:\n",
        "                  learning_rate_extract = param_group[\"lr\"]\n",
        "                print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Test_loss: {:0.2f} | Test mAP: {:0.2f}% | Test mAR: {:0.2f}% | Missed Test Images: {}\".format(\n",
        "                    epoch + 1, epochs, steps, learning_rate_extract, train_loss, test_loss,  \n",
        "                    (test_mAP / float(lo_test_dataset)) * 100., (test_mAR / float(lo_test_dataset)) * 100.,missed_test_images))\n",
        "\n",
        "              assert (steps % print_every) == 0\n",
        "              train_loss = 0\n",
        "                 \n",
        "        print(\"\\n Epoch {} Final Train mAP: {:0.2f}% | Epoch {} Final Train mAR: {:0.2f}% | Epoch {} Final Missed Train Images: {} out of {} images \\n\".format(\n",
        "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, (train_mAR / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, missed_train_images, lo_train_dataset\n",
        "        ))\n",
        "    \n",
        "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))\n",
        "\n",
        "    return net"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj7pzNpCdH6j"
      },
      "source": [
        "Note check if should use tensorv2 or tensor and universal normalizing step. \n",
        "https://github.com/toandaominh1997/EfficientDet.Pytorch/blob/fbe56e58c9a2749520303d2d380427e5f01305ba/datasets/augmentation.py#L94"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjFx0ZuYFca"
      },
      "source": [
        "class EffdetFruitDetectDataset(object):\n",
        "  def __init__(self, id_labels, id_bounding_boxes, transforms, mode):\n",
        "\n",
        "    assert len(id_labels) == len(id_bounding_boxes)\n",
        "    assert sorted(id_labels.keys()) == sorted(id_bounding_boxes.keys())\n",
        "    self.imgs_key = sorted(id_labels.keys())\n",
        "\n",
        "    np.random.shuffle(self.imgs_key)\n",
        "    if (mode == \"train\"):\n",
        "      self.imgs_key = self.imgs_key[:int(len(self.imgs_key) * 0.8)]\n",
        "    else:\n",
        "      self.imgs_key = self.imgs_key[int(len(self.imgs_key) * 0.8):]\n",
        "\n",
        "    self.id_labels = id_labels\n",
        "    self.id_bounding_boxes = id_bounding_boxes\n",
        "    self.full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_path = ffile_path(self.imgs_key[idx], self.full_image_file_paths) \n",
        "    img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "    boxes = convert_min_max(torch.as_tensor(self.id_bounding_boxes[self.imgs_key[idx]], dtype=torch.float32))\n",
        "    \n",
        "    labels = torch.as_tensor(self.id_labels[self.imgs_key[idx]], dtype=torch.int64)\n",
        "    \n",
        "    #Query about transforms for labels of images\n",
        "    if self.transforms: \n",
        "      sample = {\n",
        "                'image': img,\n",
        "                'bboxes': boxes,\n",
        "                'labels': labels\n",
        "            }\n",
        "\n",
        "      sample = self.transforms(**sample)\n",
        "      img = sample['image']\n",
        "      boxes = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "\n",
        "    return {'image': img, 'bboxes': boxes, 'category_id': labels}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs_key)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmonKCf2YtY2"
      },
      "source": [
        "def detection_collate(batch):\n",
        "    imgs = [s['image'] for s in batch]\n",
        "    annots = [s['bboxes'] for s in batch]\n",
        "    labels = [s['category_id'] for s in batch]\n",
        "\n",
        "    max_num_annots = max(len(annot) for annot in annots)\n",
        "    annot_padded = np.ones((len(annots), max_num_annots, 5))*-1\n",
        "\n",
        "    if max_num_annots > 0:\n",
        "        for idx, (annot, lab) in enumerate(zip(annots, labels)):\n",
        "            if len(annot) > 0:\n",
        "                annot_padded[idx, :len(annot), :4] = annot\n",
        "                annot_padded[idx, :len(annot), 4] = lab\n",
        "    return (torch.stack(imgs, 0), torch.FloatTensor(annot_padded))\n",
        "\n",
        "train_batch_size = 5\n",
        "\n",
        "train_dataset = EffdetFruitDetectDataset(labels_dict, bounding_box_dict, get_transforms(mode = \"effdet_train\"), mode = \"train\")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= detection_collate)\n",
        "#Build a valid loader"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsCcFeBalgYu"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, targets = next(dataiter)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsNm_eBWlxWv",
        "outputId": "eb3e892d-76e9-4b64-d626-3f1c30e58d2e"
      },
      "source": [
        "images[0].unsqueeze(0).size()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 512, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC6a5bdEi-af"
      },
      "source": [
        "def train_effdet(train_loader, model, scheduler, optimizer, epoch, print_every):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # global iteration \n",
        "    print(\"{} epoch: \\t start training....\".format(epoch))\n",
        "    start = time.time()\n",
        "    total_loss = []\n",
        "    model.train()\n",
        "    model.module.is_training = True\n",
        "    model.module.freeze_bn()\n",
        "    optimizer.zero_grad()\n",
        "    # Make sure it has a for loop for epochs too.\n",
        "    for e in range(epoch):\n",
        "      for idx, (images, annotations) in enumerate(train_loader):\n",
        "        images = images.cuda().float()\n",
        "        annotations = annotations.cuda()\n",
        "\n",
        "        classification_loss, regression_loss = model([images, annotations])\n",
        "        classification_loss = classification_loss.mean()\n",
        "        regression_loss = regression_loss.mean()\n",
        "        loss = classification_loss + regression_loss\n",
        "        if bool(loss == 0):\n",
        "            print('loss equal zero(0)')\n",
        "            continue\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss.append(loss.item())\n",
        "        if(iteration % 50 == 0):\n",
        "\n",
        "\n",
        "\n",
        "          \n",
        "            print('{} iteration: training ...'.format(iteration))\n",
        "            ans = {\n",
        "                'epoch': epoch,\n",
        "                'iteration': iteration,\n",
        "                'cls_loss': classification_loss.item(),\n",
        "                'reg_loss': regression_loss.item(),\n",
        "                'mean_loss': np.mean(total_loss)\n",
        "            }\n",
        "            for key, value in ans.items():\n",
        "                print('    {:15s}: {}'.format(str(key), value))\n",
        "        iteration += 1\n",
        "    scheduler.step(np.mean(total_loss))\n",
        "    result = {\n",
        "        'time': time.time() - start,\n",
        "        'loss': np.mean(total_loss)\n",
        "    }\n",
        "    for key, value in result.items():\n",
        "        print('    {:15s}: {}'.format(str(key), value))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByWnzIzIU-9v"
      },
      "source": [
        "Might need to move model to device \n",
        "If losses printing is not working properly then try to accumalate losses and print them like how I did in the normal train function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE3sX7xfV3ZO",
        "outputId": "72701f37-3a68-4f0c-97b0-d955c12ce380"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device == torch.device(\"cpu\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS4fTCSYULYU"
      },
      "source": [
        "def train_effdet(net, epochs, train_loader, test_loader, lr, weight_decay, \n",
        "          print_every = 6, lo_test_dataset = len(test_dataset), lo_train_dataset = len(train_dataset)):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device: {}\".format(device))\n",
        "    print(\"Note: Train Accuracies are only run through one train image per batch\")\n",
        "\n",
        "    if device == torch.device(\"cpu\"):\n",
        "      warnings.warn(\"Code does not support running on CPU but only GPU\")\n",
        "\n",
        "    optimizer = optim.AdamW(net.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
        "\n",
        "    start_time = time.time()\n",
        "    net.module.freeze_bn()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        net.train()\n",
        "        net.module.is_training = True\n",
        "        \n",
        "        train_loss = steps = train_mAP = train_mAR = missed_train_images = 0\n",
        "        \n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "\n",
        "            net.train()\n",
        "            net.module.is_training = True\n",
        "\n",
        "            steps += 1\n",
        "            \n",
        "            images = images.cuda().float()\n",
        "            targets = targets.cuda()\n",
        "\n",
        "            classification_loss, regression_loss = model([images, targets])\n",
        "            classification_loss = classification_loss.mean()\n",
        "            regression_loss = regression_loss.mean()\n",
        "            loss = classification_loss + regression_loss\n",
        "            if bool(loss == 0):\n",
        "              print('loss equal zero(0)')\n",
        "              continue\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            net.eval()\n",
        "            net.module.is_training = False\n",
        "\n",
        "            #Needs changing to fit to new metrics.\n",
        "            # Update this code to run on a single image in a batch of training data. Use (images[0].unsqueeze(0).size())\n",
        "            train_mAP, train_mAR, missed_train_images = run_metrics_for_effdet_batch(scores.cuda(), classification.cuda(), transformed_anchors.cuda(), targets.cuda(), 0, 0, 0, torch.device(\"cuda\"))\n",
        "            train_mAP, train_mAR, missed_train_images = run_metrics_for_batch(net(images), targets, train_mAP, train_mAR, missed_train_images, device)\n",
        "\n",
        "            net.train()\n",
        "            net.module.is_training = True\n",
        "\n",
        "            train_loss += losses.item()\n",
        "\n",
        "            #Make a validation happen\n",
        "            if (steps % print_every) == 0:\n",
        "\n",
        "              with torch.no_grad():\n",
        "                test_mAP = test_mAR = missed_test_images = test_loss = 0\n",
        "\n",
        "                for images, targets in test_loader:\n",
        "\n",
        "                  if images.size(0) != 1:\n",
        "                    warning.warn(\"Only can validate fully with batch size of 1, \\\n",
        "                    bigger batch sizes risk Errors or Incomplete Validation\")\n",
        "                  \n",
        "                  net.eval()\n",
        "                  net.module.is_training = False\n",
        "\n",
        "                  if device == torch.device(\"cuda\"):\n",
        "                    images = images.cuda().float()\n",
        "                    targets = targets.cuda()\n",
        "\n",
        "                  #Needs changing to fit to new metrics\n",
        "                  scores, classification, transformed_anchors = net(images)\n",
        "                  test_mAP, test_mAR, missed_test_images = run_metrics_for_batch(output, targets, test_mAP, test_mAR, missed_test_images, device)\n",
        "\n",
        "                  net.train()\n",
        "                  net.module.is_training = True\n",
        "\n",
        "                  classification_loss, regression_loss = model([images, annotations])\n",
        "                  classification_loss = classification_loss.mean()\n",
        "                  regression_loss = regression_loss.mean()\n",
        "                  loss = classification_loss + regression_loss\n",
        "\n",
        "                  test_loss += loss.item()\n",
        "\n",
        "                for param_group in optimizer.param_groups:\n",
        "                  learning_rate_extract = param_group[\"lr\"]\n",
        "                print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Test_loss: {:0.2f} | Test mAP: {:0.2f}% | Test mAR: {:0.2f}% | Missed Test Images: {}\".format(\n",
        "                    epoch + 1, epochs, steps, learning_rate_extract, train_loss, test_loss,  \n",
        "                    (test_mAP / float(lo_test_dataset)) * 100., (test_mAR / float(lo_test_dataset)) * 100.,missed_test_images))\n",
        "\n",
        "              assert (steps % print_every) == 0\n",
        "              train_loss = 0\n",
        "                 \n",
        "        print(\"\\n Epoch {} Final Train mAP: {:0.2f}% | Epoch {} Final Train mAR: {:0.2f}% | Epoch {} Final Missed Train Images: {} out of {} images \\n\".format(\n",
        "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, (train_mAR / float(lo_train_dataset)) * 100., \n",
        "            epoch + 1, missed_train_images, lo_train_dataset\n",
        "        ))\n",
        "    \n",
        "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcYHTukRKbOc"
      },
      "source": [
        "model.module.is_training = False\n",
        "scores, classification, transformed_anchors = model(dummy_image)\n",
        "model.module.is_training = True"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsMirEP1TNKL",
        "outputId": "917440bc-53c2-4243-e54d-483fb795acd8"
      },
      "source": [
        "# How about more than one batches\n",
        "print(scores)\n",
        "print(classification)\n",
        "print(transformed_anchors)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.0000, 1.0000, 1.0000,  ..., 0.4727, 0.3681, 0.3659], device='cuda:0',\n",
            "       grad_fn=<MaxBackward0>)\n",
            "tensor([1, 1, 6,  ..., 2, 1, 1], device='cuda:0')\n",
            "tensor([[426.0839,  98.3806, 448.0107, 512.0000],\n",
            "        [462.5461, 451.3073, 512.0000, 453.5079],\n",
            "        [455.8012, 566.1769, 512.0000, 512.0000],\n",
            "        ...,\n",
            "        [  0.0000, 492.2289,  29.7879, 495.3742],\n",
            "        [355.8596, 512.0571, 389.9931, 512.0000],\n",
            "        [380.8128, 441.5396, 437.6480, 445.9307]], device='cuda:0',\n",
            "       grad_fn=<IndexBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn0vmaR5aGcy"
      },
      "source": [
        "## This link shows the problem https://github.com/pytorch/vision/issues/2740\n",
        "The answer from @oke-aditya is correct. You are probably passing to the model bounding boxes in the format [xmin, ymin, width, height], while Faster R-CNN expects boxes to be in [xmin, ymin, xmax, ymax] format.\n",
        "\n",
        "Changing this should fix the issue.\n",
        "\n",
        "We have btw recently added box conversion utilities to torchvision (thanks to @oke-aditya ), they can be found in\n",
        "\n",
        "Look at box convert or doing it locally also works.\n",
        "\n",
        "### https://github.com/pytorch/vision/blob/a98e17e50146529cdfadb590ba063e6bbee71de2/torchvision/ops/boxes.py#L137-L156\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYAu4FDknTwH"
      },
      "source": [
        "### Let us try another bigger model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF1AnUSZienq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "513586f568764f66a2ec6c1ce6e48ad1",
            "efe79b0083b04ad2bd627cf8b810d2cf",
            "c47a98cc3c6f4b168d011ca10610c4fa",
            "ed954f85d1b04ed49258a3762b0a75d5",
            "4b66dccb59fb41a593925d92f83647d0",
            "292f67bf7c214d0aa109969537a3b691",
            "575144ade3e644ac927ba54651fd5427",
            "c1c7aad80eaa4a00b56c01ed377d594c"
          ]
        },
        "outputId": "81e8fd08-d70b-4e61-a80e-48fc1ca6f88f"
      },
      "source": [
        "MODEL_MAP = {\n",
        "    'efficientdet-d0': 'efficientnet-b0',\n",
        "    'efficientdet-d1': 'efficientnet-b1',\n",
        "    'efficientdet-d2': 'efficientnet-b2',\n",
        "    'efficientdet-d3': 'efficientnet-b3',\n",
        "    'efficientdet-d4': 'efficientnet-b4',\n",
        "    'efficientdet-d5': 'efficientnet-b5',\n",
        "    'efficientdet-d6': 'efficientnet-b6',\n",
        "    'efficientdet-d7': 'efficientnet-b6',\n",
        "}\n",
        "class EfficientDet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 network='efficientdet-d0',\n",
        "                 D_bifpn=3,\n",
        "                 W_bifpn=88,\n",
        "                 D_class=3,\n",
        "                 is_training=True,\n",
        "                 threshold=0.01,\n",
        "                 iou_threshold=0.5):\n",
        "        super(EfficientDet, self).__init__()\n",
        "        \n",
        "        self.backbone = EfficientNet.from_pretrained(MODEL_MAP[network])\n",
        "        self.is_training = is_training\n",
        "        self.neck = BIFPN(in_channels=self.backbone.get_list_features()[-5:],\n",
        "                          out_channels=W_bifpn,\n",
        "                          stack=D_bifpn,\n",
        "                          num_outs=5)\n",
        "        self.bbox_head = RetinaHead(num_classes=num_classes,\n",
        "                                    in_channels=W_bifpn)\n",
        "\n",
        "        self.anchors = Anchors()\n",
        "        self.regressBoxes = BBoxTransform()\n",
        "        self.clipBoxes = ClipBoxes()\n",
        "        self.threshold = threshold\n",
        "        self.iou_threshold = iou_threshold\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "        self.freeze_bn()\n",
        "        self.criterion = FocalLoss()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.is_training:\n",
        "            inputs, annotations = inputs\n",
        "        else:\n",
        "            inputs = inputs\n",
        "        x = self.extract_feat(inputs)\n",
        "        outs = self.bbox_head(x)\n",
        "        classification = torch.cat([out for out in outs[0]], dim=1)\n",
        "        regression = torch.cat([out for out in outs[1]], dim=1)\n",
        "        anchors = self.anchors(inputs)\n",
        "        if self.is_training:\n",
        "            return self.criterion(classification, regression, anchors, annotations)\n",
        "        else:\n",
        "            transformed_anchors = self.regressBoxes(anchors, regression)\n",
        "            transformed_anchors = self.clipBoxes(transformed_anchors, inputs)\n",
        "            scores = torch.max(classification, dim=2, keepdim=True)[0]\n",
        "            scores_over_thresh = (scores > self.threshold)[0, :, 0]\n",
        "\n",
        "            if scores_over_thresh.sum() == 0:\n",
        "                print('No boxes to NMS')\n",
        "                # no boxes to NMS, just return\n",
        "                return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n",
        "            classification = classification[:, scores_over_thresh, :]\n",
        "            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n",
        "            scores = scores[:, scores_over_thresh, :]\n",
        "            anchors_nms_idx = nms(\n",
        "                transformed_anchors[0, :, :], scores[0, :, 0], iou_threshold=self.iou_threshold)\n",
        "            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(\n",
        "                dim=1)\n",
        "            return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]\n",
        "\n",
        "    def freeze_bn(self):\n",
        "        '''Freeze BatchNorm layers.'''\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.BatchNorm2d):\n",
        "                layer.eval()\n",
        "\n",
        "    def extract_feat(self, img):\n",
        "        \"\"\"\n",
        "            Directly extract features from the backbone+neck\n",
        "        \"\"\"\n",
        "        x = self.backbone(img)\n",
        "        x = self.neck(x[-5:])\n",
        "        return x\n",
        "\n",
        "model= EfficientDet(num_classes=len(classes),is_training=True)\n",
        "model.train()\n",
        "\n",
        "model.freeze_bn()\n",
        "\n",
        "model = model.cuda()\n",
        "print('Run with DataParallel ....')\n",
        "\n",
        "## Make sure that you add this line, even though you are not using more than one \n",
        "# GPU DataParallel adds \"module\" to the start of the model structure \n",
        "# allowing for the syntax to be correct when calling \"model.module.freeze_bn()\" for example\n",
        "model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "# I am doing this here an example, you do not have to call the lines below here\n",
        "model.module.is_training = True\n",
        "model.module.freeze_bn()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "513586f568764f66a2ec6c1ce6e48ad1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=21388428.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Run with DataParallel ....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA_fsu_6JeEj"
      },
      "source": [
        "dummy_image = torch.ones((2, 3, 512, 512)).cuda()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kobyKa74jVPq",
        "outputId": "ebc6587c-404e-4648-843b-82c52085904f"
      },
      "source": [
        "if dummy_image.size(0) == 2:\n",
        "  warnings.warn(\"njw\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: njw\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuFiRct7P6F4",
        "outputId": "5af62bf6-f22f-4751-9933-26614bb7120b"
      },
      "source": [
        "model.module.is_training = False\n",
        "#only output one result which means batch size of only one for the validation.\n",
        "model(dummy_image)\n",
        "model.module.is_training = True"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([1.0000, 1.0000, 1.0000,  ..., 0.0403, 0.0398, 0.0358], device='cuda:0',\n",
            "       grad_fn=<MaxBackward0>), tensor([1, 2, 1,  ..., 6, 4, 6], device='cuda:0'), tensor([[404.8494,   0.0000, 487.3432, 241.8042],\n",
            "        [176.3473,  87.1043, 512.0000, 162.7402],\n",
            "        [345.1752,  33.6655, 500.8384, 264.2399],\n",
            "        ...,\n",
            "        [445.5258, 319.0584, 471.3172, 338.8416],\n",
            "        [444.8307, 298.3829, 473.0628, 324.0254],\n",
            "        [441.6893, 285.7062, 470.3988, 307.8020]], device='cuda:0',\n",
            "       grad_fn=<IndexBackward>)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNYky01Bhr1a",
        "outputId": "6098006f-ceac-4e36-fd01-c6059c0c7d3a"
      },
      "source": [
        "len(scores)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17104"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToKK_o30QKWU",
        "outputId": "f8000d10-e980-4a3c-95c0-f8d36783cc4e"
      },
      "source": [
        "transformed_anchors"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 25.4853, 113.7197,  89.7030, 119.1567],\n",
              "        [ 83.2042, 113.9103, 115.0474, 121.7166],\n",
              "        [ 17.9059, 118.8181,  66.1361, 132.8636],\n",
              "        ...,\n",
              "        [119.9296, 508.6038, 181.2677, 512.0000],\n",
              "        [147.4839, 510.8638, 211.4706, 512.0000],\n",
              "        [134.4572, 510.2365, 195.0367, 512.0000]], device='cuda:0',\n",
              "       grad_fn=<IndexBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqdazNGQP1dI"
      },
      "source": [
        "# bboxes = list()\n",
        "# labels = list()\n",
        "# bbox_scores = list()\n",
        "            \n",
        "# for j in range(scores.shape[0]):\n",
        "#                 bbox = transformed_anchors[[j], :][0].data.cpu().numpy()\n",
        "#                 x1 = int(bbox[0]*origin_img.shape[1]/self.size_image[1])\n",
        "#                 y1 = int(bbox[1]*origin_img.shape[0]/self.size_image[0])\n",
        "#                 x2 = int(bbox[2]*origin_img.shape[1]/self.size_image[1])\n",
        "#                 y2 = int(bbox[3]*origin_img.shape[0]/self.size_image[0])\n",
        "#                 bboxes.append([x1, y1, x2, y2])\n",
        "#                 label_name = VOC_CLASSES[int(classification[[j]])]\n",
        "#                 labels.append(label_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x2raneqOzmZ",
        "outputId": "e5ec3745-a3e9-4dd3-88cd-b21f4ed86f9b"
      },
      "source": [
        "len(scores), len(classification), len(transformed_anchors)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12330, 12330, 12330)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbYjFJSaUAPy",
        "outputId": "f5865667-418e-4b60-a239-46c4a0f3d738"
      },
      "source": [
        "iteration = 1\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, patience=3, verbose=True)\n",
        "train_effdet(train_loader, model, scheduler, optimizer, 5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 epoch: \t start training....\n",
            "50 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 50\n",
            "    cls_loss       : 2.302124500274658\n",
            "    reg_loss       : 1.0861663818359375\n",
            "    mean_loss      : 161.84513342380524\n",
            "100 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 100\n",
            "    cls_loss       : 2.302124500274658\n",
            "    reg_loss       : 1.030957579612732\n",
            "    mean_loss      : 82.68581563234329\n",
            "150 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 150\n",
            "    cls_loss       : 2.302124500274658\n",
            "    reg_loss       : 1.129921793937683\n",
            "    mean_loss      : 56.28243855953217\n",
            "200 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 200\n",
            "    cls_loss       : 2.302124500274658\n",
            "    reg_loss       : 1.6258493661880493\n",
            "    mean_loss      : 43.056051405668256\n",
            "250 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 250\n",
            "    cls_loss       : 2.302124261856079\n",
            "    reg_loss       : 1.0305578708648682\n",
            "    mean_loss      : 35.112570625305175\n",
            "300 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 300\n",
            "    cls_loss       : 2.302124500274658\n",
            "    reg_loss       : 1.0540986061096191\n",
            "    mean_loss      : 29.899985178311667\n",
            "350 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 350\n",
            "    cls_loss       : 2.302124261856079\n",
            "    reg_loss       : 1.0182584524154663\n",
            "    mean_loss      : 26.36029699734279\n",
            "400 iteration: training ...\n",
            "    epoch          : 5\n",
            "    iteration      : 400\n",
            "    cls_loss       : 2.302124500274658\n",
            "    reg_loss       : 1.066689133644104\n",
            "    mean_loss      : 23.48197797715664\n",
            "    time           : 60.75796031951904\n",
            "    loss           : 22.341373769742138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPmuew0CnX92"
      },
      "source": [
        "### The Mobile Net Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbDtIsBs-OWD"
      },
      "source": [
        "backbone = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
        "backbone.roi_heads.box_predictor.cls_score.out_features = 6\n",
        "backbone.roi_heads.box_predictor.bbox_pred.out_features = 24\n",
        "# backbone.roi_heads.box_predictor.cls_score.out_features = 3\n",
        "# backbone.roi_heads.box_predictor.bbox_pred.out_features = 12\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-cknP0_uy0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8456de7-ae48-4366-cda6-99b2692c2142"
      },
      "source": [
        "# https://github.com/amdegroot/ssd.pytorch/blob/master/layers/box_utils.py#L48\n",
        "def intersect(box_a, box_b):\n",
        "\n",
        "    A = box_a.size(0)\n",
        "    B = box_b.size(0)\n",
        "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
        "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
        "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
        "    return inter[:, :, 0] * inter[:, :, 1]\n",
        "\n",
        "def jaccard_iou(box_a, box_b):\n",
        "\n",
        "    inter = intersect(box_a, box_b)\n",
        "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
        "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
        "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
        "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
        "    union = area_a + area_b - inter\n",
        "    return inter / union  # [A,B]\n",
        "\n",
        "def calculate_iou_on_label(results, len_of_results, iou_thresh, device):\n",
        "  for current_index, _ in enumerate(results[\"boxes\"]):\n",
        "    if current_index >= len_of_results:\n",
        "      break\n",
        "\n",
        "    current_index_iou = jaccard_iou(results[\"boxes\"][current_index].view(1, -1).to(device),\n",
        "                                    results[\"boxes\"].to(device))\n",
        "    \n",
        "    mask = (current_index_iou > iou_thresh) & (current_index_iou != 1)\n",
        "    mask = mask.squeeze()\n",
        "    for key in results:\n",
        "      results[key] = results[key][~mask]\n",
        "\n",
        "    len_of_results -= sum(mask)\n",
        "  \n",
        "  return results\n",
        "\n",
        "def get_labels_categ(classes, want):\n",
        "  fruit_index_list, bad_spot_index_list = list(), list()\n",
        "  for ii, name in enumerate(classes):\n",
        "    if re.search(\"Spot\", name):\n",
        "      bad_spot_index_list.append(ii)\n",
        "    elif re.search(\"Placeholder\", name):\n",
        "      continue\n",
        "    else:\n",
        "      fruit_index_list.append(ii)\n",
        "  \n",
        "  if want == \"fruit\":\n",
        "    return fruit_index_list\n",
        "  elif want == \"bad_spot\":\n",
        "    return bad_spot_index_list\n",
        "  else:\n",
        "    raise ValueError(\"want Type not applicable [fruit or bad_spot only]\")\n",
        "\n",
        "print(classes)\n",
        "get_labels_categ(classes, \"bad_spot\")"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Placeholder', 'Apples', 'Strawberry', 'Tomato', 'Apple_Bad_Spot', 'Strawberry_Bad_Spot', 'Tomato_Bad_Spot']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 5, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFhjlggXlAx7"
      },
      "source": [
        "def infer_image(image_file_path, trained_model, distance_thresh, iou_thresh, webcam = False, show_image = True):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  #Just load it up as PIL. Avoid using cv2 because do not need albumentations\n",
        "  if not webcam:\n",
        "    torch_image = F.to_tensor(Image.open(image_file_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    trained_model.to(device)\n",
        "    trained_model.eval()\n",
        "    print(\"Image Size: {}\".format(torch_image.size()))\n",
        "\n",
        "    start_time = time.time()\n",
        "    results = trained_model(torch_image)\n",
        "    end_time = time.time() - start_time\n",
        "\n",
        "    print(\"Time of Inference {:0.2f}\".format(end_time))\n",
        "  else:\n",
        "\n",
        "    torch_image = F.to_tensor(image_file_path).unsqueeze(1).to(device)\n",
        "\n",
        "    results = trained_model(torch_image)\n",
        "\n",
        "  valid_box_count = 0\n",
        "  for ii, score in enumerate(results[0][\"scores\"]):\n",
        "    if score < distance_thresh:\n",
        "      low_index_start = ii\n",
        "      break\n",
        "    else:\n",
        "      valid_box_count += 1\n",
        "\n",
        "  if valid_box_count == len(results[0][\"scores\"]):\n",
        "    low_index_start = len(results[0][\"scores\"])\n",
        "  \n",
        "  for key in results[0]:\n",
        "    results[0][key] = results[0][key][:low_index_start]\n",
        "  \n",
        "  #This is where I place the order of the list\n",
        "  fruit_spot_iou_thresh, bad_spot_iou_thresh = iou_thresh\n",
        "\n",
        "  #Update when I get more data of fruits and when running for script beware of classes.\n",
        "  bad_spot_index = [ii for ii, label in enumerate(results[0][\"labels\"]) if label in get_labels_categ(classes, \"bad_spot\")]\n",
        "  fruit_index = [ii for ii, _ in enumerate(results[0][\"labels\"]) if ii not in bad_spot_index]\n",
        "\n",
        "  bad_spot_results, fruit_results = dict(), dict()\n",
        "\n",
        "  for key in results[0]:\n",
        "    bad_spot_results[key], fruit_results[key] = results[0][key][[bad_spot_index]], results[0][key][[fruit_index]]\n",
        "\n",
        "  assert len(bad_spot_results[\"boxes\"]) == len(bad_spot_results[\"scores\"]) == len(bad_spot_results[\"labels\"])\n",
        "  assert len(fruit_results[\"boxes\"]) == len(fruit_results[\"scores\"]) == len(fruit_results[\"labels\"])\n",
        "\n",
        "  len_of_bad_spots, len_of_fruit = len(bad_spot_results[\"boxes\"]), len(fruit_results[\"boxes\"])\n",
        "\n",
        "  if len_of_bad_spots > 1:\n",
        "    bad_spot_results = calculate_iou_on_label(bad_spot_results, len_of_bad_spots, bad_spot_iou_thresh, device)\n",
        "  if len_of_fruit > 1:\n",
        "    fruit_results = calculate_iou_on_label(fruit_results, len_of_fruit, fruit_spot_iou_thresh, device)\n",
        "  \n",
        "  for key in results[0]: \n",
        "    if (key == \"boxes\"):\n",
        "      results[0][\"boxes\"] = torch.cat((fruit_results[\"boxes\"], bad_spot_results[\"boxes\"]), axis = 0)\n",
        "    else:\n",
        "      results[0][key] = torch.cat((fruit_results[key], bad_spot_results[key]), dim = 0)\n",
        "\n",
        "  if show_image:\n",
        "    if device == torch.device(\"cuda\"):\n",
        "      torch_image = torch_image.cpu() \n",
        "    written_image = cv2.cvtColor(draw_boxes(results[0][\"boxes\"], results[0][\"labels\"], torch_image.squeeze(), infer = True, put_text= True), cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(written_image)\n",
        "  \n",
        "  return results"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZd4nVDuiu4M"
      },
      "source": [
        "import base64\n",
        "import html\n",
        "import io\n",
        "import time\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "def start_input():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      /* try changing the capture canvas and see what happens*/\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def take_photo(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T45EScTjJ5e"
      },
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def js_reply_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          js_reply: JavaScript object, contain image from webcam\n",
        "\n",
        "    output: \n",
        "          image_array: image array RGB size 512 x 512 from webcam\n",
        "    \"\"\"\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\n",
        "    image_array = np.array(image_PIL)\n",
        "\n",
        "    return image_array\n",
        "\n",
        "def drawing_array_to_bytes(drawing_array):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          drawing_array: image RGBA size 512 x 512 \n",
        "                              contain bounding box and text from yolo prediction, \n",
        "                              channel A value = 255 if the pixel contains drawing properties (lines, text) \n",
        "                              else channel A value = 0\n",
        "\n",
        "    output: \n",
        "          drawing_bytes: string, encoded from drawing_array\n",
        "    \"\"\"\n",
        "\n",
        "    drawing_PIL = Image.fromarray(drawing_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    drawing_PIL.save(iobuf, format='png')\n",
        "    drawing_bytes = 'data:image/png;base64,{}'.format((str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "    return drawing_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FSrjfHjgX4q"
      },
      "source": [
        "data_transforms = get_transforms(mode = \"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "j0WidBmGlktM",
        "outputId": "966e0c06-eb6b-4bb4-f10a-9b5c7087947b"
      },
      "source": [
        "start_input()\n",
        "label_html = 'Capturing...'\n",
        "img_data = ''\n",
        "count = 0 \n",
        "\n",
        "color=None\n",
        "label=None\n",
        "line_thickness=None\n",
        "another_one.to(device).eval();\n",
        "while True:\n",
        "    js_reply = take_photo(label_html, img_data)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    image = js_reply_to_image(js_reply)\n",
        "    prediciton = infer_image(image, another_one, 0.2, [0.3, 0.1], webcam= True, show_image = False)\n",
        "\n",
        "    drawing_array = np.zeros([512,512,4], dtype=np.uint8)\n",
        "\n",
        "    for x in prediciton[0]['boxes']:\n",
        "\n",
        "      tl = line_thickness or round(0.002 * (drawing_array.shape[0] + drawing_array.shape[1]) / 2) + 1  # line/font thickness\n",
        "      color = color or [random.randint(0, 255) for _ in range(3)]\n",
        "      c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
        "      cv2.rectangle(drawing_array, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
        "      if label:\n",
        "        tf = max(tl - 1, 1)  # font thickness\n",
        "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
        "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
        "        cv2.rectangle(drawing_array, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
        "        cv2.putText(drawing_array, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "    drawing_array[:,:,3] = (drawing_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "\n",
        "    drawing_PIL = Image.fromarray(drawing_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    drawing_PIL.save(iobuf, format='png')\n",
        "    drawing_bytes = 'data:image/png;base64,{}'.format((str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "    img_data = drawing_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      /* try changing the capture canvas and see what happens*/\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 512; //video.videoWidth;\n",
              "      captureCanvas.height = 512; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function takePhoto(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}